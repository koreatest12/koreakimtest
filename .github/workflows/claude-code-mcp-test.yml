name: "🧪 Claude Code — MCP MEGA++++ + Banking Pipeline (PG COPY • 파티션 자동화 • 권한/롤 • 성능 그래프 • 멀티DB • 만전 대비 • PATH/런타임 스냅샷)"

on:
  push:
  pull_request:
  workflow_dispatch:
    inputs:
      mode:
        type: choice
        options: [full, lite]
        default: full
      node_version:
        default: "20"
      python_version:
        default: "3.12"
      db_path:
        default: ".github/echo_artifacts/mcp_demo.sqlite"
      seed_rows:
        default: "5"
      csv_rows:
        default: "20000"
      batch_size:
        default: "2000"
      use_copy:
        description: "PG COPY FROM STDIN 사용"
        type: boolean
        default: true
  
  # 💡 추가된 상주 배치 기능: 매일 3:30 AM KST에 실행 (UTC 기준 18:30)
  schedule:
    # 0 0 * * * (매일 자정 UTC)
    # 30 18 * * * (매일 18:30 UTC = KST 03:30)
    - cron: '30 18 * * *'

# 💡 러너 풀이 찼을 때 대기열/진행 중인 이전 실행을 자동으로 비웁니다.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

env:
  TZ: Asia/Seoul
  LOG_DIR: .github/echo_logs
  ARTIFACT_DIR: .github/echo_artifacts
  MCP_DIR: .github/mcp_demo
  PG_CONTAINER: pgtest
  PG_IMAGE: postgres:16
  PG_USER: postgres
  PG_PASS: pass
  PG_DB: mcpdb
  PG_PORT: "5432"
  PG_DSN: postgres://postgres:pass@localhost:5432/mcpdb

jobs:
  ###########################################################################
  # 1) MCP MEGA (Claude Code + Postgres COPY + Report) - 만전 대비 / PATH 스냅샷
  ###########################################################################
  mcp-mega:
    runs-on: ubuntu-24.04
    env:
      NODE_VERSION_FALLBACK: "20"
      PY_VERSION_FALLBACK: "3.12"
      NODE_VERSION_INPUT: ${{ inputs.node_version }}
      PY_VERSION_INPUT: ${{ inputs.python_version }}
      DB_PATH_INPUT: ${{ inputs.db_path }}
      CSV_ROWS_INPUT: ${{ inputs.csv_rows }}
      BATCH_SIZE_INPUT: ${{ inputs.batch_size }}
      USE_COPY_INPUT: ${{ inputs.use_copy }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        continue-on-error: true

      - name: Prepare service state (pre-flight harden)
        shell: bash
        continue-on-error: true
        run: |
          set -Eeuo pipefail
          mkdir -p "$LOG_DIR" "$ARTIFACT_DIR" "$MCP_DIR" \
                   "$MCP_DIR/migrations/00_common" \
                   "$MCP_DIR/migrations/10_post_bulk/common" \
                   "$MCP_DIR/migrations/10_post_bulk/pg"
          touch "$LOG_DIR/.keep" "$ARTIFACT_DIR/.keep"
          : > "${ARTIFACT_DIR}/transactions_bulk.csv" || true
          : > "${ARTIFACT_DIR}/benchmark_sqlite.json" || true
          : > "${ARTIFACT_DIR}/benchmark_pg.json" || true
          : > "${ARTIFACT_DIR}/report.md" || true
          : > "${ARTIFACT_DIR}/report.html" || true
          {
            echo "=== PRE-FLIGHT SERVICE SNAPSHOT ==="
            echo "workflow=${GITHUB_WORKFLOW}"
            echo "job=mcp-mega"
            echo "runner_uname=$(uname -a)"
            echo "pwd_now=$(pwd)"
            echo "whoami_now=$(whoami)"
            echo "TZ=$TZ"
            echo "LOG_DIR=$LOG_DIR"
            echo "ARTIFACT_DIR=$ARTIFACT_DIR"
            echo "MCP_DIR=$MCP_DIR"
            echo "PG_IMAGE=${PG_IMAGE}"
            echo "PG_CONTAINER=${PG_CONTAINER}"
            echo "PG_DSN=${PG_DSN}"
            echo "RAW_NODE_VERSION_INPUT='${{ env.NODE_VERSION_INPUT }}'"
            echo "RAW_PY_VERSION_INPUT='${{ env.PY_VERSION_INPUT }}'"
            echo "NODE_VERSION_EFFECTIVE='${{ inputs.node_version != '' && inputs.node_version || env.NODE_VERSION_FALLBACK }}'"
            echo "PY_VERSION_EFFECTIVE='${{ inputs.python_version != '' && inputs.python_version || env.PY_VERSION_FALLBACK }}'"
            echo "DB_PATH_INPUT='${{ env.DB_PATH_INPUT }}'"
            echo "CSV_ROWS_INPUT='${{ env.CSV_ROWS_INPUT }}'"
            echo "BATCH_SIZE_INPUT='${{ env.BATCH_SIZE_INPUT }}'"
            echo "USE_COPY_INPUT='${{ env.USE_COPY_INPUT }}'"
            echo "--- INITIAL ENV PATH ---"
            echo "$PATH"
          } | tee "${LOG_DIR}/00_preflight_mcp.txt"
      - name: Prepare dirs & echo helpers
        shell: bash
        continue-on-error: true
        run: |
          set -Eeuo pipefail
          mkdir -p "$LOG_DIR" "$ARTIFACT_DIR" "$MCP_DIR" \
                   "$MCP_DIR/migrations/00_common" \
                   "$MCP_DIR/migrations/10_post_bulk/common" \
                   "$MCP_DIR/migrations/10_post_bulk/pg"
          cat > /tmp/echo_helpers.sh <<'SH'
          #!/usr/bin/env bash
          TS(){ date "+%Y-%m-%d %H:%M:%S%z"; }
          OK(){  echo "✅ [$(TS)] $*"; }
          WARN(){ echo "⚠️  [$(TS)] $*" >&2; }
          FAIL(){ echo "❌ [$(TS)] $*" >&2; }
          mklogs(){ mkdir -p "$LOG_DIR" "$ARTIFACT_DIR"; }
          safe(){
            local name="$1"; shift
            mklogs
            set +e
            ( "$@" ) >"$LOG_DIR/${name}.log" 2>&1
            local ec=$?
            if [ $ec -eq 0 ]; then OK "$name: OK"; else FAIL "$name: FAILED (exit=$ec)"; fi
            return 0
          }
          runlog(){
            local name="$1"; shift
            mklogs
            set +e
            ( "$@" ) \
              > >(tee "$LOG_DIR/${name}.log") \
              2> >(tee "$LOG_DIR/${name}.err.log" >&2)
            local ec=$?
            if [ $ec -eq 0 ]; then OK "$name: OK"; else FAIL "$name: FAILED (exit=$ec)"; fi
            return 0
          }
          SH
          chmod +x /tmp/echo_helpers.sh
          source /tmp/echo_helpers.sh
          OK "Directories + echo helpers ready"
      - name: Setup Node.js (with fallback)
        uses: actions/setup-node@v4
        with:
          node-version: ${{ inputs.node_version != '' && inputs.node_version || env.NODE_VERSION_FALLBACK }}
        continue-on-error: true

      - name: Setup Python runtime (with fallback)
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python_version != '' && inputs.python_version || env.PY_VERSION_FALLBACK }}
        continue-on-error: true

      - name: Runtime PATH / versions snapshot
        shell: bash
        continue-on-error: true
        run: |
          source /tmp/echo_helpers.sh
          {
            echo "=== RUNTIME SNAPSHOT AFTER setup-node & setup-python ==="
            echo "--- PATH ---"
            echo "$PATH"
            echo "--- which node/npm/python/pip ---"
            which node || true
            which npm || true
            which python || true
            which python3 || true
            which pip || true
            which pip3 || true
            echo "--- versions ---"
            node --version || true
            npm --version || true
            python --version || python3 --version || true
            pip --version || pip3 --version || true
            echo "--- effective versions ---"
            echo "NODE_VERSION_EFFECTIVE=${{ inputs.node_version != '' && inputs.node_version || env.NODE_VERSION_FALLBACK }}"
            echo "PY_VERSION_EFFECTIVE=${{ inputs.python_version != '' && inputs.python_version || env.PY_VERSION_FALLBACK }}"
          } | tee "${LOG_DIR}/01_runtime_snapshot.txt"
      - name: Start PostgreSQL container
        shell: bash
        continue-on-error: true
        run: |
          source /tmp/echo_helpers.sh
          safe "docker.pull.pg" docker pull "${PG_IMAGE}"
          docker rm -f "${PG_CONTAINER}" >/dev/null 2>&1 || true
          runlog "docker.run.pg" docker run -d --name "${PG_CONTAINER}" \
            -e POSTGRES_PASSWORD="${PG_PASS}" \
            -e POSTGRES_DB="${PG_DB}" \
            -p "${PG_PORT}:5432" "${PG_IMAGE}"
          for i in {1..30}; do
            if docker exec "${PG_CONTAINER}" pg_isready -U "${PG_USER}" >/dev/null 2>&1; then
              OK "Postgres is ready"
              break
            fi
            sleep 2
          done
          docker exec -i "${PG_CONTAINER}" psql -U "${PG_USER}" -d "${PG_DB}" \
            -c "SELECT NOW() AS pg_now, current_database() AS db, current_user AS usr;" \
            | tee "${LOG_DIR}/pg_healthcheck.log" || true
          echo "${PG_DSN}" > "$ARTIFACT_DIR/pg_dsn.txt"
          docker ps -a | tee "${LOG_DIR}/pg_container_status.log" || true
      - name: Generate CSV (대량·RFC4180)
        shell: bash
        continue-on-error: true
        env:
          CSV_ROWS_EFF: ${{ inputs.csv_rows != '' && inputs.csv_rows || env.CSV_ROWS_INPUT || '20000' }}
        run: |
          source /tmp/echo_helpers.sh
          CSV="$ARTIFACT_DIR/transactions_bulk.csv"
          echo "id,customer_id,bank_id,account_id,merchant_id,ts,memo,amount" > "$CSV"
          rows=${CSV_ROWS_EFF:-20000}
          for i in $(seq 1 $rows); do
            cust=$(( (i % 100) + 1 ))
            bank=$(( (i % 6) + 1 ))
            acc=$(( (i % 500) + 1 ))
            mer=$(( (i % 50) + 1 ))
            amt=$(( (i % 100000) + 100 ))
            memo="seed-${i}"
            if (( i % 200 == 0 )); then memo="\"memo, with, commas ${i}\""; fi
            if (( i % 333 == 0 )); then memo="\"quote \"\"inside\"\" ${i}\""; fi
            echo "$i,$cust,$bank,$acc,$mer,$(date -u +%Y-%m-%dT%H:%M:%SZ),$memo,$amt" >> "$CSV"
          done
          OK "Generated CSV: $CSV (rows=$rows)"
          ls -lh "$CSV" | tee -a "${LOG_DIR}/csv_gen.log" || true
          head -n 5 "$CSV" | tee -a "${LOG_DIR}/csv_gen.log" || true
          tail -n 5 "$CSV" | tee -a "${LOG_DIR}/csv_gen.log" || true
      - name: Generate migrations (도메인/권한/파티션/잡)
        shell: bash
        continue-on-error: true
        run: |
          source /tmp/echo_helpers.sh
          # 00_common schema
          cat > "$MCP_DIR/migrations/00_common/001_core.sql" <<'SQL'
          CREATE TABLE IF NOT EXISTS customers(
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL,
            email TEXT
          );
          CREATE TABLE IF NOT EXISTS banks(
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL,
            bic  TEXT
          );
          CREATE TABLE IF NOT EXISTS accounts(
            id INTEGER PRIMARY KEY,
            customer_id INTEGER NOT NULL,
            bank_id INTEGER NOT NULL,
            number TEXT NOT NULL,
            created_at TEXT NOT NULL,
            FOREIGN KEY(customer_id) REFERENCES customers(id),
            FOREIGN KEY(bank_id) REFERENCES banks(id)
          );
          CREATE TABLE IF NOT EXISTS merchants(
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL
          );
          CREATE TABLE IF NOT EXISTS transactions(
            id INTEGER PRIMARY KEY,
            customer_id INTEGER NOT NULL,
            bank_id INTEGER NOT NULL,
            account_id INTEGER NOT NULL,
            merchant_id INTEGER,
            ts TEXT NOT NULL,
            memo TEXT,
            amount INTEGER NOT NULL,
            FOREIGN KEY(customer_id) REFERENCES customers(id),
            FOREIGN KEY(bank_id) REFERENCES banks(id),
            FOREIGN KEY(account_id) REFERENCES accounts(id),
            FOREIGN KEY(merchant_id) REFERENCES merchants(id)
          );
          SQL
          # post bulk common indexes
          cat > "$MCP_DIR/migrations/10_post_bulk/common/030_indexes.sql" <<'SQL'
          CREATE INDEX IF NOT EXISTS idx_tx_customer ON transactions(customer_id);
          CREATE INDEX IF NOT EXISTS idx_tx_account  ON transactions(account_id);
          CREATE INDEX IF NOT EXISTS idx_tx_bank     ON transactions(bank_id);
          CREATE INDEX IF NOT EXISTS idx_tx_amount   ON transactions(amount);
          CREATE INDEX IF NOT EXISTS idx_tx_ts       ON transactions(ts);
          SQL
          # pg-specific roles, partitions, matviews, cron
          cat > "$MCP_DIR/migrations/10_post_bulk/pg/020_roles_privs.sql" <<'SQL'
          DO $$
          BEGIN
            IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname='finance_admin') THEN CREATE ROLE finance_admin LOGIN; END IF;
            IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname='finance_writer') THEN CREATE ROLE finance_writer; END IF;
            IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname='finance_reader') THEN CREATE ROLE finance_reader; END IF;
          END$$;
          GRANT CONNECT ON DATABASE mcpdb TO finance_reader, finance_writer, finance_admin;
          GRANT USAGE ON SCHEMA public TO finance_reader, finance_writer, finance_admin;
          GRANT SELECT ON ALL TABLES IN SCHEMA public TO finance_reader;
          GRANT SELECT,INSERT,UPDATE,DELETE ON ALL TABLES IN SCHEMA public TO finance_writer;
          GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO finance_admin;
          ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO finance_reader;
          ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT,INSERT,UPDATE,DELETE ON TABLES TO finance_writer;
          ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL PRIVILEGES ON TABLES TO finance_admin;
          SQL
          cat > "$MCP_DIR/migrations/10_post_bulk/pg/025_sequences.sql" <<'SQL'
          DO $$
          BEGIN
            IF NOT EXISTS (SELECT 1 FROM pg_class WHERE relname='transactions_id_seq') THEN
              CREATE SEQUENCE transactions_id_seq START 1 OWNED BY transactions.id;
              ALTER TABLE transactions ALTER COLUMN id SET DEFAULT nextval('transactions_id_seq');
            END IF;
          END $$;
          SQL
          cat > "$MCP_DIR/migrations/10_post_bulk/pg/040_partitioning.sql" <<'SQL'
          DO $$
          BEGIN
            IF NOT EXISTS (
              SELECT 1 FROM pg_class c JOIN pg_namespace n ON n.oid=c.relnamespace
              WHERE c.relname='transactions_p' AND n.nspname='public'
            ) THEN
              CREATE TABLE transactions_p(
                id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                customer_id INTEGER NOT NULL,
                bank_id INTEGER NOT NULL,
                account_id INTEGER NOT NULL,
                merchant_id INTEGER,
                ts TIMESTAMP NOT NULL,
                memo TEXT,
                amount INTEGER NOT NULL
              ) PARTITION BY RANGE (ts);
            END IF;
          END$$;
          DO $$
          DECLARE
            start_ts DATE := date_trunc('month', now())::date;
            end_ts   DATE := (date_trunc('month', now()) + interval '1 month')::date;
            pname    TEXT := 'transactions_p_' || to_char(start_ts, 'YYYYMM');
          BEGIN
            IF NOT EXISTS (SELECT 1 FROM pg_class WHERE relname=pname) THEN
              EXECUTE format('CREATE TABLE %I PARTITION OF transactions_p FOR VALUES FROM (%L) TO (%L);',
                             pname, start_ts, end_ts);
            END IF;
          END$$;
          SQL
          cat > "$MCP_DIR/migrations/10_post_bulk/pg/050_trigger_route.sql" <<'SQL'
          CREATE OR REPLACE FUNCTION trg_route_transactions() RETURNS trigger AS $$
          BEGIN
            INSERT INTO transactions_p(customer_id, bank_id, account_id, merchant_id, ts, memo, amount)
              VALUES (
                NEW.customer_id,
                NEW.bank_id,
                NEW.account_id,
                NEW.merchant_id,
                COALESCE(NEW.ts::timestamp, now()),
                NEW.memo,
                NEW.amount
              );
            RETURN NULL;
          END
          $$ LANGUAGE plpgsql;
          DROP TRIGGER IF EXISTS route_transactions ON transactions;
          CREATE TRIGGER route_transactions
            BEFORE INSERT ON transactions
            FOR EACH ROW EXECUTE FUNCTION trg_route_transactions();
          SQL
          cat > "$MCP_DIR/migrations/10_post_bulk/pg/060_views.sql" <<'SQL'
          CREATE OR REPLACE VIEW v_tx_agg_by_customer AS
          SELECT customer_id, COUNT(*) AS cnt, SUM(amount) AS sum
          FROM (
            SELECT customer_id, amount FROM transactions
            UNION ALL
            SELECT customer_id, amount FROM transactions_p
          ) t
          GROUP BY customer_id
          ORDER BY customer_id;
          CREATE MATERIALIZED VIEW IF NOT EXISTS mv_tx_agg_by_customer AS
          SELECT customer_id, COUNT(*) AS cnt, SUM(amount) AS sum
          FROM (
            SELECT customer_id, amount FROM transactions
            UNION ALL
            SELECT customer_id, amount FROM transactions_p
          ) t
          GROUP BY customer_id
          WITH NO DATA;
          SQL
          cat > "$MCP_DIR/migrations/10_post_bulk/pg/070_monthly_partition_job.sql" <<'SQL'
          CREATE OR REPLACE FUNCTION ensure_next_month_partition() RETURNS void AS $$
          DECLARE
            start_ts DATE := (date_trunc('month', now()) + interval '1 month')::date;
            end_ts   DATE := (date_trunc('month', now()) + interval '2 month')::date;
            pname    TEXT := 'transactions_p_' || to_char(start_ts, 'YYYYMM');
          BEGIN
            IF NOT EXISTS (SELECT 1 FROM pg_class WHERE relname=pname) THEN
              EXECUTE format('CREATE TABLE %I PARTITION OF transactions_p FOR VALUES FROM (%L) TO (%L);',
                             pname, start_ts, end_ts);
            END IF;
          END
          $$ LANGUAGE plpgsql;
          DO $$
          BEGIN
            PERFORM 1 FROM pg_extension WHERE extname='pg_cron';
            IF NOT FOUND THEN
              RAISE NOTICE 'pg_cron not installed; CI will call ensure_next_month_partition() once.';
            ELSE
              PERFORM cron.schedule(
                'ensure_next_month_partition_job',
                '10 0 1 * *',
                $$SELECT ensure_next_month_partition();$$
              ) ON CONFLICT DO NOTHING;
            END IF;
          EXCEPTION WHEN others THEN
            NULL;
          END$$;
          SQL
      - name: Generate MCP server/client/benchmark/report code
        shell: bash
        continue-on-error: true
        env:
          BATCH_SIZE_ENV: ${{ inputs.batch_size != '' && inputs.batch_size || env.BATCH_SIZE_INPUT || '2000' }}
        run: |
          source /tmp/echo_helpers.sh
          # package.json
          cat > "$MCP_DIR/package.json" <<'JSON'
          {
            "name": "mcp-sqlite-pg-mega",
            "version": "0.5.1",
            "private": true,
            "type": "module",
            "scripts": {
              "start": "node server.js",
              "test:sqlite": "node test_client.js sqlite",
              "test:pg": "node test_client.js pg",
              "bench": "node benchmark.js",
              "report": "node report.js",
              "check:deps": "node -e \"require('sql.js'); require('pg'); require('pg-copy-streams'); console.log('deps ok')\""
            },
            "dependencies": {
              "pg": "^8.12.0",
              "pg-copy-streams": "^6.0.5",
              "sql.js": "^1.11.0"
            }
          }
          JSON
          # server.js
          cat > "$MCP_DIR/server.js" <<'JS'
          import initSqlJs from "sql.js";
          import fs from "node:fs";
          import { EOL } from "node:os";
          import path from "node:path";
          import { Client as PG } from "pg";
          import { from as copyFrom } from "pg-copy-streams";
          let engine = null;
          let sqlite = { SQL: null, db: null, path: null };
          let pg = { client: null, tx: false };
          const send = (result, id=null, error=null) => {
            const payload = { jsonrpc:"2.0", ...(id!==null?{id}:{}) , ...(error?{error}:{result}) };
            process.stdout.write(JSON.stringify(payload)+EOL);
          };
          const listTools = () => ({
            tools: [
              { name:"db.setup",        description:"Open DB (engine='sqlite'|'pg')", input_schema:{ type:"object", properties:{ engine:{type:"string"}, sqlite_path:{type:"string"}, pg_dsn:{type:"string"} }, required:["engine"] } },
              { name:"sql.exec",        description:"Execute SQL", input_schema:{ type:"object", properties:{ sql:{type:"string"}, params:{type:"array"} }, required:["sql"] } },
              { name:"tx.begin",        description:"Begin transaction" },
              { name:"tx.commit",       description:"Commit transaction" },
              { name:"tx.rollback",     description:"Rollback transaction" },
              { name:"bulk.load.csv",   description:"Load CSV with streaming (and PG COPY option)", input_schema:{ type:"object", properties:{ csv_path:{type:"string"}, batch_size:{type:"integer"}, use_copy:{type:"boolean"} }, required:["csv_path"] } },
              { name:"migrate.apply",   description:"Apply .sql migrations (pre/post separated)", input_schema:{ type:"object", properties:{ dir:{type:"string"} }, required:["dir"] } }
            ]
          });
          async function openDb({engine:eng, sqlite_path, pg_dsn}) {
            engine = eng;
            if (engine === "sqlite") {
              if (!sqlite.SQL) sqlite.SQL = await initSqlJs();
              sqlite.path = sqlite_path;
              let dbData = null;
              if (sqlite_path && fs.existsSync(sqlite_path)) dbData = fs.readFileSync(sqlite_path);
              sqlite.db = new sqlite.SQL.Database(dbData);
              return { ok:true, engine };
            } else if (engine === "pg") {
              pg.client = new PG({ connectionString: pg_dsn });
              await pg.client.connect();
              return { ok:true, engine };
            } else {
              throw new Error("Unsupported engine");
            }
          }
          async function runSql(sql, params=[]) {
            if (engine === "sqlite") {
              const q = sql.trim().toLowerCase();
              if (q.startsWith("select")) {
                const res = sqlite.db.exec(sql);
                const rows = res.length ? res[0].values.map(r =>
                  Object.fromEntries(res[0].columns.map((c,i)=>[c,r[i]]))
                ) : [];
                return { rows };
              } else {
                sqlite.db.run(sql, params);
                if (sqlite.path) {
                  const data = sqlite.db.export();
                  fs.writeFileSync(sqlite.path, Buffer.from(data));
                }
                return { changes: null, lastID: null };
              }
            } else if (engine === "pg") {
              const r = await pg.client.query(sql, params);
              return { rows: r.rows, rowCount: r.rowCount };
            }
            throw new Error("DB not initialized");
          }
          async function txBegin(){
            if (engine==="sqlite") return runSql("BEGIN");
            if (engine==="pg") {
              await pg.client.query("BEGIN");
              pg.tx=true;
              return {ok:true};
            }
          }
          async function txCommit(){
            if (engine==="sqlite") return runSql("COMMIT");
            if (engine==="pg"&&pg.tx){
              await pg.client.query("COMMIT");
              pg.tx=false;
              return {ok:true};
            }
          }
          async function txRollback(){
            if (engine==="sqlite") return runSql("ROLLBACK");
            if (engine==="pg"&&pg.tx){
              await pg.client.query("ROLLBACK");
              pg.tx=false;
              return {ok:true};
            }
          }
          async function migrateApply(dir){
            const applyDir = async (d) => {
              if (!fs.existsSync(d)) return;
              const files = fs.readdirSync(d).filter(f=>f.endsWith(".sql")).sort();
              for (const f of files){
                const sql = fs.readFileSync(path.join(d,f), "utf8");
                await runSql(sql);
              }
            };
            await applyDir(path.join(dir, "00_common"));
            return { ok:true };
          }
          function parseCSVLine(line){
            const out = [];
            let cur = "";
            let inQ = false;
            let i = 0;
            while (i < line.length){
              const ch = line[i];
              if (inQ){
                if (ch === '"'){
                  if (i+1 < line.length && line[i+1] === '"'){
                    cur += '"'; i+=2; continue;
                  }
                  inQ = false; i++; continue;
                }
                cur += ch; i++; continue;
              } else {
                if (ch === '"'){ inQ = true; i++; continue; }
                if (ch === ','){ out.push(cur); cur=""; i++; continue; }
                cur += ch; i++; continue;
              }
            }
            out.push(cur);
            return out;
          }
          function *readLines(filePath){
            const buf = fs.readFileSync(filePath, "utf8");
            yield* buf.split(/\r?\n/);
          }
          async function bulkLoadCsv(csv_path, batch_size=1000, use_copy=false){
            const iter = readLines(csv_path);
            const header = iter.next().value;
            if (!header) return { ok:true, loaded:0 };
            const cols = parseCSVLine(header);
            const idx = Object.fromEntries(cols.map((c,i)=>[c,i]));
            if (engine === "pg" && use_copy){
              await pg.client.query("BEGIN");
              try{
                const stream = pg.client.query(copyFrom(
                  `COPY transactions(customer_id,bank_id,account_id,merchant_id,ts,memo,amount)
                   FROM STDIN WITH (FORMAT csv)`
                ));
                for (const line of readLines(csv_path)){
                  if (!line || !line.trim() || line === header) continue;
                  const arr = parseCSVLine(line);
                  const rec = [
                    "customer_id","bank_id","account_id","merchant_id",
                    "ts","memo","amount"
                  ].map(k=>arr[idx[k]] ?? "");
                  stream.write(rec.map(v => (v==null?"":String(v))).join(",") + "\n");
                }
                await new Promise((res,rej)=>{
                  stream.on("finish",res);
                  stream.on("error",rej);
                  stream.end();
                });
                await pg.client.query("COMMIT");
                return { ok:true };
              } catch(e){
                await pg.client.query("ROLLBACK");
                throw e;
              }
            }
            let loaded = 0, batch = 0;
            await txBegin();
            try{
              for (const line of readLines(csv_path)){
                if (!line || !line.trim() || line === header) continue;
                const c = parseCSVLine(line);
                const customer_id = Number(c[idx["customer_id"]]);
                const bank_id     = Number(c[idx["bank_id"]]);
                const account_id  = Number(c[idx["account_id"]]);
                const merchant_id = Number(c[idx["merchant_id"]]);
                const ts          = c[idx["ts"]];
                const memo        = c[idx["memo"]];
                const amount      = Number(c[idx["amount"]]);
                if (engine === "sqlite") {
                  await runSql(
                    `INSERT OR IGNORE INTO customers(id,name,email)
                     VALUES(${customer_id}, 'cust-${customer_id}', 'c${customer_id}@ex')`
                  );
                  await runSql(
                    `INSERT OR IGNORE INTO banks(id,name,bic)
                     VALUES(${bank_id}, 'bank-${bank_id}', 'BIC${bank_id}')`
                  );
                  await runSql(
                    `INSERT OR IGNORE INTO accounts(id,customer_id,bank_id,number,created_at)
                     VALUES(${account_id}, ${customer_id}, ${bank_id},
                            'ACCT-${account_id}', datetime('now'))`
                  );
                  await runSql(
                    `INSERT OR IGNORE INTO merchants(id,name)
                     VALUES(${merchant_id}, 'mer-${merchant_id}')`
                  );
                  await runSql(`
                    INSERT INTO transactions(id,customer_id,bank_id,account_id,merchant_id,ts,memo,amount)
                    VALUES(
                      NULL,
                      ${customer_id},
                      ${bank_id},
                      ${account_id},
                      ${merchant_id},
                      '${ts}',
                      '${(memo||'').replaceAll("'", "''")}',
                      ${amount}
                    )
                  `);
                } else {
                  await runSql(
                    `INSERT INTO customers(id,name,email)
                     VALUES($1,$2,$3)
                     ON CONFLICT (id) DO NOTHING`,
                    [customer_id, `cust-${customer_id}`, `c${customer_id}@ex`]
                  );
                  await runSql(
                    `INSERT INTO banks(id,name,bic)
                     VALUES($1,$2,$3)
                     ON CONFLICT (id) DO NOTHING`,
                    [bank_id, `bank-${bank_id}`, `BIC${bank_id}`]
                  );
                  await runSql(
                    `INSERT INTO accounts(id,customer_id,bank_id,number,created_at)
                     VALUES($1,$2,$3,$4,NOW())
                     ON CONFLICT (id) DO NOTHING`,
                    [account_id, customer_id, bank_id, `ACCT-${account_id}`]
                  );
                  await runSql(
                    `INSERT INTO merchants(id,name)
                     VALUES($1,$2)
                     ON CONFLICT (id) DO NOTHING`,
                    [merchant_id, `mer-${merchant_id}`]
                  );
                  await runSql(
                    `INSERT INTO transactions(customer_id,bank_id,account_id,merchant_id,ts,memo,amount)
                     VALUES($1,$2,$3,$4,$5,$6,$7)`,
                    [customer_id, bank_id, account_id, merchant_id, ts, memo, amount]
                  );
                }
                loaded++; batch++;
                if (batch >= batch_size){
                  await txCommit();
                  await txBegin();
                  batch = 0;
                }
              }
              await txCommit();
              return { ok:true, loaded };
            } catch(e){
              await txRollback();
              throw e;
            }
          }
          export function createInProc(){
            return {
              call: async (name, args={}) => {
                if (name==="db.setup")       return openDb(args);
                if (name==="sql.exec")       return runSql(args.sql, args.params||[]);
                if (name==="tx.begin")       return txBegin();
                if (name==="tx.commit")      return txCommit();
                if (name==="tx.rollback")    return txRollback();
                if (name==="bulk.load.csv")  return bulkLoadCsv(args.csv_path, args.batch_size||1000, !!args.use_copy);
                if (name==="migrate.apply")  return migrateApply(args.dir);
                if (name==="tools.list")     return listTools();
                throw new Error("Unknown tool");
              }
            };
          }
          if (import.meta.url === `file://${process.argv[1]}`) {
            let buffer = "";
            process.stdin.setEncoding("utf8");
            process.stdin.on("data", async chunk => {
              buffer += chunk;
              const lines = buffer.split(/\r?\n/);
              buffer = lines.pop();
              for (const line of lines) {
                if (!line.trim()) continue;
                let msg;
                try {
                  msg=JSON.parse(line);
                } catch {
                  send(null,null,{code:-32700,message:"Parse error"});
                  continue;
                }
                const {id=null, method, params={}} = msg;
                try {
                  if (method === "tools/list")
                    return send(listTools(), id);
                  if (method === "tools/call") {
                    const { name, arguments: args={} } = params;
                    const inproc = createInProc();
                    const result = await inproc.call(name, args);
                    return send(result, id);
                  }
                  if (method === "ping")
                    return send({ pong:true }, id);
                  return send(null, id, { code:-32601, message:"Method not found" });
                } catch (err) {
                  return send(null, id, { code:-32000, message:String(err) });
                }
              }
            });
          }
          JS
          # test_client.js
          cat > "$MCP_DIR/test_client.js" <<'JS'
          import { createInProc } from "./server.js";
          const mode = process.argv[2] || "sqlite";
          const DB_PATH = process.env.DB_PATH || ".github/echo_artifacts/mcp_demo.sqlite";
          const CSV = process.env.CSV_FILE || ".github/echo_artifacts/transactions_bulk.csv";
          const MIG_ROOT = process.env.MIG_ROOT || ".github/mcp_demo/migrations";
          const PG_DSN = process.env.PG_DSN;
          const BATCH = parseInt(process.env.BATCH_SIZE || "2000", 10);
          const USE_COPY = (process.env.USE_COPY === "true");
          const mcp = createInProc();
          const log = (label, obj) => {
            console.log(`--- ${mode} :: ${label} ---`);
            console.log(JSON.stringify(obj, null, 2));
          };
          (async () => {
            let r;
            if (mode === "sqlite") {
              r = await mcp.call("db.setup", {
                engine:"sqlite",
                sqlite_path: DB_PATH
              });
            } else {
              r = await mcp.call("db.setup", {
                engine:"pg",
                pg_dsn: PG_DSN
              });
            }
            log("db.setup", r);
            r = await mcp.call("migrate.apply", { dir: `${MIG_ROOT}` });
            log("migrate.apply.pre", r);
            // seed rows
            for (let i=1;i<=5;i++){
              const sqls = [
                `INSERT INTO customers(id,name,email) VALUES(${i},'cust-${i}','c${i}@ex') ON CONFLICT DO NOTHING`,
                `INSERT INTO banks(id,name,bic) VALUES(${i},'bank-${i}','BIC${i}') ON CONFLICT DO NOTHING`,
                `INSERT INTO accounts(id,customer_id,bank_id,number,created_at)
                 VALUES(${i},${i},${i},'ACCT-${i}',datetime('now')) ON CONFLICT DO NOTHING`,
                `INSERT INTO merchants(id,name) VALUES(${i},'mer-${i}') ON CONFLICT DO NOTHING`
              ];
              for (const s of sqls){
                if (mode === "sqlite") {
                  await mcp.call("sql.exec", { sql: s.replace(" ON CONFLICT DO NOTHING","") });
                } else {
                  await mcp.call("sql.exec", { sql: s });
                }
              }
              const t = (mode==="sqlite")
                ? `INSERT INTO transactions(customer_id,bank_id,account_id,merchant_id,ts,memo,amount)
                   VALUES(${i},${i},${i},${i},datetime('now'),'seed',${1000+i})`
                : `INSERT INTO transactions(customer_id,bank_id,account_id,merchant_id,ts,memo,amount)
                   VALUES(${i},${i},${i},${i},NOW()::text,'seed',${1000+i})`;
              await mcp.call("sql.exec", { sql: t });
            }
            // tx + rollback test
            await mcp.call("tx.begin");
            const before = await mcp.call("sql.exec", {
              sql:`SELECT COUNT(*) AS c FROM transactions`
            });
            let err=false;
            try{
              await mcp.call("sql.exec", {
                sql:`INSERT INTO transactions(customer_id,bank_id,account_id,merchant_id,ts,memo,amount,nope)
                     VALUES(1,1,1,1,datetime('now'),'bad',123,0)`
              });
            }catch(e){
              err=true;
            }
            await mcp.call("tx.rollback");
            const after = await mcp.call("sql.exec", {
              sql:`SELECT COUNT(*) AS c FROM transactions`
            });
            log("tx.beforeCount", before);
            log("tx.afterCount", after);
            log("tx.rollbackTriggered", {err});
            // bulk load CSV
            r = await mcp.call("bulk.load.csv", {
              csv_path: CSV,
              batch_size: BATCH,
              use_copy: (mode==="pg" && USE_COPY)
            });
            log("bulk.load.csv", r);
            // post bulk migrations
            const COMMON_POST = `${MIG_ROOT}/10_post_bulk/common`;
            await mcp.call("migrate.apply", { dir: COMMON_POST });
            if (mode === "pg"){
              const PG_POST = `${MIG_ROOT}/10_post_bulk/pg`;
              await mcp.call("migrate.apply", { dir: PG_POST });
              await mcp.call("sql.exec", {
                sql: `REFRESH MATERIALIZED VIEW CONCURRENTLY mv_tx_agg_by_customer;`
              }).catch(()=>mcp.call("sql.exec",{
                sql:`REFRESH MATERIALIZED VIEW mv_tx_agg_by_customer;`
              }));
              await mcp.call("sql.exec", {
                sql: `SELECT ensure_next_month_partition();`
              }).catch(()=>{});
            }
            const sel = await mcp.call("sql.exec", {
              sql:`SELECT customer_id, COUNT(*) AS cnt, SUM(amount) AS sum
                   FROM transactions
                   GROUP BY customer_id
                   ORDER BY customer_id
                   LIMIT 5`
            });
            log("select.group", sel);
          })().catch(e => {
            console.error("CLIENT ERROR:", e);
            process.exit(0);
          });
          JS
          # benchmark.js
          cat > "$MCP_DIR/benchmark.js" <<'JS'
          import { createInProc } from "./server.js";
          const mode = process.argv[2] || "pg";
          const DB_PATH = process.env.DB_PATH || ".github/echo_artifacts/mcp_demo.sqlite";
          const PG_DSN = process.env.PG_DSN;
          const mcp = createInProc();
          const cases = [
            { name:"count_all",     sql:"SELECT COUNT(*) FROM transactions" },
            { name:"sum_amount",    sql:"SELECT SUM(amount) FROM transactions" },
            { name:"group_customer",sql:"SELECT customer_id, COUNT(*) c, SUM(amount) s FROM transactions GROUP BY customer_id" }
          ];
          function nowMs(){ return Number(process.hrtime.bigint() / 1000000n); }
          (async()=>{
            if (mode==="sqlite") {
              await mcp.call("db.setup", {
                engine:"sqlite",
                sqlite_path: DB_PATH
              });
            } else {
              await mcp.call("db.setup", {
                engine:"pg",
                pg_dsn: PG_DSN
              });
            }
            const out = [];
            for (const c of cases){
              const t0=nowMs();
              await mcp.call("sql.exec", { sql:c.sql });
              const t1=nowMs();
              out.push({ name:c.name, ms: t1-t0 });
            }
            if (mode==="pg"){
              const t0=nowMs();
              await mcp.call("sql.exec", {
                sql:"SELECT * FROM v_tx_agg_by_customer LIMIT 1000"
              });
              const t1=nowMs();
              out.push({ name:"view_agg", ms:t1-t0 });
              const t2=nowMs();
              await mcp.call("sql.exec", {
                sql:"SELECT * FROM mv_tx_agg_by_customer LIMIT 1000"
              });
              const t3=nowMs();
              out.push({ name:"matview_agg", ms:t3-t2 });
            }
            console.log(JSON.stringify(out, null, 2));
          })().catch(e=>{
            console.error(e);
            process.exit(1);
          });
          JS
          # report.js
          cat > "$MCP_DIR/report.js" <<'JS'
          import fs from "node:fs";
          import path from "node:path";
          const LOG_DIR = process.env.LOG_DIR || ".github/echo_logs";
          const OUT_DIR = process.env.ARTIFACT_DIR || ".github/echo_artifacts";
          function tail(p, n=80){
            if (!fs.existsSync(p)) return "";
            const lines = fs.readFileSync(p, "utf8").trimEnd().split(/\r?\n/);
            return lines.slice(-n).join("\n");
          }
          function renderSVGBar(data){
            const W=600, H=220, pad=30;
            const max = Math.max(...data.map(d=>d.ms), 1);
            const barW = (W - pad*2) / data.length - 10;
            let svg = `<svg width="${W}" height="${H}" viewBox="0 0 ${W} ${H}" xmlns="http://www.w3.org/2000/svg">`;
            svg += `<rect width="${W}" height="${H}" fill="#fff"/>`;
            data.forEach((d,i)=>{
              const h = Math.max(4, (H - pad*2) * d.ms / max);
              const x = pad + i*(barW+10);
              const y = H - pad - h;
              svg += `<rect x="${x}" y="${y}" width="${barW}" height="${h}" fill="#4e79a7"/>`;
              svg += `<text x="${x+barW/2}" y="${H-pad+14}" font-size="11" text-anchor="middle">${d.name}</text>`;
              svg += `<text x="${x+barW/2}" y="${y-4}" font-size="11" text-anchor="middle">${d.ms}ms</text>`;
            });
            svg += `</svg>`;
            return svg;
          }
          const sections = [
            { title: "SQLite Client Log (tail)", file: "sqlite_client.log" },
            { title: "Postgres Client Log (tail)", file: "pg_client.log" },
            { title: "PG Tables", file: "pg_tables.log" },
            { title: "PG Count", file: "pg_count.log" },
            { title: "Summary", file: "summary.txt" }
          ];
          let md = `# MCP MEGA++++ Run Report\n\n`;
          let html = `<!doctype html><meta charset="utf-8"><title>MCP MEGA++++ Report</title><style>body{font-family:system-ui,Segoe UI,Arial,sans-serif;line-height:1.45;padding:24px;} pre{background:#f6f8fa;padding:12px;overflow:auto;border-radius:8px;} h2{margin-top:28px}</style><h1>MCP MEGA++++ Run Report</h1>`;
          for (const s of sections){
            const p = path.join(LOG_DIR, s.file);
            const content = tail(p, 120);
            md += `\n## ${s.title}\n\n\`\`\`\n${content}\n\`\`\`\n`;
            html += `<h2>${s.title}</h2><pre>${content.replace(/[&<>]/g, m=>({ "&":"&amp;","<":"&lt;",">":"&gt;" }[m]))}</pre>`;
          }
          const benchPgPath = path.join(OUT_DIR, "benchmark_pg.json");
          const benchSqlitePath = path.join(OUT_DIR, "benchmark_sqlite.json");
          const pgData = fs.existsSync(benchPgPath) ? JSON.parse(fs.readFileSync(benchPgPath,"utf8")) : null;
          const sqliteData = fs.existsSync(benchSqlitePath) ? JSON.parse(fs.readFileSync(benchSqlitePath,"utf8")) : null;
          if (pgData){
            html += `<h2>Benchmark (Postgres)</h2>${renderSVGBar(pgData)}`;
            md += `\n## Benchmark (Postgres)\n\n\`\`\`json\n${JSON.stringify(pgData,null,2)}\n\`\`\`\n`;
          }
          if (sqliteData){
            html += `<h2>Benchmark (SQLite sql.js)</h2>${renderSVGBar(sqliteData)}`;
            md += `\n## Benchmark (SQLite sql.js)\n\n\`\`\`json\n${JSON.stringify(sqliteData,null,2)}\n\`\`\`\n`;
          }
          fs.writeFileSync(path.join(OUT_DIR, "report.md"), md);
          fs.writeFileSync(path.join(OUT_DIR, "report.html"), html);
          console.log("Report generated:", path.join(OUT_DIR, "report.html"));
          JS
          # mcp.config.json
          cat > "$MCP_DIR/mcp.config.json" <<JSON
          {
            "mcpServers": {
              "db-mega": {
                "command": "node",
                "args": ["server.js"],
                "cwd": "${MCP_DIR}"
              }
            }
          }
          JSON
          pushd "$MCP_DIR" >/dev/null
          if [ ! -f package-lock.json ]; then
            runlog "npm.install" bash -lc 'npm install --no-audit --no-fund'
          else
            runlog "npm.install" bash -lc '(npm ci || npm install --no-audit --no-fund)'
          fi
          runlog "npm.check.deps" bash -lc 'npm run -s check:deps || true'
          popd >/dev/null
      - name: Ensure DB parent dir (SQLite persistence path)
        shell: bash
        continue-on-error: true
        env:
          DB_PATH_FALLBACK: ".github/echo_artifacts/mcp_demo.sqlite"
        run: |
          source /tmp/echo_helpers.sh
          DB_PATH_EFF="${{ inputs.db_path != '' && inputs.db_path || env.DB_PATH_INPUT || env.DB_PATH_FALLBACK }}"
          mkdir -p "$(dirname "$DB_PATH_EFF")"
          OK "SQLite DB parent dir ensured: $DB_PATH_EFF"
          echo "$DB_PATH_EFF" > "$LOG_DIR/db_path_effective.txt"
      - name: Run SQLite scenario
        shell: bash
        continue-on-error: true
        env:
          DB_PATH: ${{ inputs.db_path != '' && inputs.db_path || env.DB_PATH_INPUT || '.github/echo_artifacts/mcp_demo.sqlite' }}
          CSV_FILE: ${{ env.ARTIFACT_DIR }}/transactions_bulk.csv
          MIG_ROOT: ${{ env.MCP_DIR }}/migrations
          BATCH_SIZE: ${{ inputs.batch_size != '' && inputs.batch_size || env.BATCH_SIZE_INPUT || '2000' }}
          USE_COPY: "false"
        run: |
          source /tmp/echo_helpers.sh
          pushd "$MCP_DIR" >/dev/null
          set +e
          node test_client.js sqlite >"../.."/$LOG_DIR/sqlite_client.log 2>&1
          ec=$?
          if [ $ec -ne 0 ]; then
            FAIL "sqlite.client: FAILED (exit=$ec)"
            tail -n 200 "../.."/$LOG_DIR/sqlite_client.log || true
          else
            OK "sqlite.client: completed (exit=0)"
          fi
          node benchmark.js sqlite >"../.."/$ARTIFACT_DIR/benchmark_sqlite.json 2>>"../.."/$LOG_DIR/sqlite_client.log || true
          set -e
          popd >/dev/null
      - name: Run PostgreSQL scenario (COPY 옵션 반영)
        shell: bash
        continue-on-error: true
        env:
          PG_DSN: ${{ env.PG_DSN }}
          CSV_FILE: ${{ env.ARTIFACT_DIR }}/transactions_bulk.csv
          MIG_ROOT: ${{ env.MCP_DIR }}/migrations
          BATCH_SIZE: ${{ inputs.batch_size != '' && inputs.batch_size || env.BATCH_SIZE_INPUT || '2000' }}
          USE_COPY: ${{ inputs.use_copy != '' && inputs.use_copy || env.USE_COPY_INPUT || true }}
        run: |
          source /tmp/echo_helpers.sh
          pushd "$MCP_DIR" >/dev/null
          set +e
          node test_client.js pg >"../.."/$LOG_DIR/pg_client.log 2>&1
          ec=$?
          if [ $ec -ne 0 ]; then
            FAIL "pg.client: FAILED (exit=$ec)"
            tail -n 200 "../.."/$LOG_DIR/pg_client.log || true
          else
            OK "pg.client: completed (exit=0)"
          fi
          docker exec -i "${PG_CONTAINER}" psql -U "${PG_USER}" -d "${PG_DB}" -c "\dt" \
            > "$LOG_DIR/pg_tables.log" 2>&1 || true
          docker exec -i "${PG_CONTAINER}" psql -U "${PG_USER}" -d "${PG_DB}" \
            -c "SELECT (SELECT COUNT(*) FROM transactions) + (SELECT COUNT(*) FROM transactions_p) AS total;" \
            > "$LOG_DIR/pg_count.log" 2>&1 || true
          node benchmark.js pg >"../.."/$ARTIFACT_DIR/benchmark_pg.json 2>>"../.."/$LOG_DIR/pg_client.log || true
          set -e
          popd >/dev/null
      - name: Build HTML/Markdown report (with charts)
        shell: bash
        continue-on-error: true
        env:
          LOG_DIR: ${{ env.LOG_DIR }}
          ARTIFACT_DIR: ${{ env.ARTIFACT_DIR }}
        run: |
          source /tmp/echo_helpers.sh
          pushd "$MCP_DIR" >/dev/null
          runlog "report.build" bash -lc 'node report.js'
          popd >/dev/null
          ls -lh "$ARTIFACT_DIR/report.html" "$ARTIFACT_DIR/report.md" \
            | tee -a "${LOG_DIR}/report_build.log" || true
          head -n 50 "$ARTIFACT_DIR/report.md" \
            | tee -a "${LOG_DIR}/report_build.log" || true
          echo "=== BENCH SQLITE ===" | tee -a "${LOG_DIR}/report_build.log"
          cat "$ARTIFACT_DIR/benchmark_sqlite.json" 2>/dev/null \
            | tee -a "${LOG_DIR}/report_build.log" || true
          echo "=== BENCH PG ===" | tee -a "${LOG_DIR}/report_build.log"
          cat "$ARTIFACT_DIR/benchmark_pg.json" 2>/dev/null \
            | tee -a "${LOG_DIR}/report_build.log" || true
      - name: Summaries
        if: always()
        shell: bash
        continue-on-error: true
        run: |
          source /tmp/echo_helpers.sh
          {
            echo "=== RUN SUMMARY ($(date)) ==="
            echo "sqlite_db_effective=$(cat "$LOG_DIR/db_path_effective.txt" 2>/dev/null || echo '(n/a)')"
            echo "pg_dsn=${PG_DSN}"
            echo "csv_rows_effective=${{ inputs.csv_rows || env.CSV_ROWS_INPUT || '20000' }}"
            echo "batch_size_effective=${{ inputs.batch_size || env.BATCH_SIZE_INPUT || '2000' }}"
            echo "use_copy_effective=${{ inputs.use_copy || env.USE_COPY_INPUT || true }}"
            echo "roles: finance_admin / finance_writer / finance_reader"
            echo "partitions: transactions_p (monthly), ensure_next_month_partition()"
            echo "--- docker ps -a ---"
            docker ps -a || true
            echo "--- ss -tlnp (ports) ---"
            ss -tlnp || true
            echo "--- PATH (final) ---"
            echo "$PATH"
            echo "--- which node/python ---"
            which node || true
            which python || true
            node --version || true
            python --version || python3 --version || true
          } | tee "$LOG_DIR/summary.txt"
          OK "Summary generated"
      - name: Upload artifacts (logs, db, sources, configs, reports)
        if: always()
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: mcp-mega-artifacts
          path: |
            .github/echo_logs/**
            .github/echo_artifacts/**
            .github/mcp_demo/**
          if-no-files-found: warn
          retention-days: 7

      - name: Always success footer
        if: always()
        shell: bash
        run: |
          echo "✅ MCP MEGA++++ DONE — All steps attempted."
          echo "Artifacts snapshot:"
          ls -R .github/echo_logs || true
          ls -R .github/echo_artifacts || true
          ls -R .github/mcp_demo || true
          exit 0
  ###########################################################################
  # 2) Banking Data Pipeline — 멀티DB 매트릭스 / bad substitution fix / PATH 스냅샷
  ###########################################################################
  banking-data-pipeline:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        db: [postgres, mysql, mariadb, sqlite, duckdb]
    env:
      TZ: Asia/Seoul
      LOG_DIR: .github/echo_logs
      ARTIFACT_DIR: .github/echo_artifacts
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        continue-on-error: true

      - name: Prepare service state (pre-flight harden)
        shell: bash
        continue-on-error: true
        env:
          DB_MATRIX: ${{ matrix.db }}
        run: |
          set -Eeuo pipefail
          mkdir -p "$LOG_DIR" "$ARTIFACT_DIR" artifacts "artifacts/${DB_MATRIX}"
          touch "$LOG_DIR/.keep" "$ARTIFACT_DIR/.keep"
          : > "$LOG_DIR/pipeline_${DB_MATRIX}.log" || true
          : > "$LOG_DIR/bank_ctx_${DB_MATRIX}.log" || true
          {
            echo "=== PRE-FLIGHT SERVICE SNAPSHOT ==="
            echo "workflow=${GITHUB_WORKFLOW}"
            echo "job=banking-data-pipeline"
            echo "matrix.db=${DB_MATRIX}"
            echo "runner_uname=$(uname -a)"
            echo "pwd_now=$(pwd)"
            echo "whoami_now=$(whoami)"
            echo "TZ=$TZ"
            echo "LOG_DIR=$LOG_DIR"
            echo "ARTIFACT_DIR=$ARTIFACT_DIR"
            echo "--- INITIAL PATH ---"
            echo "$PATH"
          } | tee "${LOG_DIR}/00_preflight_banking_${DB_MATRIX}.txt"
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
        continue-on-error: true

      - name: Runtime PATH / versions snapshot (after python setup)
        shell: bash
        continue-on-error: true
        env:
          DB_MATRIX: ${{ matrix.db }}
        run: |
          {
            echo "=== banking-data-pipeline runtime snapshot (${DB_MATRIX}) ==="
            echo "--- PATH ---"
            echo "$PATH"
            echo "--- which python/pip ---"
            which python || true
            which python3 || true
            which pip || true
            which pip3 || true
            echo "--- python --version ---"
            python --version || python3 --version || true
            echo "--- pip --version ---"
            pip --version || pip3 --version || true
          } | tee -a "$LOG_DIR/bank_ctx_${DB_MATRIX}.log"
      - name: Install system clients (psql, mysqlclient)
        shell: bash
        continue-on-error: true
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y postgresql-client mysql-client || true
      - name: Install Python dependencies
        shell: bash
        continue-on-error: true
        run: |
          set -euxo pipefail
          python -m pip install --upgrade pip || python3 -m pip install --upgrade pip || true
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt || pip3 install -r requirements.txt || true
          else
            echo "requirements.txt not found; skipping."
          fi
      - name: Start database container
        env:
          DB_MATRIX: ${{ matrix.db }}
        shell: bash
        continue-on-error: true
        run: |
          set -euxo pipefail
          case "$DB_MATRIX" in
            postgres)
              docker run -d --name pgdb \
                -e POSTGRES_USER=postgres \
                -e POSTGRES_PASSWORD=postgres \
                -e POSTGRES_DB=bank \
                -p 5432:5432 postgres:16
              ;;
            mysql)
              docker run -d --name mysqldb \
                -e MYSQL_ROOT_PASSWORD=password \
                -e MYSQL_DATABASE=bank \
                -p 3306:3306 mysql:8
              ;;
            mariadb)
              docker run -d --name mariadb \
                -e MARIADB_ROOT_PASSWORD=password \
                -e MARIADB_DATABASE=bank \
                -p 3307:3306 mariadb:11
              ;;
            sqlite|duckdb)
              echo "No external DB container required for $DB_MATRIX"
              ;;
            *)
              echo "Unsupported DB $DB_MATRIX"
              exit 1
              ;;
          esac
      - name: Wait for DB readiness
        env:
          DB_MATRIX: ${{ matrix.db }}
        shell: bash
        continue-on-error: true
        run: |
          set -euxo pipefail
          if [ "$DB_MATRIX" = "postgres" ]; then
            for i in $(seq 1 60); do
              pg_isready -h localhost -p 5432 -U postgres && break || sleep 2
            done
          elif [ "$DB_MATRIX" = "mysql" ]; then
            for i in $(seq 1 90); do
              mysqladmin ping -h 127.0.0.1 -P 3306 -ppassword && break || sleep 2
            done
          elif [ "$DB_MATRIX" = "mariadb" ]; then
            for i in $(seq 1 90); do
              mysqladmin ping -h 127.0.0.1 -P 3307 -ppassword && break || sleep 2
            done
          else
            echo "No readiness wait needed for $DB_MATRIX"
          fi
          {
            echo "=== SERVICE SNAPSHOT AFTER READINESS CHECK (${DB_MATRIX}) ==="
            echo "--- docker ps -a ---"
            docker ps -a || true
            echo "--- ss -tlnp (ports) ---"
            ss -tlnp || true
            echo "--- PATH (current) ---"
            echo "$PATH"
          } | tee -a "$LOG_DIR/bank_ctx_${DB_MATRIX}.log" 2>&1 || true
      - name: Run pipeline
        env:
          DB_MATRIX: ${{ matrix.db }}
          DB_USER: ${{ matrix.db == 'postgres' && 'postgres' || 'root' }}
          DB_PASSWORD: ${{ matrix.db == 'postgres' && 'postgres' || 'password' }}
          DB_HOST: localhost
          DB_PORT: ${{ matrix.db == 'postgres' && '5432' || (matrix.db == 'mysql' && '3306' || (matrix.db == 'mariadb' && '3307' || '')) }}
          DB_NAME: bank
        shell: bash
        continue-on-error: true
        run: |
          set -euxo pipefail
          # 사용자가 제공한 banking_pipeline.pipeline 모듈 호출
          python -m banking_pipeline.pipeline \
            --target "${DB_MATRIX}" \
            --rows 20000 \
            --seed 42 \
            --out artifacts \
            | tee -a "$LOG_DIR/pipeline_${DB_MATRIX}.log" || true
          ls -R "artifacts/${DB_MATRIX}" \
            | tee -a "$LOG_DIR/pipeline_${DB_MATRIX}.log" || true
          {
            echo "=== POST PIPELINE RUNTIME (${DB_MATRIX}) ==="
            echo "--- PATH ---"
            echo "$PATH"
            echo "--- which python ---"
            which python || which python3 || true
            echo "--- python --version ---"
            python --version || python3 --version || true
          } | tee -a "$LOG_DIR/pipeline_${DB_MATRIX}.log" || true
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: reports-${{ matrix.db }}
          path: |
            .github/echo_logs/**
            .github/echo_artifacts/**
            artifacts/${{ matrix.db }}
          if-no-files-found: warn
          retention-days: 7

      - name: Always success footer (matrix ${{ matrix.db }})
        if: always()
        shell: bash
        env:
          DB_MATRIX: ${{ matrix.db }}
        run: |
          echo "✅ BANKING ($DB_MATRIX) DONE — All steps attempted."
          echo "Artifacts snapshot:"
          ls -R "$LOG_DIR" || true
          ls -R "artifacts/${DB_MATRIX}" || true
          exit 0
