name: "üß™ Claude Code ‚Äî MCP MEGA++ (SQLite=sql.js ‚Ä¢ Postgres ÌååÌã∞ÏÖîÎãù ‚Ä¢ CSV Î∞∞Ïπò ‚Ä¢ Tx/Rollback ‚Ä¢ Î¶¨Ìè¨ÌåÖ ‚Ä¢ Always-Success)"

on:
  workflow_dispatch:
    inputs:
      mode:
        type: choice
        options: [full, lite]
        default: full
      node_version:
        default: "20"
      python_version:
        default: "3.12"
      db_path:
        default: ".github/echo_artifacts/mcp_demo.sqlite"
      table_name:
        default: "transactions"
      seed_rows:
        default: "5"
      csv_rows:
        default: "10000"   # ÎåÄÎüâ ÌÖåÏä§Ìä∏ Í∏∞Î≥∏Í∞í ÏÉÅÌñ•
      batch_size:
        default: "1000"
      demo_mode:
        type: boolean
        default: true

permissions:
  contents: read

env:
  TZ: Asia/Seoul
  LOG_DIR: .github/echo_logs
  ARTIFACT_DIR: .github/echo_artifacts
  MCP_DIR: .github/mcp_demo
  PG_CONTAINER: pgtest
  PG_IMAGE: postgres:16
  PG_USER: postgres
  PG_PASS: pass
  PG_DB: mcpdb
  PG_PORT: "5432"
  PG_DSN: postgres://postgres:pass@localhost:5432/mcpdb

jobs:
  mcp-mega:
    runs-on: ubuntu-24.04
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        continue-on-error: true

      - name: Prepare dirs & echo helpers
        shell: bash
        continue-on-error: true
        run: |
          set -Eeuo pipefail
          mkdir -p "$LOG_DIR" "$ARTIFACT_DIR" "$MCP_DIR" \
                   "$MCP_DIR/migrations/00_common" \
                   "$MCP_DIR/migrations/10_post_bulk/common" \
                   "$MCP_DIR/migrations/10_post_bulk/pg"
          cat > /tmp/echo_helpers.sh <<'SH'
          #!/usr/bin/env bash
          TS(){ date "+%Y-%m-%d %H:%M:%S%z"; }
          OK(){  echo "‚úÖ [$(TS)] $*"; }
          WARN(){ echo "‚ö†Ô∏è  [$(TS)] $*" >&2; }
          FAIL(){ echo "‚ùå [$(TS)] $*" >&2; }
          mklogs(){ mkdir -p "$LOG_DIR" "$ARTIFACT_DIR"; }
          safe(){ local name="$1"; shift; mklogs; set +e; ( "$@" ) >"$LOG_DIR/${name}.log" 2>&1; local ec=$?; if [ $ec -eq 0 ]; then OK "$name: OK"; else FAIL "$name: FAILED (exit=$ec)"; fi; return 0; }
          runlog(){ local name="$1"; shift; mklogs; set +e; ( "$@" ) > >(tee "$LOG_DIR/${name}.log") 2> >(tee "$LOG_DIR/${name}.err.log" >&2); local ec=$?; if [ $ec -eq 0 ]; then OK "$name: OK"; else FAIL "$name: FAILED (exit=$ec)"; fi; return 0; }
          SH
          chmod +x /tmp/echo_helpers.sh
          source /tmp/echo_helpers.sh
          OK "Directories ready"

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ inputs.node_version }}
        continue-on-error: true

      - name: Setup Python (optional)
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python_version }}
        continue-on-error: true

      - name: Start PostgreSQL container
        shell: bash
        continue-on-error: true
        run: |
          source /tmp/echo_helpers.sh
          safe "docker.pull.pg" docker pull "${PG_IMAGE}"
          docker rm -f "${PG_CONTAINER}" >/dev/null 2>&1 || true
          runlog "docker.run.pg" docker run -d --name "${PG_CONTAINER}" \
            -e POSTGRES_PASSWORD="${PG_PASS}" -e POSTGRES_DB="${PG_DB}" \
            -p "${PG_PORT}:5432" "${PG_IMAGE}"
          for i in {1..30}; do
            if docker exec "${PG_CONTAINER}" pg_isready -U "${PG_USER}" >/dev/null 2>&1; then OK "Postgres is ready"; break; fi
            sleep 2
          done
          docker exec -i "${PG_CONTAINER}" psql -U "${PG_USER}" -d "${PG_DB}" -c "SELECT current_database();"
          echo "${PG_DSN}" > "$ARTIFACT_DIR/pg_dsn.txt"

      - name: Generate CSV (ÎåÄÎüâ)
        shell: bash
        continue-on-error: true
        env:
          CSV_ROWS: ${{ inputs.csv_rows }}
        run: |
          source /tmp/echo_helpers.sh
          CSV="$ARTIFACT_DIR/transactions_bulk.csv"
          echo "id,account_id,merchant_id,ts,memo,amount" > "$CSV"
          rows=${CSV_ROWS:-10000}
          # CSV: RFC4180-friendly Í∞í ÏÉùÏÑ±(Îî∞Ïò¥Ìëú/ÏΩ§Îßà Ìè¨Ìï® ÏòàÏ†ú ÏÑûÍ∏∞)
          for i in $(seq 1 $rows); do
            acc=$(( (i % 5) + 1 ))
            mer=$(( (i % 7) + 1 ))
            amt=$(( (i % 1000) + 100 ))
            memo="seed-${i}"
            if (( i % 200 == 0 )); then memo="\"memo, with, commas ${i}\""; fi
            if (( i % 333 == 0 )); then memo="\"quote \"\"inside\"\" ${i}\""; fi
            echo "$i,$acc,$mer,$(date -u +%Y-%m-%dT%H:%M:%SZ),$memo,$amt" >> "$CSV"
          done
          OK "Generated CSV: $CSV (rows=$rows)"

      - name: Generate migrations (Îã®Í≥ÑÌôî & ÏóîÏßÑÎ≥Ñ)
        shell: bash
        continue-on-error: true
        run: |
          source /tmp/echo_helpers.sh

          # 00_common (ÏÇ¨Ï†Ñ Îã®Í≥Ñ: ÌååÌã∞ÏÖò/Ïù∏Îç±Ïä§ ÏóÜÏù¥ ÏΩîÏñ¥ Ïä§ÌÇ§ÎßàÎßå)
          cat > "$MCP_DIR/migrations/00_common/001_init.sql" <<'SQL'
          CREATE TABLE IF NOT EXISTS schema_version(
            id INTEGER PRIMARY KEY,
            version TEXT NOT NULL,
            applied_at TEXT NOT NULL
          );
          SQL
          cat > "$MCP_DIR/migrations/00_common/010_core_tables.sql" <<'SQL'
          CREATE TABLE IF NOT EXISTS accounts(
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL
          );
          CREATE TABLE IF NOT EXISTS merchants(
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL
          );
          CREATE TABLE IF NOT EXISTS transactions(
            id INTEGER PRIMARY KEY,
            account_id INTEGER NOT NULL,
            merchant_id INTEGER,
            ts TEXT NOT NULL,
            memo TEXT,
            amount INTEGER NOT NULL
          );
          SQL

          # 10_post_bulk/common (Í≥µÌÜµ Ïù∏Îç±Ïä§)
          cat > "$MCP_DIR/migrations/10_post_bulk/common/030_indexes.sql" <<'SQL'
          CREATE INDEX IF NOT EXISTS idx_tx_account ON transactions(account_id);
          CREATE INDEX IF NOT EXISTS idx_tx_amount  ON transactions(amount);
          CREATE INDEX IF NOT EXISTS idx_tx_ts      ON transactions(ts);
          SQL

          # 10_post_bulk/pg (PG Ï†ÑÏö©: Ïù∏Îç±Ïä§ + ÌååÌã∞ÏÖîÎãù + Ìä∏Î¶¨Í±∞ + Î∑∞/Î®∏Î∑∞)
          cat > "$MCP_DIR/migrations/10_post_bulk/pg/031_pg_indexes.sql" <<'SQL'
          -- PG Ï†ÑÏö© Ïù∏Îç±Ïä§ (Ïòà: partial/functional Ïù∏Îç±Ïä§ ÏòàÏãú)
          CREATE INDEX IF NOT EXISTS idx_tx_amount_pos ON transactions((CASE WHEN amount > 0 THEN amount END));
          SQL

          cat > "$MCP_DIR/migrations/10_post_bulk/pg/040_partitioning.sql" <<'SQL'
          -- ÌååÌã∞ÏÖò ÌÖåÏù¥Î∏î Íµ¨ÏÑ±: Ïõî Îã®ÏúÑ RANGE ÌååÌã∞ÏÖîÎãù
          -- Ï†ÑÏ†ú: ÏÉÅÏúÑ ÌÖåÏù¥Î∏îÏùÑ ÌååÌã∞ÏÖòÎìúÎ°ú Ïû¨Ï†ïÏùòÍ∞Ä Ïñ¥Î†µÍ∏∞ ÎïåÎ¨∏Ïóê Ïã†Í∑ú ÌÖåÏù¥Î∏î + Ïä§ÏôÄÌïëÏù¥ Î≥¥ÌÜµÏù¥ÏßÄÎßå,
          -- Îç∞Î™® Î™©Ï†ÅÏÉÅ Í∞ÑÎã®Ìûà ÌååÌã∞ÏÖò Íµ¨Ï°∞Î•º ÎßàÎ†®ÌïòÍ≥† Ìñ•ÌõÑ Îç∞Ïù¥ÌÑ∞Îäî ÌååÌã∞ÏÖòÏúºÎ°ú Ïú†ÏûÖÎêòÎèÑÎ°ù ÏÑ§Ï†ï.
          DO $$
          BEGIN
            IF NOT EXISTS (
              SELECT 1 FROM pg_class c JOIN pg_namespace n ON n.oid=c.relnamespace
              WHERE c.relname='transactions_p' AND n.nspname='public'
            ) THEN
              CREATE TABLE transactions_p(
                id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                account_id INTEGER NOT NULL,
                merchant_id INTEGER,
                ts TIMESTAMP NOT NULL,
                memo TEXT,
                amount INTEGER NOT NULL
              ) PARTITION BY RANGE (ts);
            END IF;
          END$$;

          -- Ïù¥Î≤à Îã¨ ÌååÌã∞ÏÖò ÏòàÏãú (ÌòÑÏû¨ UTC Í∏∞Ï§Ä)
          DO $$
          DECLARE
            start_ts DATE := date_trunc('month', now())::date;
            end_ts   DATE := (date_trunc('month', now()) + interval '1 month')::date;
            pname    TEXT := 'transactions_p_' || to_char(start_ts, 'YYYYMM');
          BEGIN
            IF NOT EXISTS (SELECT 1 FROM pg_class WHERE relname=pname) THEN
              EXECUTE format('CREATE TABLE %I PARTITION OF transactions_p FOR VALUES FROM (%L) TO (%L);',
                             pname, start_ts, end_ts);
            END IF;
          END$$;
          SQL

          cat > "$MCP_DIR/migrations/10_post_bulk/pg/050_trigger_route.sql" <<'SQL'
          -- ÎùºÏö∞ÌåÖ Ìä∏Î¶¨Í±∞: ÏÉÅÏúÑ(ÎÖºÌååÌã∞ÏÖò) transactions Î°ú INSERT Ïãú ÌååÌã∞ÏÖò ÌÖåÏù¥Î∏îÎ°ú Î¶¨Îã§Ïù¥Î†âÌä∏
          CREATE OR REPLACE FUNCTION trg_route_transactions() RETURNS trigger AS $$
          BEGIN
            INSERT INTO transactions_p(account_id, merchant_id, ts, memo, amount)
              VALUES (NEW.account_id, NEW.merchant_id, COALESCE(NEW.ts::timestamp, now()), NEW.memo, NEW.amount);
            RETURN NULL; -- ÏõêÎûò ÌÖåÏù¥Î∏îÏóêÎäî Ï†ÅÏû¨ÌïòÏßÄ ÏïäÏùå
          END
          $$ LANGUAGE plpgsql;

          DROP TRIGGER IF EXISTS route_transactions ON transactions;
          CREATE TRIGGER route_transactions
            BEFORE INSERT ON transactions
            FOR EACH ROW EXECUTE FUNCTION trg_route_transactions();
          SQL

          cat > "$MCP_DIR/migrations/10_post_bulk/pg/060_views.sql" <<'SQL'
          -- Î∑∞ & Î®∏Ìã∞Î¶¨ÏñºÎùºÏù¥Ï¶àÎìú Î∑∞
          CREATE OR REPLACE VIEW v_tx_agg_by_account AS
          SELECT account_id, COUNT(*) AS cnt, SUM(amount) AS sum
          FROM (
            SELECT account_id, amount FROM transactions
            UNION ALL
            SELECT account_id, amount FROM transactions_p
          ) t
          GROUP BY account_id
          ORDER BY account_id;

          -- Î®∏Ìã∞Î¶¨ÏñºÎùºÏù¥Ï¶àÎìú Î∑∞ (Î¶¨ÌîÑÎ†àÏãú ÌïÑÏöî)
          CREATE MATERIALIZED VIEW IF NOT EXISTS mv_tx_agg_by_account AS
          SELECT account_id, COUNT(*) AS cnt, SUM(amount) AS sum
          FROM (
            SELECT account_id, amount FROM transactions
            UNION ALL
            SELECT account_id, amount FROM transactions_p
          ) t
          GROUP BY account_id
          WITH NO DATA;
          SQL

      - name: Generate MCP server (SQLite=sql.js) + config + in-proc client
        shell: bash
        continue-on-error: true
        env:
          BATCH_SIZE: ${{ inputs.batch_size }}
        run: |
          source /tmp/echo_helpers.sh

          # package.json
          cat > "$MCP_DIR/package.json" <<'JSON'
          {
            "name": "mcp-sqlite-pg-mega",
            "version": "0.4.0",
            "private": true,
            "type": "module",
            "scripts": {
              "start": "node server.js",
              "test:sqlite": "node test_client.js sqlite",
              "test:pg": "node test_client.js pg",
              "report": "node report.js",
              "check:deps": "node -e \"require('sql.js'); require('pg'); console.log('deps ok')\""
            },
            "dependencies": {
              "pg": "^8.12.0",
              "sql.js": "^1.11.0"
            }
          }
          JSON

          # server.js (CSV ÌååÏÑú Í≥†ÎèÑÌôî, Î∞∞ÏπòÏª§Î∞ã, ÏóîÏßÑÎ≥Ñ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò ÎîîÎ†âÌÜ†Î¶¨ ÏßÄÏõê)
          cat > "$MCP_DIR/server.js" <<'JS'
          import initSqlJs from "sql.js";
          import fs from "node:fs";
          import { EOL } from "node:os";
          import path from "node:path";
          import { Client as PG } from "pg";

          let engine = null;                   // 'sqlite' | 'pg'
          let sqlite = { SQL: null, db: null, path: null };
          let pg = { client: null, tx: false };

          const send = (result, id=null, error=null) => {
            const payload = { jsonrpc:"2.0", ...(id!==null?{id}:{}) , ...(error?{error}:{result}) };
            process.stdout.write(JSON.stringify(payload)+EOL);
          };

          const listTools = () => ({
            tools: [
              { name:"db.setup",        description:"Open DB (engine='sqlite'|'pg')", input_schema:{ type:"object", properties:{ engine:{type:"string"}, sqlite_path:{type:"string"}, pg_dsn:{type:"string"} }, required:["engine"] } },
              { name:"sql.exec",        description:"Execute SQL", input_schema:{ type:"object", properties:{ sql:{type:"string"}, params:{type:"array"} }, required:["sql"] } },
              { name:"tx.begin",        description:"Begin transaction" },
              { name:"tx.commit",       description:"Commit transaction" },
              { name:"tx.rollback",     description:"Rollback transaction" },
              { name:"bulk.load.csv",   description:"Load CSV (transactions) with batch", input_schema:{ type:"object", properties:{ csv_path:{type:"string"}, batch_size:{type:"integer"} }, required:["csv_path"] } },
              { name:"migrate.apply",   description:"Apply .sql migrations in a folder (engine-aware)", input_schema:{ type:"object", properties:{ dir:{type:"string"} }, required:["dir"] } }
            ]
          });

          async function openDb({engine:eng, sqlite_path, pg_dsn}) {
            engine = eng;
            if (engine === "sqlite") {
              if (!sqlite.SQL) sqlite.SQL = await initSqlJs();
              sqlite.path = sqlite_path;
              let dbData = null;
              if (sqlite_path && fs.existsSync(sqlite_path)) dbData = fs.readFileSync(sqlite_path);
              sqlite.db = new sqlite.SQL.Database(dbData);
              return { ok:true, engine };
            } else if (engine === "pg") {
              pg.client = new PG({ connectionString: pg_dsn });
              await pg.client.connect();
              return { ok:true, engine };
            } else {
              throw new Error("Unsupported engine");
            }
          }

          async function runSql(sql, params=[]) {
            if (engine === "sqlite") {
              const q = sql.trim().toLowerCase();
              if (q.startsWith("select")) {
                const res = sqlite.db.exec(sql);
                const rows = res.length ? res[0].values.map(r =>
                  Object.fromEntries(res[0].columns.map((c,i)=>[c,r[i]]))
                ) : [];
                return { rows };
              } else {
                sqlite.db.run(sql, params);
                if (sqlite.path) {
                  const data = sqlite.db.export();
                  fs.writeFileSync(sqlite.path, Buffer.from(data));
                }
                return { changes: null, lastID: null };
              }
            } else if (engine === "pg") {
              const r = await pg.client.query(sql, params);
              return { rows: r.rows, rowCount: r.rowCount };
            }
            throw new Error("DB not initialized");
          }

          async function txBegin(){ if (engine==="sqlite") return runSql("BEGIN"); if (engine==="pg") { await pg.client.query("BEGIN"); pg.tx=true; return {ok:true}; } }
          async function txCommit(){ if (engine==="sqlite") return runSql("COMMIT"); if (engine==="pg"&&pg.tx){ await pg.client.query("COMMIT"); pg.tx=false; return {ok:true}; } }
          async function txRollback(){ if (engine==="sqlite") return runSql("ROLLBACK"); if (engine==="pg"&&pg.tx){ await pg.client.query("ROLLBACK"); pg.tx=false; return {ok:true}; } }

          // ÏóîÏßÑ Ïù∏Ïãù ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò: dir ÌïòÏúÑÏóêÏÑú Í≥µÌÜµ(00_common)¬∑ÏóîÏßÑÎ≥Ñ(post_bulk Îì±) Íµ¨Ï°∞Î•º ÏàúÌöå
          async function migrateApply(dir){
            const applyDir = async (d) => {
              if (!fs.existsSync(d)) return;
              const files = fs.readdirSync(d).filter(f=>f.endsWith(".sql")).sort();
              for (const f of files){
                const sql = fs.readFileSync(path.join(d,f), "utf8");
                await runSql(sql);
              }
            };
            await applyDir(path.join(dir, "00_common"));                 // Í≥µÌÜµ ÏÇ¨Ï†Ñ ÌÖåÏù¥Î∏î
            // post_bulkÎäî CSV Ï†ÅÏû¨ ÌõÑ Î≥ÑÎèÑÎ°ú Ìò∏Ï∂ú
            return { ok:true };
          }

          // RFC4180-ish ÌååÏÑú: Îî∞Ïò¥Ìëú/ÏΩ§Îßà/Ïù¥Ïä§ÏºÄÏù¥ÌîÑ ÏßÄÏõê(Îã®Ïùº ÎùºÏù∏ Ï†ÑÏ†ú)
          function parseCSVLine(line){
            const out = []; let cur = ""; let inQ = false; let i = 0;
            while (i < line.length){
              const ch = line[i];
              if (inQ){
                if (ch === '"'){
                  if (i+1 < line.length && line[i+1] === '"'){ cur += '"'; i+=2; continue; } // escaped quote
                  inQ = false; i++; continue;
                }
                cur += ch; i++; continue;
              } else {
                if (ch === '"'){ inQ = true; i++; continue; }
                if (ch === ','){ out.push(cur); cur=""; i++; continue; }
                cur += ch; i++; continue;
              }
            }
            out.push(cur);
            return out;
          }

          async function bulkLoadCsv(csv_path, batch_size=1000){
            const lines = fs.readFileSync(csv_path, "utf8").split(/\r?\n/);
            const header = lines.shift();
            if (!header) return { ok:true, loaded:0 };
            const cols = parseCSVLine(header);
            const idx = Object.fromEntries(cols.map((c,i)=>[c,i]));

            let loaded = 0, batch = 0;
            await txBegin();
            try{
              for (const line of lines){
                if (!line || !line.trim()) continue;
                const c = parseCSVLine(line);
                const account_id  = Number(c[idx["account_id"]]);
                const merchant_id = Number(c[idx["merchant_id"]]);
                const ts          = c[idx["ts"]];
                const memo        = c[idx["memo"]];
                const amount      = Number(c[idx["amount"]]);

                if (engine === "sqlite") {
                  await runSql(`INSERT OR IGNORE INTO accounts(id,name) VALUES(${account_id}, 'acc-${account_id}')`);
                  await runSql(`INSERT OR IGNORE INTO merchants(id,name) VALUES(${merchant_id}, 'mer-${merchant_id}')`);
                  await runSql(`INSERT INTO transactions(account_id,merchant_id,ts,memo,amount) VALUES(${account_id}, ${merchant_id}, '${ts}', '${memo?.replaceAll("'", "''")}', ${amount})`);
                } else {
                  await runSql(`INSERT INTO accounts(id,name) VALUES($1,$2) ON CONFLICT (id) DO NOTHING`, [account_id, `acc-${account_id}`]);
                  await runSql(`INSERT INTO merchants(id,name) VALUES($1,$2) ON CONFLICT (id) DO NOTHING`, [merchant_id, `mer-${merchant_id}`]);
                  await runSql(`INSERT INTO transactions(account_id,merchant_id,ts,memo,amount) VALUES($1,$2,$3,$4,$5)`,
                               [account_id, merchant_id, ts, memo, amount]);
                }

                loaded++; batch++;
                if (batch >= batch_size){
                  await txCommit();
                  await txBegin();
                  batch = 0;
                }
              }
              await txCommit();
              return { ok:true, loaded };
            } catch(e){
              await txRollback();
              throw e;
            }
          }

          // ÌÖåÏä§Ìä∏/ÏûêÎèôÌôîÏö© in-proc API
          export function createInProc(){
            return {
              call: async (name, args={}) => {
                if (name==="db.setup")       return openDb(args);
                if (name==="sql.exec")       return runSql(args.sql, args.params||[]);
                if (name==="tx.begin")       return txBegin();
                if (name==="tx.commit")      return txCommit();
                if (name==="tx.rollback")    return txRollback();
                if (name==="bulk.load.csv")  return bulkLoadCsv(args.csv_path, args.batch_size||1000);
                if (name==="migrate.apply")  return migrateApply(args.dir);
                if (name==="tools.list")     return listTools();
                throw new Error("Unknown tool");
              }
            };
          }

          // stdio Î™®Îìú(Claude CodeÏóêÏÑú ÏÇ¨Ïö©)
          if (import.meta.url === `file://${process.argv[1]}`) {
            let buffer = "";
            process.stdin.setEncoding("utf8");
            process.stdin.on("data", async chunk => {
              buffer += chunk;
              const lines = buffer.split(/\r?\n/);
              buffer = lines.pop();
              for (const line of lines) {
                if (!line.trim()) continue;
                let msg; try{ msg=JSON.parse(line);}catch{ send(null,null,{code:-32700,message:"Parse error"}); continue;}
                const {id=null, method, params={}} = msg;
                try {
                  if (method === "tools/list") return send(listTools(), id);
                  if (method === "tools/call") {
                    const { name, arguments: args={} } = params;
                    const inproc = createInProc();
                    const result = await inproc.call(name, args);
                    return send(result, id);
                  }
                  if (method === "ping") return send({ pong:true }, id);
                  return send(null, id, { code:-32601, message:"Method not found" });
                } catch (err) { return send(null, id, { code:-32000, message:String(err) }); }
              }
            });
          }
          JS

          # test_client.js ‚Äî Îã®Í≥ÑÏ†Å ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò(ÏÇ¨Ï†Ñ ‚Üí CSV ‚Üí ÏÇ¨ÌõÑ), Î∞∞Ïπò ÏòµÏÖò, PG Ï†ÑÏö© ÏÇ¨ÌõÑ Îã®Í≥Ñ Ìè¨Ìï®
          cat > "$MCP_DIR/test_client.js" <<'JS'
          import { createInProc } from "./server.js";

          const mode = process.argv[2] || "sqlite";
          const DB_PATH = process.env.DB_PATH || ".github/echo_artifacts/mcp_demo.sqlite";
          const TABLE = process.env.TABLE_NAME || "transactions";
          const SEED = parseInt(process.env.SEED_ROWS || "5", 10);
          const CSV = process.env.CSV_FILE || ".github/echo_artifacts/transactions_bulk.csv";
          const MIG_ROOT = process.env.MIG_ROOT || ".github/mcp_demo/migrations";
          const PG_DSN = process.env.PG_DSN;
          const BATCH = parseInt(process.env.BATCH_SIZE || "1000", 10);

          const mcp = createInProc();
          const log = (label, obj) => { console.log(`--- ${mode} :: ${label} ---`); console.log(JSON.stringify(obj, null, 2)); };

          (async () => {
            // 0) Ïó∞Í≤∞
            let r;
            if (mode === "sqlite") r = await mcp.call("db.setup", { engine:"sqlite", sqlite_path: DB_PATH });
            else                   r = await mcp.call("db.setup", { engine:"pg", pg_dsn: PG_DSN });
            log("db.setup", r);

            // 1) ÏÇ¨Ï†Ñ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò (Í≥µÌÜµ ÏΩîÏñ¥)
            r = await mcp.call("migrate.apply", { dir: `${MIG_ROOT}` });
            log("migrate.apply.pre", r);

            // 2) ÏãúÎìú(Îã®Í±¥)
            for (let i=0; i<SEED; i++){
              const acc=(i%3)+1, mer=(i%5)+1, amt=1000+i*17;
              const sql = (mode==="sqlite")
                ? `INSERT INTO ${TABLE}(account_id,merchant_id,ts,memo,amount) VALUES(${acc},${mer},datetime('now'),'seed-${i}',${amt})`
                : `INSERT INTO ${TABLE}(account_id,merchant_id,ts,memo,amount) VALUES(${acc},${mer},NOW()::text,'seed-${i}',${amt})`;
              r = await mcp.call("sql.exec", { sql }); log(`sql.exec(seed:${i})`, r);
            }

            // 3) Tx/Î°§Î∞± ÏãúÎÇòÎ¶¨Ïò§
            await mcp.call("tx.begin");
            const before = await mcp.call("sql.exec", { sql:`SELECT COUNT(*) AS c FROM ${TABLE}` });
            let errHappened=false;
            try { await mcp.call("sql.exec", { sql:`INSERT INTO ${TABLE}(account_id,merchant_id,ts,memo,amount,nope) VALUES(1,1,datetime('now'),'bad',123,0)` }); }
            catch(e){ errHappened=true; }
            await mcp.call("tx.rollback");
            const after = await mcp.call("sql.exec", { sql:`SELECT COUNT(*) AS c FROM ${TABLE}` });

            log("tx.beforeCount", before); log("tx.afterCount", after); log("tx.rollbackTriggered", { err: errHappened });

            // 4) CSV ÎåÄÎüâ Î°úÎî©(Î∞∞Ïπò Ïª§Î∞ã)
            r = await mcp.call("bulk.load.csv", { csv_path: CSV, batch_size: BATCH });
            log("bulk.load.csv", r);

            // 5) ÏÇ¨ÌõÑ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò: Í≥µÌÜµ Ïù∏Îç±Ïä§ + PG Ï†ÑÏö© Í≥†Í∏â Í∏∞Îä•
            // Í≥µÌÜµ Ïù∏Îç±Ïä§
            const COMMON_POST = `${MIG_ROOT}/10_post_bulk/common`;
            r = await mcp.call("migrate.apply", { dir: COMMON_POST });
            log("migrate.apply.post.common", r);
            // PG Ï†ÑÏö©
            if (mode === "pg"){
              const PG_POST = `${MIG_ROOT}/10_post_bulk/pg`;
              r = await mcp.call("migrate.apply", { dir: PG_POST });
              log("migrate.apply.post.pg", r);
              await mcp.call("sql.exec", { sql: `REFRESH MATERIALIZED VIEW CONCURRENTLY mv_tx_agg_by_account;` }).catch(()=>mcp.call("sql.exec",{sql:`REFRESH MATERIALIZED VIEW mv_tx_agg_by_account;`}));
            }

            // 6) Í≤ÄÏ¶ù ÏøºÎ¶¨
            const sel1 = await mcp.call("sql.exec", { sql:`SELECT account_id, COUNT(*) AS cnt, SUM(amount) AS sum FROM ${TABLE} GROUP BY account_id ORDER BY account_id LIMIT 5` });
            log("select.group.base", sel1);
            if (mode === "pg"){
              const sel2 = await mcp.call("sql.exec", { sql:`SELECT * FROM v_tx_agg_by_account ORDER BY account_id LIMIT 5` });
              log("select.view.pg", sel2);
            }
          })().catch(e => { console.error("CLIENT ERROR:", e); process.exit(0); });
          JS

          # report.js ‚Äî HTML & Markdown ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
          cat > "$MCP_DIR/report.js" <<'JS'
          import fs from "node:fs";
          import path from "node:path";

          const LOG_DIR = process.env.LOG_DIR || ".github/echo_logs";
          const OUT_DIR = process.env.ARTIFACT_DIR || ".github/echo_artifacts";

          function tail(p, n=50){
            if (!fs.existsSync(p)) return "";
            const lines = fs.readFileSync(p, "utf8").trimEnd().split(/\r?\n/);
            return lines.slice(-n).join("\n");
          }

          const sections = [
            { title: "SQLite Client Log (tail)", file: "sqlite_client.log" },
            { title: "Postgres Client Log (tail)", file: "pg_client.log" },
            { title: "PG Tables", file: "pg_tables.log" },
            { title: "PG Count", file: "pg_count.log" },
            { title: "Summary", file: "summary.txt" }
          ];

          let md = `# MCP MEGA++ Run Report\n\n`;
          let html = `<!doctype html><meta charset="utf-8"><title>MCP MEGA++ Report</title><style>body{font-family:system-ui,Segoe UI,Arial,sans-serif;line-height:1.4;padding:24px;} pre{background:#f6f8fa;padding:12px;overflow:auto;border-radius:8px;} h2{margin-top:28px}</style><h1>MCP MEGA++ Run Report</h1>`;

          for (const s of sections){
            const p = path.join(LOG_DIR, s.file);
            const content = tail(p, 120);
            md += `\n## ${s.title}\n\n\`\`\`\n${content}\n\`\`\`\n`;
            html += `<h2>${s.title}</h2><pre>${content.replace(/[&<>]/g, m=>({ "&":"&amp;","<":"&lt;",">":"&gt;" }[m]))}</pre>`;
          }

          fs.writeFileSync(path.join(OUT_DIR, "report.md"), md);
          fs.writeFileSync(path.join(OUT_DIR, "report.html"), html);
          console.log("Report generated:", path.join(OUT_DIR, "report.html"));
          JS

          # MCP ÏÑ§Ï†ï(Î≥ÄÍ≤Ω ÏóÜÏùå)
          cat > "$MCP_DIR/mcp.config.json" <<JSON
          {
            "mcpServers": {
              "db-mega": {
                "command": "node",
                "args": ["server.js"],
                "cwd": "${MCP_DIR}"
              }
            }
          }
          JSON

          # ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò
          pushd "$MCP_DIR" >/dev/null
          if [ ! -f package-lock.json ]; then
            runlog "npm.install" bash -lc 'npm install --no-audit --no-fund'
          else
            runlog "npm.install" bash -lc '(npm ci || npm install --no-audit --no-fund)'
          fi
          runlog "npm.check.deps" bash -lc 'npm run -s check:deps || true'
          popd >/dev/null

          echo "${{ inputs.db_path }}" > "$ARTIFACT_DIR/db_path.txt"
          echo "${{ inputs.table_name }}" > "$ARTIFACT_DIR/table.txt"

      - name: Ensure DB parent dir (SQLite file for sql.js persistence)
        shell: bash
        continue-on-error: true
        run: |
          source /tmp/echo_helpers.sh
          mkdir -p "$(dirname "${{ inputs.db_path }}")"
          OK "SQLite DB parent dir ensured"

      - name: Run SQLite scenario (pre-mig ‚Üí CSV batch ‚Üí post-mig common)
        shell: bash
        continue-on-error: true
        env:
          DB_PATH: ${{ inputs.db_path }}
          TABLE_NAME: ${{ inputs.table_name }}
          SEED_ROWS: ${{ inputs.seed_rows }}
          CSV_FILE: ${{ env.ARTIFACT_DIR }}/transactions_bulk.csv
          MIG_ROOT: ${{ env.MCP_DIR }}/migrations
          BATCH_SIZE: ${{ inputs.batch_size }}
        run: |
          source /tmp/echo_helpers.sh
          pushd "$MCP_DIR" >/dev/null
          set +e
          node test_client.js sqlite >"../.."/$LOG_DIR/sqlite_client.log 2>&1
          ec=$?
          if [ $ec -ne 0 ]; then
            FAIL "sqlite.client: FAILED (exit=$ec)"
            tail -n 200 "../.."/$LOG_DIR/sqlite_client.log || true
          else
            OK "sqlite.client: completed (exit=0)"
          fi
          set -e
          popd >/dev/null

      - name: Run PostgreSQL scenario (pre-mig ‚Üí CSV batch ‚Üí post-mig common+pg)
        shell: bash
        continue-on-error: true
        env:
          PG_DSN: ${{ env.PG_DSN }}
          TABLE_NAME: ${{ inputs.table_name }}
          SEED_ROWS: ${{ inputs.seed_rows }}
          CSV_FILE: ${{ env.ARTIFACT_DIR }}/transactions_bulk.csv
          MIG_ROOT: ${{ env.MCP_DIR }}/migrations
          BATCH_SIZE: ${{ inputs.batch_size }}
        run: |
          source /tmp/echo_helpers.sh
          pushd "$MCP_DIR" >/dev/null
          set +e
          node test_client.js pg >"../.."/$LOG_DIR/pg_client.log 2>&1
          ec=$?
          if [ $ec -ne 0 ]; then
            FAIL "pg.client: FAILED (exit=$ec)"
            tail -n 200 "../.."/$LOG_DIR/pg_client.log || true
          else
            OK "pg.client: completed (exit=0)"
          fi
          set -e
          popd >/dev/null
          docker exec -i "${PG_CONTAINER}" psql -U "${PG_USER}" -d "${PG_DB}" -c "\dt" > "$LOG_DIR/pg_tables.log" 2>&1 || true
          docker exec -i "${PG_CONTAINER}" psql -U "${PG_USER}" -d "${PG_DB}" -c "SELECT COUNT(*) FROM transactions;" > "$LOG_DIR/pg_count.log" 2>&1 || true

      - name: Build HTML/Markdown report
        shell: bash
        continue-on-error: true
        env:
          LOG_DIR: ${{ env.LOG_DIR }}
          ARTIFACT_DIR: ${{ env.ARTIFACT_DIR }}
        run: |
          source /tmp/echo_helpers.sh
          pushd "$MCP_DIR" >/dev/null
          runlog "report.build" bash -lc 'node report.js'
          popd >/dev/null

      - name: Summaries
        if: always()
        shell: bash
        continue-on-error: true
        run: |
          source /tmp/echo_helpers.sh
          {
            echo "=== RUN SUMMARY ($(date)) ==="
            echo "mode=${{ inputs.mode }}"
            echo "sqlite_db=${{ inputs.db_path }}"
            echo "pg_dsn=${PG_DSN}"
            echo "tables=accounts, merchants, transactions"
            echo "migrations: 00_common/*, 10_post_bulk/common/*, 10_post_bulk/pg/*"
            echo "csv_rows=${{ inputs.csv_rows }}, batch_size=${{ inputs.batch_size }}"
          } | tee "$LOG_DIR/summary.txt"
          OK "Summary generated"

      - name: Upload artifacts (logs, db, sources, configs, reports)
        if: always()
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: mcp-mega-artifacts
          path: |
            .github/echo_logs/**
            .github/echo_artifacts/**
            .github/mcp_demo/**
          if-no-files-found: warn
          retention-days: 7

      - name: Always success footer
        if: always()
        shell: bash
        run: |
          echo "‚úÖ DONE ‚Äî All steps attempted. Any failures are logged above. Exiting SUCCESS by design."
          exit 0
