name: "üè¶ Teradata FinOps Batch ‚Äî EchoOps + Audit + Snapshot + Schedule (ÎßåÏ†ÑÎåÄÎπÑ+DBA+DR)"

on:
  push:
    branches: [ "main" ]

  # ÏàòÎèô Ïã§Ìñâ (dispatch inputsÎäî 10Í∞ú Ïù¥Ìïò)
  workflow_dispatch:
    inputs:
      mode:                  # 1
        description: "Ïã§Ìñâ Î™®Îìú(full=Ï†ÑÏ≤¥ / lite=ÏùºÎ∂Ä Îã®Í≥ÑÎßå)"
        type: choice
        options: [full, lite]
        default: full
      gen_rows:              # 2
        description: "ÏÉùÏÑ±Ìï† Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞ Ìñâ Ïàò (ÎπàÍ∞íÏù¥Î©¥ Í∏∞Î≥∏ GEN_ROWS)"
        required: false
        default: ""
      expected_rows:         # 3
        description: "ÌíàÏßàÍ≤ÄÏ¶ù Í∏∞ÎåÄ Î°úÏö∞ Ïàò (ÎπàÍ∞íÏù¥Î©¥ Í∏∞Î≥∏ EXPECTED_ROWCOUNT)"
        required: false
        default: ""
      dr_backup:             # 4
        description: "DR(Ïô∏Î∂Ä) Î∞±ÏóÖ ÏãúÎèÑ Ïó¨Î∂Ä (true/false)"
        type: choice
        options: ["true", "false"]
        default: "false"
      include_masking_audit: # 5
        description: "ÎØºÍ∞êÌïÑÎìú ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Î°úÍ∑∏ Ìè¨Ìï® Ïó¨Î∂Ä"
        type: choice
        options: ["true", "false"]
        default: "true"
      include_healthcheck:   # 6
        description: "ÏãúÏä§ÌÖú Ìó¨Ïä§/Î≥¥Ïïà Ï†êÍ≤Ä Î°úÍ∑∏ Ìè¨Ìï® Ïó¨Î∂Ä"
        type: choice
        options: ["true", "false"]
        default: "true"
      include_schema_diff:   # 7
        description: "DDL/TPT Î≥ÄÍ≤Ω diff Í∏∞Î°ù Ïó¨Î∂Ä"
        type: choice
        options: ["true", "false"]
        default: "true"
      sla_tracking:          # 8
        description: "SLA/SLO ÏãúÍ∞Ñ Ï∏°Ï†ï, Ïû¨ÏãúÎèÑ Í∏∞Î°ù Ïó¨Î∂Ä"
        type: choice
        options: ["true", "false"]
        default: "true"

  schedule:
    # Îß§Ïùº 15:30 UTC == ÌïúÍµ≠ÏãúÍ∞Ñ(KST) ÏÉàÎ≤Ω 00:30 ÏûêÎèô ÏïºÍ∞ÑÎ∞∞Ïπò
    - cron: "30 15 * * *"

permissions:
  contents: write   # Î¶¥Î¶¨Ï¶à ÌÉúÍ∑∏/ÏûêÏÇ∞ ÏóÖÎ°úÎìúÏö© (gh release create)

env:
  ###########################################################################
  # Í≥µÌÜµ Í≤ΩÎ°ú / ÎåÄÏÉÅ ÌÖåÏù¥Î∏î
  ###########################################################################
  DATA_ROOT: /home/runner/td_data

  # ÏÑúÎπÑÏä§/ÎèÑÎ©îÏù∏Î≥Ñ ÌÖåÏù¥Î∏î (Staging -> Final -> Audit)
  TBL_STAGE: STG_DATA
  TBL_FINAL: FINAL_DATA
  TBL_AUDIT: LOAD_AUDIT_LOG

  # Î≥ëÌï© ÌÉÄÍπÉ Ïö¥ÏòÅ ÌÖåÏù¥Î∏î
  TARGET_TABLE: FINAL_DATA

  ###########################################################################
  # Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞/ÌíàÏßà Í∏∞Ï§Ä
  ###########################################################################
  GEN_ROWS: "500000"
  EXPECTED_ROWCOUNT: "500000"

  ###########################################################################
  # DR / Î≥¥Í¥ÄÏ£ºÍ∏∞ Ï†ïÏ±Ö
  ###########################################################################
  DR_RETENTION_DAYS: "1"          # DR Îç∞Ïù¥ÌÑ∞ÏÑºÌÑ∞ Î≥¥Í¥ÄÏ£ºÍ∏∞ = 1Ïùº
  DR_DIR_NAME: "dr_backup"        # DR Ïä§ÎÉÖÏÉ∑ Î°úÏª¨ Î≥¥Í¥Ä ÎîîÎ†âÌÜ†Î¶¨
  DR_TOPOLOGY_DIR: "governance/dr_site"

jobs:
  teradata_pipeline_job:
    runs-on: ubuntu-latest

    steps:
      #######################################################################
      # 0. ÏΩîÎìú Ï≤¥ÌÅ¨ÏïÑÏõÉ + ÏãúÏûë
      #######################################################################
      - name: üì• ÏΩîÎìú Ï≤¥ÌÅ¨ÏïÑÏõÉ
        uses: actions/checkout@v4

      - name: üìù ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏãúÏûë Î°úÍπÖ
        run: |
          echo "::notice::Teradata ÌÜµÌï© ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏãúÏûë (ÏïºÍ∞ÑÎ∞∞Ïπò/ÏàòÎèôÏã§Ìñâ/Push Í≥µÌÜµ)."
          echo "::debug::DATA_ROOT=${DATA_ROOT}, TARGET_TABLE=${TARGET_TABLE}, GEN_ROWS=${GEN_ROWS}"
          echo "::debug::dispatch.mode=${{ github.event.inputs.mode || 'N/A' }}"
          echo "::debug::dispatch.dr_backup=${{ github.event.inputs.dr_backup || 'N/A' }}"

      #######################################################################
      # 0.2 SLA ÌÉÄÏù¥Î®∏ ÏãúÏûë
      #######################################################################
      - name: ‚è± SLA ÌÉÄÏù¥Î®∏ ÏãúÏûë
        run: |
          mkdir -p "${DATA_ROOT}/logs" "${DATA_ROOT}/history"
          date +%s > "${DATA_ROOT}/logs/start_epoch.txt"
          {
            echo "SLA_START_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "${DATA_ROOT}/logs/sla_timing.log"

      #######################################################################
      # 0.3 DR Îç∞Ïù¥ÌÑ∞ÏÑºÌÑ∞ Íµ¨Ï°∞ÎèÑ/Ï†ïÏ±Ö Î¨∏ÏÑú ÏÉùÏÑ±
      #     - DR Topology
      #     - DR Î≥¥Í¥ÄÏ£ºÍ∏∞ (1Ïùº)
      #     - RPO/RTO ÎßµÌïë
      #######################################################################
      - name: üè¢ DR Îç∞Ïù¥ÌÑ∞ÏÑºÌÑ∞ Íµ¨Ï°∞ÎèÑ & Ï†ïÏ±Ö Í∏∞Î°ù
        run: |
          TOPO_DIR="${DATA_ROOT}/${DR_TOPOLOGY_DIR}"
          mkdir -p "$TOPO_DIR"
          DR_TOPO_FILE="${TOPO_DIR}/dr_topology.txt"

          {
            echo "=== DR DATACENTER TOPOLOGY ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo
            echo "[Primary Site]"
            echo "- PROD Teradata / FINAL_DATA Ïö¥ÏòÅ"
            echo "- Batch ETL (STG_DATA -> FINAL_DATA)"
            echo "- Compliance / Risk Analytics"
            echo
            echo "[DR Site]"
            echo "- Warm standby Teradata or compatible warehouse"
            echo "- Daily snapshot import of FINAL_DATA partitions"
            echo "- Read-only dashboards for Risk/Compliance"
            echo
            echo "[Network Zones]"
            echo "- prod-etl-zone (private)"
            echo "- dr-recovery-zone (isolated / limited inbound)"
            echo "- mgmt-zone (jump/bastion for DBA+SRE only)"
            echo
            echo "[Replication / Snapshot Flow]"
            echo "1) Batch completes in Primary."
            echo "2) Snapshot tar.gz is generated (FINAL_DATA/BATCH)."
            echo "3) Copy snapshot to DR storage bucket / ${DR_DIR_NAME}/YYYYMMDD/"
            echo "4) DR can restore that batch's partition on demand."
            echo
            echo "[Ownership / Escalation]"
            echo "- BatchSRE: owns RTO (time to restore service)."
            echo "- Compliance: owns data approval for release."
            echo "- DBA_TEAM: owns partition restore procedure."
            echo
            echo "[RPO / RTO Targets]"
            echo "- RPO_TARGET=15min (data freshness loss allowed)"
            echo "- RTO_TARGET=30min (service restore time)"
            echo
            echo "[Retention Policy]"
            echo "- DR snapshot retention: ${DR_RETENTION_DAYS} day(s)"
            echo "- Rotation job auto-removes snapshots older than ${DR_RETENTION_DAYS} day(s)"
            echo
            echo "NOTE: Ïù¥ Î¨∏ÏÑúÎäî ÏûêÎèôÏúºÎ°ú ÏÉùÏÑ±ÎêòÎ©∞ Í∞êÏÇ¨Ïóê Ìè¨Ìï®Îê©ÎãàÎã§."
          } > "$DR_TOPO_FILE"

          echo "::notice::DR Îç∞Ïù¥ÌÑ∞ÏÑºÌÑ∞ ÌÜ†Ìè¥Î°úÏßÄ Í∏∞Î°ù ÏôÑÎ£å -> $DR_TOPO_FILE"
          ls -R "${DATA_ROOT}/${DR_TOPOLOGY_DIR}" || true

      #######################################################################
      # 0.5 Îü¨ÎÑà ÌôòÍ≤Ω ÏóÖÍ∑∏Î†àÏù¥Îìú (Ïã§Ìå®Ìï¥ÎèÑ Í≥ÑÏÜç)
      #######################################################################
      - name: üîÑ Îü¨ÎÑà Ìå®ÌÇ§ÏßÄ ÏóÖÍ∑∏Î†àÏù¥Îìú (ÏãúÏä§ÌÖú ÏóÖÎç∞Ïù¥Ìä∏ & ÏóÖÍ∑∏Î†àÏù¥Îìú)
        continue-on-error: true
        run: |
          set +e
          mkdir -p "${DATA_ROOT}/logs"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          BEFORE_LIST="${DATA_ROOT}/logs/pkg_list_before.txt"
          AFTER_LIST="${DATA_ROOT}/logs/pkg_list_after.txt"

          dpkg -l > "$BEFORE_LIST" 2>/dev/null || true

          {
            echo "===== SYSTEM UPGRADE START ====="
            date
            echo "--- apt-get update ---"
            sudo apt-get update -y || echo "[WARN] apt-get update Ïã§Ìå®"
            echo "--- apt-get dist-upgrade ---"
            sudo apt-get -o Dpkg::Options::="--force-confnew" dist-upgrade -y || echo "[WARN] dist-upgrade Ïã§Ìå®"
            echo "--- apt-get autoremove ---"
            sudo apt-get autoremove -y || true
            echo "--- uname -a ---"
            uname -a
            echo "--- lsb_release -a (Í∞ÄÎä•ÌïòÎ©¥) ---"
            lsb_release -a 2>/dev/null || echo "lsb_release not available"
            echo "===== SYSTEM UPGRADE END ====="
          } > "$UPG_LOG" 2>&1

          dpkg -l > "$AFTER_LIST" 2>/dev/null || true
          diff -u "$BEFORE_LIST" "$AFTER_LIST" > "${DATA_ROOT}/logs/pkg_upgrade_diff.txt" || true

          echo "::notice::ÏãúÏä§ÌÖú ÏóÖÍ∑∏Î†àÏù¥Îìú(Ìå®ÌÇ§ÏßÄ ÏµúÏã†Ìôî) ÏãúÎèÑ ÏôÑÎ£å. ÏÉÅÏÑ∏ ÎÇ¥Ïö©ÏùÄ $UPG_LOG Î∞è pkg_upgrade_diff.txt Ï∞∏Í≥†."
          set -e

      #######################################################################
      # 1. ÎîîÎ†âÌÜ†Î¶¨ Ï§ÄÎπÑ + Íµ¨Ï°∞ ÏÉùÏÑ±
      #######################################################################
      - name: üìÇ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ± Î∞è ÌôòÍ≤Ω Ï¥àÍ∏∞Ìôî
        run: |
          set -e
          echo "ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞ ÏÉùÏÑ± Ï§ë..."
          mkdir -p "${DATA_ROOT}"/{source_files,landing_zone,logs,archive,extract_out,tmp,quality,release,history,governance,health,${DR_DIR_NAME}}
          echo "ÏÉùÏÑ±Îêú ÎîîÎ†âÌÜ†Î¶¨ ÌôïÏù∏:"
          ls -R "${DATA_ROOT}" || true
          # ÌååÌã∞ÏÖò/Î≥¥Ï°¥ Ï†ïÏ±Ö ÏãúÎÆ¨ Î°úÍ∑∏
          cat > "${DATA_ROOT}/logs/retention_policy.log" <<'EOF'
          [RETENTION POLICY SIMULATION]
          - STG_DATA: ÏùºÏûê ÌååÌã∞ÏÖò, 30Ïùº Ï¥àÍ≥º ÌååÌã∞ÏÖòÏùÄ ÏïÑÏπ¥Ïù¥Î∏å ÌõÑ ÏÇ≠Ï†ú ÎåÄÏÉÅ
          - FINAL_DATA: ÏòÅÍµ¨ Î≥¥Ï°¥, Îã® Í∞úÏù∏Ï†ïÎ≥¥ ÌïÑÎìúÎäî ÎßàÏä§ÌÇπ ÏÉÅÌÉúÎßå Ïú†ÏßÄ
          - LOAD_AUDIT_LOG: 1ÎÖÑ Î≥¥Ï°¥ ÌõÑ ÏΩúÎìúÏä§ÌÜ†Î¶¨ÏßÄ Ïù¥Ï†Ñ
          - ÏïÑÏπ¥Ïù¥Î∏å Í≤ΩÎ°ú: /home/runner/td_data/archive/YYYY/MM/DD/*.gz
          These are simulated operational policies for audit/compliance.
          EOF

      #######################################################################
      # 1.5 Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞ ÎåÄÎüâ ÏÉùÏÑ±
      #######################################################################
      - name: üèó Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞ ÎåÄÎüâ ÏÉùÏÑ±
        run: |
          set -e
          ROWS_INPUT="${{ github.event.inputs.gen_rows || '' }}"
          if [ -n "$ROWS_INPUT" ]; then
            ROWS="$ROWS_INPUT"
          else
            ROWS="${GEN_ROWS}"
          fi

          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"
          echo "COL1,COL2,AMOUNT,LOAD_TS" > "$SRC_FILE"

          echo "Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞ ${ROWS}Ìñâ ÏÉùÏÑ± Ï§ë..."
          i=1
          while [ $i -le $ROWS ]; do
            AMT=$(( (RANDOM % 999800) + 100 ))        # 100 ~ ÏïΩ 1,000,000
            MERCH=$(( (RANDOM % 9000) + 1000 ))
            if [ $AMT -gt 900000 ]; then
              NOTE="MERCHANT_${MERCH}_FLAG-HIGH_${i}"
            else
              NOTE="MERCHANT_${MERCH}_NOTE_${i}"
            fi
            echo "${i},${NOTE},${AMT},$(date +%Y-%m-%dT%H:%M:%S%z)" >> "$SRC_FILE"
            i=$((i+1))
          done

          ls -lh "$SRC_FILE"
          head -n 5 "$SRC_FILE"
          tail -n 5 "$SRC_FILE"

          echo "${ROWS}" > "${DATA_ROOT}/tmp/generated_rowcount.txt"
          echo "::notice::Í∞ÄÏÉÅ CSV ÏÉùÏÑ± ÏôÑÎ£å (${ROWS} rows) -> $SRC_FILE"

      #######################################################################
      # 2. ÌôòÍ≤Ω Ïä§ÎÉÖÏÉ∑
      #######################################################################
      - name: üîç Îü∞ÌÉÄÏûÑ ÌôòÍ≤Ω Ïä§ÎÉÖÏÉ∑ Ï†ÄÏû•
        run: |
          SNAP="${DATA_ROOT}/logs/env_snapshot.txt"
          {
            echo "===== ENV SNAPSHOT ====="
            date
            uname -a
            whoami
            echo "--- PATH ---"
            echo "$PATH"
            echo "--- DISK (df -h) ---"
            df -h
            echo "--- MEMORY (free -m) ---"
            free -m || true
            echo "--- GITHUB CONTEXT ---"
            echo "RUN_ID=$GITHUB_RUN_ID"
            echo "RUN_NUMBER=$GITHUB_RUN_NUMBER"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "SHA=$GITHUB_SHA"
            echo "BRANCH=$GITHUB_REF_NAME"
          } > "$SNAP"
          echo "::debug::ÌôòÍ≤Ω Ïä§ÎÉÖÏÉ∑ Í∏∞Î°ù ÏôÑÎ£å -> $SNAP"

      #######################################################################
      # 2.5 ÏãúÏä§ÌÖú Ìó¨Ïä§ / Î≥¥Ïïà Ï†êÍ≤Ä
      #######################################################################
      - name: ü©∫ ÏãúÏä§ÌÖú Ìó¨Ïä§ Î∞è Î≥¥Ïïà Ï†êÍ≤Ä
        if: ${{ github.event.inputs.include_healthcheck != 'false' }}
        run: |
          HEALTH_LOG="${DATA_ROOT}/health/system_health.log"
          {
            echo "===== SYSTEM HEALTH CHECK ====="
            date
            echo "--- dmesg (tail 50) ---"
            dmesg | tail -n 50 || true
            echo
            echo "--- TCP/UDP ÏÜåÏºì ÏÉÅÌÉú (ss -tuna head 20) ---"
            ss -tuna | head -n 20 || true
            echo
            echo "--- CPU/MEM load (top -b -n1 head 20) ---"
            top -b -n1 | head -n 20 || true
            echo
            echo "--- I/O stat (iostat if available) ---"
            iostat 2>/dev/null || echo "iostat not available"
            echo
            echo "--- ÎÑ§Ìä∏ÏõåÌÅ¨ Ïô∏Î∂Ä Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ ---"
            curl -I https://example.com 2>&1 | head -n 5 || echo "curl external check failed or blocked"
          } > "$HEALTH_LOG"
          echo "::notice::ÏãúÏä§ÌÖú Ìó¨Ïä§ Ï≤¥ÌÅ¨ Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $HEALTH_LOG"

      #######################################################################
      # 3. DDL / TPT / ÏøºÎ¶¨ Ï†ïÏùò Î∞±ÏóÖ
      #######################################################################
      - name: üß± DDL/TPT/ÏøºÎ¶¨ Ïä§ÌÅ¨Î¶ΩÌä∏ Î∞±ÏóÖ
        run: |
          set -e
          mkdir -p "${DATA_ROOT}/source_files/sql"
          mkdir -p "${DATA_ROOT}/source_files/tpt"

          cat > "${DATA_ROOT}/source_files/sql/create_tables.sql" <<'SQL'
          -- [STAGING TABLE]
          CREATE TABLE STG_DATA (
            COL1       INTEGER,
            COL2       VARCHAR(100),
            AMOUNT     BIGINT,
            LOAD_TS    TIMESTAMP,
            BATCH_ID   VARCHAR(40)
          );

          -- [FINAL TABLE]
          CREATE TABLE FINAL_DATA (
            COL1       INTEGER,
            COL2       VARCHAR(100),
            AMOUNT     BIGINT,
            LOAD_TS    TIMESTAMP,
            ETL_TS     TIMESTAMP,
            SRC_BATCH  VARCHAR(40)
          );

          -- [AUDIT TABLE]
          CREATE TABLE LOAD_AUDIT_LOG (
            AUDIT_TS        TIMESTAMP,
            BATCH_ID        VARCHAR(40),
            SRC_FILE        VARCHAR(255),
            ROW_LOADED      INTEGER,
            ROW_EXPECTED    INTEGER,
            STATUS_CODE     INTEGER,
            STATUS_MESSAGE  VARCHAR(2000),
            OPERATOR        VARCHAR(128)
          );

          CREATE INDEX IDX_FINAL_DATA_COL1 ON FINAL_DATA (COL1);
          CREATE INDEX IDX_AUDIT_BATCH     ON LOAD_AUDIT_LOG (BATCH_ID);
          SQL

          cat > "${DATA_ROOT}/source_files/sql/proc_LOAD_AND_MERGE.sql" <<'SQL'
          REPLACE PROCEDURE LOAD_AND_MERGE (
            IN p_batch_id VARCHAR(40),
            IN p_src_file VARCHAR(255)
          )
          BEGIN
            INSERT INTO FINAL_DATA (COL1, COL2, AMOUNT, LOAD_TS, ETL_TS, SRC_BATCH)
            SELECT
              COL1,
              TRIM(COL2),
              AMOUNT,
              LOAD_TS,
              CURRENT_TIMESTAMP,
              p_batch_id
            FROM STG_DATA
            WHERE BATCH_ID = p_batch_id;

            INSERT INTO LOAD_AUDIT_LOG (
              AUDIT_TS, BATCH_ID, SRC_FILE,
              ROW_LOADED, ROW_EXPECTED,
              STATUS_CODE, STATUS_MESSAGE, OPERATOR
            )
            VALUES (
              CURRENT_TIMESTAMP,
              p_batch_id,
              p_src_file,
              NULL,
              NULL,
              0,
              'LOAD_AND_MERGE executed',
              USER
            );
          END;
          SQL

          cat > "${DATA_ROOT}/source_files/tpt/load_stg_data.tpt" <<'TPT'
          DEFINE JOB LOAD_STG_DATA
          (
            DEFINE SCHEMA STG_SCHEMA
            (
              COL1       INTEGER,
              COL2       VARCHAR(100),
              AMOUNT     BIGINT,
              LOAD_TS    VARCHAR(30)
            );

            DEFINE OPERATOR FILE_READER
            TYPE DATACONNECTOR PRODUCER
            SCHEMA STG_SCHEMA
            ATTRIBUTES
            (
              FileName = '/home/runner/td_data/landing_zone/input_YYYYMMDD.csv',
              Format   = 'Delimited'
            );

            DEFINE OPERATOR TPT_INSERTER
            TYPE STREAM
            TARGET TABLE STG_DATA
            ATTRIBUTES
            (
              TdpId        = 'TERADATA_SID',
              UserName     = 'ETL_USER',
              UserPassword = 'ETL_PASS',
              LogTable     = 'ETL_LOG_TABLE'
            );

            APPLY
            (
              'INSERT INTO STG_DATA (COL1, COL2, AMOUNT, LOAD_TS, BATCH_ID)
               VALUES (:COL1, :COL2, :AMOUNT, TIMESTAMP :LOAD_TS, ''BATCH_PLACEHOLDER'');'
            )
            TO OPERATOR (TPT_INSERTER[1])
            SELECT
              COL1, COL2, AMOUNT, LOAD_TS
            FROM OPERATOR (FILE_READER[1]);
          );
          TPT

          cat > "${DATA_ROOT}/source_files/sql/quality_queries.sql" <<'SQL'
          SELECT COUNT(*) AS CNT_STG
          FROM STG_DATA
          WHERE BATCH_ID = :BATCH_ID;

          SELECT COUNT(*) AS CNT_FINAL
          FROM FINAL_DATA
          WHERE SRC_BATCH = :BATCH_ID;

          SELECT
            SUM(CASE WHEN COL2 IS NULL OR TRIM(COL2) = '' THEN 1 ELSE 0 END) AS NULL_MEMO_ROWS,
            SUM(CASE WHEN AMOUNT > 900000 THEN 1 ELSE 0 END) AS HIGH_AMOUNT_ROWS,
            COUNT(*) AS TOTAL_ROWS,
            SUM(AMOUNT) AS SUM_AMOUNT,
            AVG(AMOUNT) AS AVG_AMOUNT
          FROM FINAL_DATA
          WHERE SRC_BATCH = :BATCH_ID;

          SELECT *
          FROM LOAD_AUDIT_LOG
          QUALIFY ROW_NUMBER()
            OVER (PARTITION BY BATCH_ID ORDER BY AUDIT_TS DESC) = 1
          ORDER BY AUDIT_TS DESC;
          SQL

          echo "::notice::DDL/TPT/ÏøºÎ¶¨ Ïä§ÌÅ¨Î¶ΩÌä∏Í∞Ä ${DATA_ROOT}/source_files Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§."
          ls -R "${DATA_ROOT}/source_files" || true

      #######################################################################
      # 3.5 Ïä§ÌÇ§Îßà Î≥ÄÍ≤Ω diff
      #######################################################################
      - name: üßæ Ïä§ÌÇ§Îßà Î≥ÄÍ≤Ω diff Í∏∞Î°ù
        if: ${{ github.event.inputs.include_schema_diff != 'false' }}
        run: |
          PREV_SCHEMA="${DATA_ROOT}/history/last_create_tables.sql"
          CURR_SCHEMA="${DATA_ROOT}/source_files/sql/create_tables.sql"
          DIFF_LOG="${DATA_ROOT}/logs/schema_diff.log"

          if [ -f "$PREV_SCHEMA" ]; then
            diff -u "$PREV_SCHEMA" "$CURR_SCHEMA" > "$DIFF_LOG" || true
          else
            echo "[first run or no prev schema]" > "$DIFF_LOG"
          fi

          cp "$CURR_SCHEMA" "$PREV_SCHEMA" 2>/dev/null || cp "$CURR_SCHEMA" "$PREV_SCHEMA"
          echo "::notice::Ïä§ÌÇ§Îßà diff Í≤∞Í≥º -> $DIFF_LOG"
          head -n 200 "$DIFF_LOG" || true

      #######################################################################
      # 4. Îç∞Ïù¥ÌÑ∞ Ï†ÅÏû¨ & Î≥ëÌï© ÏãúÎÆ¨
      #######################################################################
      - name: üîÑ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Î≥ÄÌôò/Î≥ëÌï© Ïã§Ìñâ (ÎåÄÎüâ Í∞ÄÏÉÅÎç∞Ïù¥ÌÑ∞ / Ïã§Ìå®Ìï¥ÎèÑ Í≥ÑÏÜç)
        continue-on-error: true
        run: |
          set +e
          mkdir -p "${DATA_ROOT}/logs"

          BATCH_ID="BATCH_$(date +%Y%m%d_%H%M%S)"
          echo "$BATCH_ID" > "${DATA_ROOT}/logs/batch_id.txt"

          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"
          PIPELINE_LOG="${DATA_ROOT}/logs/pipeline_load_merge.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"

          GENERATED_COUNT=$(cat "${DATA_ROOT}/tmp/generated_rowcount.txt" 2>/dev/null || echo "${GEN_ROWS}")

          EXPECTED_IN="${{ github.event.inputs.expected_rows || '' }}"
          if [ -n "$EXPECTED_IN" ]; then
            EXPECTED_LOCAL="$EXPECTED_IN"
          else
            EXPECTED_LOCAL="${EXPECTED_ROWCOUNT}"
          fi

          echo "=== LOAD+MERGE START ==="            | tee "$PIPELINE_LOG"
          echo "BATCH_ID=$BATCH_ID"                | tee -a "$PIPELINE_LOG"
          echo "SRC_FILE=$SRC_FILE"                | tee -a "$PIPELINE_LOG"
          echo "TARGET_TABLE=${TARGET_TABLE}"      | tee -a "$PIPELINE_LOG"
          echo "TBL_STAGE=${TBL_STAGE}"            | tee -a "$PIPELINE_LOG"
          echo "TBL_FINAL=${TBL_FINAL}"            | tee -a "$PIPELINE_LOG"
          echo "TBL_AUDIT=${TBL_AUDIT}"            | tee -a "$PIPELINE_LOG"
          echo
          echo "[1] (ÏãúÎÆ¨) TPT Bulk Load ${GENERATED_COUNT}Ìñâ -> ${TBL_STAGE}" | tee -a "$PIPELINE_LOG"
          echo "    FROM ${SRC_FILE}"                                      | tee -a "$PIPELINE_LOG"
          echo "[2] (ÏãúÎÆ¨) CALL LOAD_AND_MERGE('${BATCH_ID}','${SRC_FILE}')" | tee -a "$PIPELINE_LOG"
          echo "[3] (ÏãúÎÆ¨) Í∞êÏÇ¨Î°úÍ∑∏(${TBL_AUDIT}) insert"                    | tee -a "$PIPELINE_LOG"
          echo "=== LOAD+MERGE END ==="                                    | tee -a "$PIPELINE_LOG"

          STATUS_CODE=0
          STATUS_MSG="OK(vdata-bulk-load)"

          SLA_ON="${{ github.event.inputs.sla_tracking || 'true' }}"
          if [ "$SLA_ON" != "false" ]; then
            echo "SLA: first attempt success" | tee -a "$PIPELINE_LOG"
            echo "SLA_RETRY_COUNT=0" > "${DATA_ROOT}/logs/sla_retry.log"
          else
            echo "SLA tracking disabled" > "${DATA_ROOT}/logs/sla_retry.log"
          fi

          {
            echo "STATUS_CODE=${STATUS_CODE}"
            echo "STATUS_MSG=${STATUS_MSG}"
            echo "BATCH_ID=${BATCH_ID}"
            echo "SRC_FILE=${SRC_FILE}"
            echo "ROW_EXPECTED=${EXPECTED_LOCAL}"
            echo "ROW_LOADED=${GENERATED_COUNT}"
          } > "$STATUS_FILE"

          echo "::notice::Îç∞Ïù¥ÌÑ∞ Ï†ÅÏû¨/Î≥ëÌï©(ÎåÄÏö©Îüâ Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞) Îã®Í≥Ñ ÏôÑÎ£å. ÌååÏù¥ÌîÑÎùºÏù∏ÏùÄ Í≥ÑÏÜçÎê©ÎãàÎã§."
          set -e

      #######################################################################
      # 4.5 DBA: ÌååÌã∞ÏÖò / ÎùΩ / ÌîåÎûú / ÌäúÎãù
      #######################################################################
      - name: üß† DBA Îü∞ÌÉÄÏûÑ ÏÑ±Îä•/ÎùΩ/ÌååÌã∞ÏÖò Î∂ÑÏÑù Î°úÍ∑∏ ÏÉùÏÑ±
        run: |
          BATCH_ID_FILE="${DATA_ROOT}/logs/batch_id.txt"
          BATCH_ID_VAL="$(cat "$BATCH_ID_FILE" 2>/dev/null || echo 'UNKNOWN_BATCH')"

          PARTITION_LOG="${DATA_ROOT}/logs/partition_access.log"
          {
            echo "=== PARTITION ACCESS REPORT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "STG_DATA partition key: LOAD_TS (daily range)"
            echo "FINAL_DATA partition key: SRC_BATCH (batch_id range/hash)"
            echo "TODAY_ACCESSED_PARTITIONS: LOAD_TS=$(date +%Y-%m-%d) , SRC_BATCH=${BATCH_ID_VAL}"
            echo "NOTE=Only today's partition scanned (good pruning) ‚Äî no full scan (simulated)."
          } > "$PARTITION_LOG"

          LOCK_LOG="${DATA_ROOT}/logs/lock_contention.log"
          {
            echo "=== LOCK / CONTENTION REPORT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TABLE=${TBL_FINAL}"
            echo "LOCK_MODE=WriteLockDuringMerge (simulated)"
            echo "CONTENTIONS=0"
            echo "MAX_WAIT_SEC=0"
            echo "NOTE=No blocking detected (simulated)."
          } > "$LOCK_LOG"

          PLAN_LOG="${DATA_ROOT}/logs/query_plan_sample.log"
          {
            echo "=== SAMPLE EXPLAIN PLAN (SIMULATED) ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "QUERY=SELECT COUNT(*) FROM ${TBL_FINAL} WHERE SRC_BATCH='${BATCH_ID_VAL}';"
            echo "PLAN=Partitioned access on SRC_BATCH only, no full table scan (simulated)."
            echo "NOTE=Good selectivity expected."
          } > "$PLAN_LOG"

          TUNE_LOG="${DATA_ROOT}/logs/tuning_recommendations.log"
          {
            echo "=== TUNING RECOMMENDATIONS ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TABLE=${TBL_FINAL}"
            echo "RECOMMENDATION_1=Consider Secondary Index on (SRC_BATCH, LOAD_TS)"
            echo "WHY=Frequent WHERE SRC_BATCH=? AND LOAD_TS>=? in reporting queries"
            echo "ESTIMATED_BENEFIT=Faster morning dashboards, less full scan"
            echo "ACTION_OWNER=DBA_TEAM"
          } > "$TUNE_LOG"

          echo "::notice::DBA ÏÑ±Îä•/ÎùΩ/ÌååÌã∞ÏÖò/ÌäúÎãù Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å."

      #######################################################################
      # 5. ÌíàÏßà Í≤ÄÏ¶ù
      #######################################################################
      - name: ‚úÖ ÌíàÏßà Í≤ÄÏ¶ù Î∞è QC Î°úÍ∑∏ ÏÉùÏÑ±
        run: |
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          EXPECTED="$(grep '^ROW_EXPECTED=' \"$STATUS_FILE\" 2>/dev/null | cut -d= -f2)"
          ACTUAL="$(grep '^ROW_LOADED='   \"$STATUS_FILE\" 2>/dev/null | cut -d= -f2)"

          [ -z "$EXPECTED" ] && EXPECTED="${EXPECTED_ROWCOUNT}"
          [ -z "$ACTUAL" ] && ACTUAL="0"

          ROW_COUNT_OK="false"
          if [ "$EXPECTED" = "$ACTUAL" ]; then
            ROW_COUNT_OK="true"
          fi

          HIGH_COUNT=$(grep 'FLAG-HIGH' "${DATA_ROOT}/landing_zone"/input_*.csv | wc -l || echo "0")
          if [ "$ACTUAL" -gt 0 ]; then
            HIGH_RATIO=$(echo "$HIGH_COUNT * 100 / $ACTUAL" | bc 2>/dev/null || echo "0")
          else
            HIGH_RATIO="0"
          fi

          LAST_HIST="${DATA_ROOT}/history/last_run_stats.txt"
          PREV_ROWS="N/A"
          GROWTH="N/A"
          if [ -f "$LAST_HIST" ]; then
            PREV_ROWS=$(grep '^ACTUAL_ROWCOUNT=' "$LAST_HIST" | cut -d= -f2)
            if [ -n "$PREV_ROWS" ] && [ "$PREV_ROWS" != "N/A" ] && [ "$PREV_ROWS" -gt 0 ]; then
              GROWTH=$(echo "($ACTUAL-$PREV_ROWS)*100/$PREV_ROWS" | bc 2>/dev/null || echo "N/A")
            fi
          fi

          {
            echo "=== QUALITY CHECK ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "EXPECTED_ROWCOUNT=$EXPECTED"
            echo "ACTUAL_ROWCOUNT=$ACTUAL"
            echo "ROW_COUNT_OK=$ROW_COUNT_OK"
            echo
            echo "--- HIGH AMOUNT (FLAG-HIGH) ---"
            echo "HIGH_COUNT=$HIGH_COUNT"
            echo "HIGH_RATIO_PERCENT=$HIGH_RATIO"
            echo
            echo "--- GROWTH vs PREV RUN ---"
            echo "PREV_ACTUAL_ROWCOUNT=$PREV_ROWS"
            echo "ROW_GROWTH_PERCENT=$GROWTH"
            echo
            echo "--- SIMPLE SANITY RULES ---"
            echo "1) Í∏∞ÎåÄ Î°úÏö∞ Ïàò ÏùºÏπò? -> $ROW_COUNT_OK"
            echo "2) Ï¥àÍ≥†Ïï° Í±∞Îûò ÎπÑÏú® Í≥ºÎèÑ? -> check HIGH_RATIO_PERCENT"
            echo "3) Ï†ÑÏùº ÎåÄÎπÑ Í∏âÏ¶ù/Í∏âÍ∞ê? -> $GROWTH"
          } > "$QUALITY_LOG"

          {
            echo "RUN_ID=$GITHUB_RUN_ID"
            echo "ACTUAL_ROWCOUNT=$ACTUAL"
            echo "HIGH_RATIO_PERCENT=$HIGH_RATIO"
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$LAST_HIST"

          echo "::notice::ÌíàÏßà Í≤ÄÏ¶ù Î°úÍ∑∏ ÏûëÏÑ± ÏôÑÎ£å -> $QUALITY_LOG"
          head -n 50 "$QUALITY_LOG" || true

      #######################################################################
      # 5.5 DBA Ïú†ÏßÄÎ≥¥Ïàò / DR Î≥µÍµ¨ ÌîåÎûú / Ïö©Îüâ / Î≥¥Í¥ÄÏ£ºÍ∏∞
      #######################################################################
      - name: üßÆ DBA Ïú†ÏßÄÎ≥¥Ïàò/DR/Ïö©Îüâ/RPO-RTO Î°úÍ∑∏ ÏÉùÏÑ±
        run: |
          BATCH_ID_FILE="${DATA_ROOT}/logs/batch_id.txt"
          BATCH_ID_VAL="$(cat "$BATCH_ID_FILE" 2>/dev/null || echo 'UNKNOWN_BATCH')"

          STATS_LOG="${DATA_ROOT}/logs/stats_maintenance.log"
          {
            echo "=== STATS MAINTENANCE REPORT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TABLE ${TBL_FINAL}: COLLECT STATS on (COL1, SRC_BATCH, LOAD_TS) (simulated)"
            echo "IMPACT=Optimizer cardinality accuracy ‚Üë"
            echo "NEXT_REVIEW=+1 day or rowcount_delta > 20%"
          } > "$STATS_LOG"

          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          LAST_CAP="${DATA_ROOT}/history/last_capacity.txt"
          CURR_STG_MB=$(( (RANDOM % 4000) + 1000 ))
          CURR_FINAL_MB=$(( (RANDOM % 9000) + 2000 ))
          PREV_STG_MB="N/A"
          PREV_FINAL_MB="N/A"
          if [ -f "$LAST_CAP" ]; then
            PREV_STG_MB=$(grep '^STG_MB=' "$LAST_CAP" | cut -d= -f2)
            PREV_FINAL_MB=$(grep '^FINAL_MB=' "$LAST_CAP" | cut -d= -f2)
          fi
          {
            echo "=== CAPACITY / GROWTH ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "STG_MB_CURR=$CURR_STG_MB"
            echo "FINAL_MB_CURR=$CURR_FINAL_MB"
            echo "STG_MB_PREV=$PREV_STG_MB"
            echo "FINAL_MB_PREV=$PREV_FINAL_MB"
            echo "NOTE=Track daily growth for capacity planning & cost mgmt."
          } > "$CAP_LOG"
          {
            echo "STG_MB=$CURR_STG_MB"
            echo "FINAL_MB=$CURR_FINAL_MB"
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$LAST_CAP"

          DRPLAY="${DATA_ROOT}/governance/recovery_playbook.txt"
          {
            echo "=== RECOVERY PLAYBOOK ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "BATCH_ID=${BATCH_ID_VAL}"
            echo "1) Pause downstream reporting jobs."
            echo "2) Restore FINAL_DATA partition for BATCH_ID=${BATCH_ID_VAL} from latest DR snapshot."
            echo "3) Validate rowcount vs EXPECTED_ROWCOUNT."
            echo "4) Re-run LOAD_AND_MERGE for failed batch only."
            echo "5) Notify BatchSRE and Compliance if discrepancy > 0.5%."
            echo
            echo "RPO_TARGET=15min"
            echo "RTO_TARGET=30min"
            echo "RPO_ESTIMATE=15min_ok (simulated)"
            echo "RTO_ESTIMATE=25min_ok (simulated)"
            echo
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            echo "POLICY=Snapshots older than ${DR_RETENTION_DAYS} day(s) are purged from ${DR_DIR_NAME}"
          } > "$DRPLAY"

          CLASS_LOG="${DATA_ROOT}/governance/table_classification.log"
          {
            echo "=== TABLE CLASSIFICATION MAP ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "STG_DATA: SENSITIVITY=HIGH_RAW (pre-mask), OWNER=ETL_TEAM, RETENTION=30d"
            echo "FINAL_DATA: SENSITIVITY=MASKED_CONFIDENTIAL, OWNER=RISK_ANALYTICS, RETENTION=indef(masked)"
            echo "LOAD_AUDIT_LOG: SENSITIVITY=OPERATIONS_AUDIT, OWNER=COMPLIANCE_TEAM, RETENTION=365d"
            echo "DR_BACKUP (FINAL_DATA subset): SENSITIVITY=MASKED_CONFIDENTIAL, OWNER=DBA_TEAM"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
          } > "$CLASS_LOG"

          echo "::notice::DBA Ïú†ÏßÄÎ≥¥Ïàò/DR/RPO-RTO/Ïö©Îüâ/Î∂ÑÎ•ò Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å."

      #######################################################################
      # 5.6 ÎØºÍ∞ê Îç∞Ïù¥ÌÑ∞ ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Í∞êÏÇ¨
      #######################################################################
      - name: üõ° ÎØºÍ∞ê Îç∞Ïù¥ÌÑ∞ ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Í∞êÏÇ¨ Î°úÍ∑∏
        if: ${{ github.event.inputs.include_masking_audit != 'false' }}
        run: |
          MASK_LOG="${DATA_ROOT}/logs/masking_audit.log"
          {
            echo "[MASKING POLICY SIMULATION]"
            echo "- ÎØºÍ∞ê ÌïÑÎìú Ïòà: CARD_NO, SSN, ACCOUNT_ID Îì±"
            echo "- FINAL_DATA ÏóêÏÑúÎäî Ìï¥Îãπ ÎØºÍ∞ê ÌïÑÎìúÎäî SHA256 ÎòêÎäî TOKEN_ID Î°úÎßå Ï†ÄÏû•"
            echo "- STG_DATA ÏõêÎ≥∏ÌòïÏãùÏùÄ 24ÏãúÍ∞Ñ ÎÇ¥ ÌååÌã∞ÏÖò ÏïÑÏπ¥Ïù¥Î∏å ÌõÑ Ï†ëÍ∑ºÏ∞®Îã®"
            echo "- ACCESS CONTROL: ANALYST_ROLE ÏùÄ ÎßàÏä§ÌÇπÎêú Ïª¨ÎüºÎßå SELECT Í∞ÄÎä•"
            echo "- Î≥ÄÍ≤Ω ÏäπÏù∏Ïûê(DATA_OWNER_X) ÏäπÏù∏Ïùº=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$MASK_LOG"
          echo "::notice::ÎØºÍ∞ê Îç∞Ïù¥ÌÑ∞ ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $MASK_LOG"
          head -n 40 "$MASK_LOG" || true

      #######################################################################
      # 6. Í∂åÌïú/Ï†ëÍ∑ºÏ†úÏñ¥ Í∞êÏÇ¨
      #######################################################################
      - name: üîê Í∂åÌïú/Ï†ëÍ∑ºÏ†úÏñ¥ Í∞êÏÇ¨ Í∏∞Î°ù ÏÉùÏÑ±
        run: |
          ACL_LOG="${DATA_ROOT}/logs/acl_audit.sql"
          {
            echo "-- Í∂åÌïú Í∞êÏÇ¨ Î°úÍ∑∏ (ÏãúÎÆ¨Î†àÏù¥ÏÖò)"
            echo "-- FINAL_DATA Ï°∞Ìöå/Ïì∞Í∏∞ Í∂åÌïú ÏÑ§Ï†ï ÏòàÏãú"
            echo "GRANT SELECT ON ${TBL_FINAL} TO ROLE ANALYST_ROLE;"
            echo "GRANT INSERT,UPDATE ON ${TBL_FINAL} TO ROLE ETL_LOADER_ROLE;"
            echo "REVOKE INSERT ON ${TBL_FINAL} FROM ROLE ANALYST_ROLE;"
            echo
            echo "-- AUDIT LOG ÌÖåÏù¥Î∏î Ï†ëÍ∑º ÌÜµÏ†ú"
            echo "GRANT SELECT ON ${TBL_AUDIT} TO ROLE AUDIT_READER_ROLE;"
            echo
            echo "-- Column-level masking / Row-level filtering (Î¨∏ÏÑúÌôîÏö©)"
            echo "-- ANALYST_ROLE ÏùÄ FINAL_DATA.COL2(Î©îÎ™®)Îäî ÎßàÏä§ÌÇπ Î≤ÑÏ†ÑÎßå Ï°∞Ìöå Í∞ÄÎä•"
            echo "-- ANALYST_ROLE ÏùÄ ÏûêÏã†Ïùò ÏßÄÏ†ê Îç∞Ïù¥ÌÑ∞Îßå Ï†ëÍ∑º (row filter)"
            echo
            echo "-- Ïã§Ìñâ Î©îÌÉÄ"
            echo "-- AUDIT_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "-- EXEC_BY=$GITHUB_ACTOR"
            echo "-- COMMIT_SHA=$GITHUB_SHA"
            echo "-- BRANCH=$GITHUB_REF_NAME"
          } > "$ACL_LOG"
          echo "::debug::Í∂åÌïú Í∞êÏÇ¨ Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $ACL_LOG"
          head -n 60 "$ACL_LOG" || true

      #######################################################################
      # 7. Î¶¥Î¶¨Ï¶à Ïä§ÎÉÖÏÉ∑ & Î¨¥Í≤∞ÏÑ± Ìï¥Ïãú
      #######################################################################
      - name: üóú Í≤∞Í≥ºÎ¨º tar.gz Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ±
        run: |
          SNAP_TAG="td-snapshot-$(date +%Y%m%d-%H%M%S)-${GITHUB_SHA:0:8}"
          SNAP_DIR="${DATA_ROOT}/release"
          SNAP_FILE="${SNAP_DIR}/${SNAP_TAG}.tar.gz"

          echo "SNAP_TAG=$SNAP_TAG"    | tee "${DATA_ROOT}/logs/snapshot_tag.txt"
          echo "SNAP_FILE=$SNAP_FILE"  | tee -a "${DATA_ROOT}/logs/snapshot_tag.txt"

          mkdir -p "$SNAP_DIR"
          tar -czf "$SNAP_FILE" \
            -C "${DATA_ROOT}" \
            logs \
            quality \
            extract_out \
            source_files \
            landing_zone \
            tmp \
            health \
            governance \
            || true

          sha256sum "$SNAP_FILE" > "${DATA_ROOT}/logs/snapshot_hash.txt" 2>/dev/null || echo "hash_failed" > "${DATA_ROOT}/logs/snapshot_hash.txt"

          echo "::notice::Ïä§ÎÉÖÏÉ∑ ÏïÑÏπ¥Ïù¥Î∏å ÏÉùÏÑ± -> $SNAP_FILE"
          ls -lh "$SNAP_FILE" || true
          head -n 5 "${DATA_ROOT}/logs/snapshot_hash.txt" || true

      #######################################################################
      # 7.5 DR(Ïû¨Ìï¥Î≥µÍµ¨) Î∞±ÏóÖ + DR Î≥¥Í¥ÄÏ£ºÍ∏∞(1Ïùº) Î°úÌÖåÏù¥ÏÖò
      #######################################################################
      - name: üåê DR(Ïû¨Ìï¥Î≥µÍµ¨) Î∞±ÏóÖ Î∞è 1Ïùº Î≥¥Í¥Ä Î°úÌÖåÏù¥ÏÖò
        continue-on-error: true
        env:
          DUMMY_KEY: ${{ secrets.DR_BACKUP_KEY }}
        run: |
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
          SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)

          mkdir -p "${DATA_ROOT}/${DR_DIR_NAME}"

          # DRÏö© ÎÇ†Ïßú ÎîîÎ†âÌÜ†Î¶¨ (Ïòà: dr_backup/2025-11-02/)
          DR_DATE_DIR="${DATA_ROOT}/${DR_DIR_NAME}/$(date +%Y-%m-%d)"
          mkdir -p "$DR_DATE_DIR"

          DR_COPY_PATH="${DR_DATE_DIR}/${SNAP_TAG}.tar.gz"
          DR_META_PATH="${DR_DATE_DIR}/${SNAP_TAG}.meta.txt"
          cp -f "$SNAP_FILE" "$DR_COPY_PATH" 2>/dev/null || true

          {
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE_LOCAL=$SNAP_FILE"
            echo "DR_STORED_FILE=$DR_COPY_PATH"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            if [ -n "$DUMMY_KEY" ]; then
              echo "DR_KEY_PRESENT=yes"
              echo "EXTERNAL_UPLOAD_STATUS=SUCCESS(simulated)"
            else
              echo "DR_KEY_PRESENT=no"
              echo "EXTERNAL_UPLOAD_STATUS=FAILED(no credentials)"
            fi
            echo "NOTE=Î°úÏª¨ DR ÏòÅÏó≠Ïóê ÏùºÎã® Ï†ÄÏû•Îê®. DR Î≥¥Ï°¥Ï£ºÍ∏∞ ÎßåÎ£åÎêú Ïä§ÎÉÖÏÉ∑ÏùÄ ÏïÑÎûò Îã®Í≥ÑÏóêÏÑú ÏÇ≠Ï†ú."
          } > "$DR_META_PATH"

          # 1Ïùº Ï¥àÍ≥ºÎêú DR Ïä§ÎÉÖÏÉ∑ Ï†ïÎ¶¨
          # -mtime +1 ÏùÄ "1ÏùºÎ≥¥Îã§ Ïò§ÎûòÎêú" ÏùòÎØ∏. DR_RETENTION_DAYS=1 Ï†ïÏ±Ö Î∞òÏòÅ.
          ROTATE_LOG="${DATA_ROOT}/logs/dr_rotation.log"
          {
            echo "=== DR ROTATION START ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "POLICY=keep ${DR_RETENTION_DAYS} day(s)"
            echo "ACTION=find ${DATA_ROOT}/${DR_DIR_NAME} -type f -mtime +${DR_RETENTION_DAYS} -delete"
            find "${DATA_ROOT}/${DR_DIR_NAME}" -type f -mtime +${DR_RETENTION_DAYS} -print
            find "${DATA_ROOT}/${DR_DIR_NAME}" -type f -mtime +${DR_RETENTION_DAYS} -delete || true
            echo "=== DR ROTATION END ==="
          } > "$ROTATE_LOG"

          DR_LOG="${DATA_ROOT}/logs/dr_backup_attempt.log"
          {
            echo "[DR BACKUP SIMULATION]"
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE=$SNAP_FILE"
            echo "DR_LOCAL_COPY=$DR_COPY_PATH"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            echo "RPO_TARGET=15min"
            echo "RTO_TARGET=30min"
            if [ -n "$DUMMY_KEY" ]; then
              echo "DR_KEY_PRESENT=yes (simulated external upload ok)"
            else
              echo "DR_KEY_PRESENT=no (external upload skipped/fail)"
            fi
          } > "$DR_LOG"

          echo "::notice::DR Î∞±ÏóÖ/DR Î°úÌÖåÏù¥ÏÖò(Î≥¥Í¥ÄÏ£ºÍ∏∞ 1Ïùº) Ï≤òÎ¶¨ ÏôÑÎ£å."
          head -n 80 "$DR_LOG" || true
          head -n 80 "$ROTATE_LOG" || true

      #######################################################################
      # 8. GitHub Release ÏóÖÎ°úÎìú ÏãúÎèÑ (ÏòµÏÖò)
      #######################################################################
      - name: üöÄ GitHub Release(Ïä§ÎÉÖÏÉ∑) ÏóÖÎ°úÎìú ÏãúÎèÑ
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          if [ -f "$SNAP_INFO" ]; then
            SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
            SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)
          fi

          if [ -n "${SNAP_TAG:-}" ] && [ -f "${SNAP_FILE:-}" ]; then
            echo "::notice::Î¶¥Î¶¨Ï¶à ÌÉúÍ∑∏ $SNAP_TAG, ÌååÏùº $SNAP_FILE ÏóÖÎ°úÎìú ÏãúÎèÑ Ï§ë"
            gh release create "$SNAP_TAG" "$SNAP_FILE" \
              --title "$SNAP_TAG" \
              --notes "ÏûêÎèô Teradata Î∞∞Ïπò Ïä§ÎÉÖÏÉ∑ (Í∞ÄÏÉÅ ÎåÄÎüâÎç∞Ïù¥ÌÑ∞, ÏóÖÍ∑∏Î†àÏù¥Îìú Î°úÍ∑∏, ÌÖåÏù¥Î∏î/Í∂åÌïú Ï†ïÏùò, ÌíàÏßàÍ≤ÄÏ¶ù, Í∞êÏÇ¨Î°úÍ∑∏, DBA ÏÑ±Îä•/ÎùΩ/Ïö©Îüâ/DR, DR Topology, DR 1Ïùº Î≥¥Í¥Ä Ï†ïÏ±Ö Ìè¨Ìï®)" \
              || echo "::warning::gh release create Ïã§Ìå® (Í∂åÌïú Î∂ÄÏ°± ÎòêÎäî ÌÉúÍ∑∏ Ï§ëÎ≥µ Í∞ÄÎä•)"
          else
            echo "::warning::Ïä§ÎÉÖÏÉ∑ Ï†ïÎ≥¥Í∞Ä ÏóÜÏñ¥ Î¶¥Î¶¨Ï¶à ÏÉùÎûµ"
          fi

      #######################################################################
      # 8.5 Í±∞Î≤ÑÎÑåÏä§/Ï±ÖÏûÑÏ∂îÏ†Å Î¨∏ÏÑú
      #######################################################################
      - name: üßë‚Äçüíº Í±∞Î≤ÑÎÑåÏä§ ÏäπÏù∏ & Îã¥ÎãπÏûê Í∏∞Î°ù
        run: |
          GOV_LOG="${DATA_ROOT}/governance/governance_approval.log"
          {
            echo "=== GOVERNANCE / APPROVAL LOG ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "DATA_OWNER=DATA_OWNER_X"
            echo "APPROVER=COMPLIANCE_TEAM"
            echo "ONCALL_TEAM=ETL_OnCall"
            echo "ONCALL_CONTACT=etloncall@example.local"
            echo "LAST_SCHEMA_CHANGE=Ïª¨Îüº AMOUNT Ï∂îÍ∞Ä / ÎØºÍ∞êÌïÑÎìú ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Î∞òÏòÅ"
            echo "SLA_CONTACT=BatchSRE"
            echo "BUSINESS_SLA=KST 06:00 Î¶¨Ìè¨Ìä∏ ÎßàÍ∞êÍπåÏßÄ ÏôÑÎ£å"
            echo "BUSINESS_IMPACT=ÏßÄÏó∞ Ïãú Î¶¨Ïä§ÌÅ¨ Ïä§ÏΩîÏñ¥ Î≥¥Í≥† ÏßÄÏó∞"
            echo "DR_POLICY=DR snapshot retention ${DR_RETENTION_DAYS} day(s) under ${DR_DIR_NAME}/"
            echo "DR_TOPOLOGY_DOC=${DR_TOPOLOGY_DIR}/dr_topology.txt"
          } > "$GOV_LOG"
          echo "::notice::Í±∞Î≤ÑÎÑåÏä§/ÏäπÏù∏/Ïò®ÏΩú Ï†ïÎ≥¥ Í∏∞Î°ù ÏôÑÎ£å -> $GOV_LOG"
          head -n 50 "$GOV_LOG" || true

      #######################################################################
      # 9. HTML ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
      #######################################################################
      - name: üñ® HTML ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
        run: |
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          SNAP_TAG="N/A"
          SNAP_FILE="N/A"
          if [ -f "$SNAP_INFO" ]; then
            SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
            SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)
          fi

          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          ACL_LOG="${DATA_ROOT}/logs/acl_audit.sql"
          HASH_LOG="${DATA_ROOT}/logs/snapshot_hash.txt"

          PARTITION_LOG="${DATA_ROOT}/logs/partition_access.log"
          LOCK_LOG="${DATA_ROOT}/logs/lock_contention.log"
          PLAN_LOG="${DATA_ROOT}/logs/query_plan_sample.log"
          TUNE_LOG="${DATA_ROOT}/logs/tuning_recommendations.log"
          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          STATS_LOG="${DATA_ROOT}/logs/stats_maintenance.log"
          CLASS_LOG="${DATA_ROOT}/governance/table_classification.log"
          DRPLAY="${DATA_ROOT}/governance/recovery_playbook.txt"
          DR_TOPO_FILE="${DATA_ROOT}/${DR_TOPOLOGY_DIR}/dr_topology.txt"
          ROTATE_LOG="${DATA_ROOT}/logs/dr_rotation.log"

          REPORT_HTML="${DATA_ROOT}/release/report_${GITHUB_RUN_ID}.html"
          {
            echo "<html><body style='font-family:monospace;'>"
            echo "<h1>Teradata FinOps Batch Summary</h1>"
            echo "<p><b>Timestamp:</b> $(date +%Y-%m-%dT%H:%M:%S%z)</p>"
            echo "<p><b>Repo:</b> $GITHUB_REPOSITORY</p>"
            echo "<p><b>Actor:</b> $GITHUB_ACTOR</p>"
            echo "<p><b>Branch:</b> $GITHUB_REF_NAME</p>"
            echo "<p><b>Commit:</b> $GITHUB_SHA</p>"
            echo "<p><b>Batch Target Table:</b> ${TARGET_TABLE}</p>"
            echo "<p><b>Tables Used:</b> ${TBL_STAGE}, ${TBL_FINAL}, ${TBL_AUDIT}</p>"
            echo "<p><b>Snapshot Tag:</b> $SNAP_TAG</p>"
            echo "<p><b>Snapshot File:</b> $SNAP_FILE</p>"
            echo "<p><b>DR Retention Policy:</b> ${DR_RETENTION_DAYS} day(s) in ${DR_DIR_NAME}/ (auto-rotation)</p>"

            echo "<hr /><h2>Pipeline Status</h2><pre>"
            cat "$STATUS_FILE" 2>/dev/null || echo "(no pipeline_status.log)"
            echo "</pre><h2>Quality Check</h2><pre>"
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo "</pre><h2>System Upgrade (head)</h2><pre>"
            head -n 40 "$UPG_LOG" 2>/dev/null || echo "(no system_upgrade.log)"
            echo "</pre><h2>ACL / Access Control</h2><pre>"
            head -n 80 "$ACL_LOG" 2>/dev/null || echo "(no acl_audit.sql)"

            echo "</pre><h2>DBA Performance & Capacity</h2><pre>"
            echo "--- Partition Access ---"
            head -n 80 "$PARTITION_LOG" 2>/dev/null || echo "(no partition_access.log)"
            echo
            echo "--- Lock Contention ---"
            head -n 80 "$LOCK_LOG" 2>/dev/null || echo "(no lock_contention.log)"
            echo
            echo "--- Query Plan Sample ---"
            head -n 80 "$PLAN_LOG" 2>/dev/null || echo "(no query_plan_sample.log)"
            echo
            echo "--- Tuning Recommendations ---"
            head -n 80 "$TUNE_LOG" 2>/dev/null || echo "(no tuning_recommendations.log)"
            echo
            echo "--- Stats Maintenance ---"
            head -n 80 "$STATS_LOG" 2>/dev/null || echo "(no stats_maintenance.log)"
            echo
            echo "--- Capacity Growth ---"
            head -n 80 "$CAP_LOG" 2>/dev/null || echo "(no capacity_growth.log)"

            echo "</pre><h2>Classification / DR / Snapshot Integrity</h2><pre>"
            echo "--- Table Classification ---"
            head -n 80 "$CLASS_LOG" 2>/dev/null || echo "(no table_classification.log)"
            echo
            echo "--- Recovery Playbook (RPO/RTO) ---"
            head -n 80 "$DRPLAY" 2>/dev/null || echo "(no recovery_playbook.txt)"
            echo
            echo "--- DR Topology ---"
            head -n 80 "$DR_TOPO_FILE" 2>/dev/null || echo "(no dr_topology.txt)"
            echo
            echo "--- DR Rotation (older than ${DR_RETENTION_DAYS} day purge) ---"
            head -n 80 "$ROTATE_LOG" 2>/dev/null || echo "(no dr_rotation.log)"
            echo
            echo "--- Snapshot Integrity Hash ---"
            cat "$HASH_LOG" 2>/dev/null || echo "(no snapshot_hash.txt)"
            echo "</pre>"

            echo "<p>-- End of Report --</p>"
            echo "</body></html>"
          } > "$REPORT_HTML"

          echo "::notice::HTML ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± -> $REPORT_HTML"
          head -n 40 "$REPORT_HTML" || true

      #######################################################################
      # 9. SLA ÌÉÄÏù¥Î®∏ Ï¢ÖÎ£å
      #######################################################################
      - name: ‚è± SLA ÌÉÄÏù¥Î®∏ Ï¢ÖÎ£å Î∞è Ïã§ÌñâÏãúÍ∞Ñ Í∏∞Î°ù
        if: ${{ github.event.inputs.sla_tracking != 'false' }}
        run: |
          END_EPOCH=$(date +%s)
          START_EPOCH=$(cat "${DATA_ROOT}/logs/start_epoch.txt" 2>/dev/null || echo "$END_EPOCH")
          DURATION_SEC=$((END_EPOCH-START_EPOCH))
          {
            echo "SLA_END_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SLA_DURATION_SEC=$DURATION_SEC"
            echo "BUSINESS_SLA=KST 06:00 Î¶¨Ìè¨Ìä∏ ÎßàÍ∞êÍπåÏßÄ ÏôÑÎ£å"
            echo "BUSINESS_IMPACT=ÏßÄÏó∞ Ïãú Î¶¨Ïä§ÌÅ¨ Ïä§ÏΩîÏñ¥ Î≥¥Í≥† ÏßÄÏó∞"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
          } >> "${DATA_ROOT}/logs/sla_timing.log"
          echo "::notice::Ïã§Ìñâ ÏãúÍ∞Ñ(SLA_DURATION_SEC=${DURATION_SEC}s) Í∏∞Î°ù ÏôÑÎ£å"

      #######################################################################
      # 10. Í∞êÏÇ¨ ÏöîÏïΩ Î°úÍ∑∏ (audit_run_summary.log)
      #######################################################################
      - name: üßæ Í∞êÏÇ¨ ÏöîÏïΩ Î°úÍ∑∏ ÏÉùÏÑ± (audit_run_summary.log)
        run: |
          AUDIT_FILE="${DATA_ROOT}/logs/audit_run_summary.log"

          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          SLA_FILE="${DATA_ROOT}/logs/sla_timing.log"
          RETENTION_LOG="${DATA_ROOT}/logs/retention_policy.log"
          DIFF_LOG="${DATA_ROOT}/logs/schema_diff.log"
          DR_LOG="${DATA_ROOT}/logs/dr_backup_attempt.log"
          GOV_LOG="${DATA_ROOT}/governance/governance_approval.log"
          HASH_LOG="${DATA_ROOT}/logs/snapshot_hash.txt"

          PARTITION_LOG="${DATA_ROOT}/logs/partition_access.log"
          LOCK_LOG="${DATA_ROOT}/logs/lock_contention.log"
          PLAN_LOG="${DATA_ROOT}/logs/query_plan_sample.log"
          TUNE_LOG="${DATA_ROOT}/logs/tuning_recommendations.log"
          STATS_LOG="${DATA_ROOT}/logs/stats_maintenance.log"
          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          CLASS_LOG="${DATA_ROOT}/governance/table_classification.log"
          DRPLAY="${DATA_ROOT}/governance/recovery_playbook.txt"
          DR_TOPO_FILE="${DATA_ROOT}/${DR_TOPOLOGY_DIR}/dr_topology.txt"
          ROTATE_LOG="${DATA_ROOT}/logs/dr_rotation.log"

          SNAP_TAG="N/A"
          SNAP_FILE="N/A"
          if [ -f "$SNAP_INFO" ]; then
            SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
            SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)
          fi

          {
            echo "=== PIPELINE AUDIT SUMMARY ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "BRANCH=$GITHUB_REF_NAME"
            echo "COMMIT_SHA=$GITHUB_SHA"
            echo
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "TABLES_USED=${TBL_STAGE},${TBL_FINAL},${TBL_AUDIT}"
            echo "GEN_ROWS=${GEN_ROWS}"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            echo
            echo "--- STATUS_FILE ---"
            cat "$STATUS_FILE" 2>/dev/null || echo "(no pipeline_status.log)"
            echo
            echo "--- QUALITY_LOG ---"
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo
            echo "--- SLA_TIMING ---"
            cat "$SLA_FILE" 2>/dev/null || echo "(no sla_timing.log)"
            echo
            echo "--- SYSTEM UPGRADE LOG (head 40) ---"
            head -n 40 "$UPG_LOG" 2>/dev/null || echo "(no system_upgrade.log)"
            echo
            echo "--- RETENTION POLICY ---"
            cat "$RETENTION_LOG" 2>/devnull || cat "$RETENTION_LOG" 2>/dev/null || echo "(no retention_policy.log)"
            echo
            echo "--- SCHEMA DIFF ---"
            head -n 80 "$DIFF_LOG" 2>/dev/null || echo "(no schema_diff.log)"
            echo
            echo "--- DR BACKUP (incl. DR rotation ${DR_RETENTION_DAYS} day) ---"
            head -n 60 "$DR_LOG" 2>/dev/null || echo "(no dr_backup_attempt.log)"
            echo ">>> DR ROTATION LOG"
            head -n 60 "$ROTATE_LOG" 2>/dev/null || echo "(no dr_rotation.log)"
            echo
            echo "--- GOVERNANCE / APPROVAL ---"
            head -n 60 "$GOV_LOG" 2>/dev/null || echo "(no governance_approval.log)"
            echo
            echo "--- DBA PERFORMANCE / PARTITION / LOCK / PLAN / TUNING ---"
            echo ">>> partition_access.log"
            head -n 60 "$PARTITION_LOG" 2>/dev/null || echo "(no partition_access.log)"
            echo ">>> lock_contention.log"
            head -n 60 "$LOCK_LOG" 2>/dev/null || echo "(no lock_contention.log)"
            echo ">>> query_plan_sample.log"
            head -n 60 "$PLAN_LOG" 2>/dev/null || echo "(no query_plan_sample.log)"
            echo ">>> tuning_recommendations.log"
            head -n 60 "$TUNE_LOG" 2>/dev/null || echo "(no tuning_recommendations.log)"
            echo ">>> stats_maintenance.log"
            head -n 60 "$STATS_LOG" 2>/dev/null || echo "(no stats_maintenance.log)"
            echo ">>> capacity_growth.log"
            head -n 60 "$CAP_LOG" 2>/dev/null || echo "(no capacity_growth.log)"
            echo
            echo "--- CLASSIFICATION / DR PLAYBOOK / DR TOPOLOGY ---"
            echo ">>> table_classification.log"
            head -n 60 "$CLASS_LOG" 2>/dev/null || echo "(no table_classification.log)"
            echo ">>> recovery_playbook.txt (RPO/RTO)"
            head -n 60 "$DRPLAY" 2>/dev/null || echo "(no recovery_playbook.txt)"
            echo ">>> dr_topology.txt"
            head -n 60 "$DR_TOPO_FILE" 2>/dev/null || echo "(no dr_topology.txt)"
            echo
            echo "--- SNAPSHOT INFO ---"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE=$SNAP_FILE"
            echo "DATA_ROOT=$DATA_ROOT"
            echo "--- SNAPSHOT HASH ---"
            cat "$HASH_LOG" 2>/dev/null || echo "(no snapshot_hash.txt)"
          } > "$AUDIT_FILE"

          echo "::notice::Í∞êÏÇ¨ ÏöîÏïΩ Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $AUDIT_FILE"
          head -n 200 "$AUDIT_FILE" || true

      #######################################################################
      # 11. Artifact ÏóÖÎ°úÎìú
      #######################################################################
      - name: üì¶ Ïã§Ìñâ ÏÇ∞Ï∂úÎ¨º ÏóÖÎ°úÎìú (logs / quality / source / snapshot Îì±)
        uses: actions/upload-artifact@v4
        with:
          name: teradata-run-${{ github.run_id }}
          path: |
            ${{ env.DATA_ROOT }}/logs/**
            ${{ env.DATA_ROOT }}/quality/**
            ${{ env.DATA_ROOT }}/source_files/**
            ${{ env.DATA_ROOT }}/release/**
            ${{ env.DATA_ROOT }}/landing_zone/**
            ${{ env.DATA_ROOT }}/${{ env.DR_DIR_NAME }}/**
            ${{ env.DATA_ROOT }}/${{ env.DR_TOPOLOGY_DIR }}/**
            ${{ env.DATA_ROOT }}/extract_out/**
            ${{ env.DATA_ROOT }}/tmp/**
            ${{ env.DATA_ROOT }}/health/**
            ${{ env.DATA_ROOT }}/governance/**
            ${{ env.DATA_ROOT }}/history/**
          if-no-files-found: warn
          retention-days: 14

      #######################################################################
      # 12. ÏµúÏ¢Ö Ï¢ÖÎ£å (Ìï≠ÏÉÅ ÏÑ±Í≥µ)
      #######################################################################
      - name: ‚úÖ ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å (Ìï≠ÏÉÅ ÏÑ±Í≥µ)
        if: always()
        run: |
          echo "‚úÖ Teradata FinOps Batch ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ÑÏ≤¥ Îã®Í≥Ñ ÏàòÌñâ ÏôÑÎ£å."
          echo "   - ÏãúÏä§ÌÖú ÏóÖÍ∑∏Î†àÏù¥Îìú & pkg diff Í∏∞Î°ù"
          echo "   - ÎåÄÎüâ Í∞ÄÏÉÅÎç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÌíàÏßàÍ≤ÄÏ¶ù(Ï¶ùÍ∞êÎ•†/Í≥†ÏúÑÌóòÎπÑÏú®)"
          echo "   - ÌååÌã∞ÏÖò/Î≥¥Ï°¥ Ï†ïÏ±Ö Î°úÍ∑∏, ÎßàÏä§ÌÇπ/Í∂åÌïú/Í±∞Î≤ÑÎÑåÏä§/Ïò®ÏΩú/ÏäπÏù∏ Í∏∞Î°ù"
          echo "   - DBA Í¥ÄÏ†ê: ÌååÌã∞ÏÖò ÌîÑÎ£®Îãù, ÎùΩ Í≤ΩÌï©, Ïã§Ìñâ Í≥ÑÌöç, ÌäúÎãù Í∂åÏû•, ÌÜµÍ≥Ñ ÏàòÏßë, Ïö©Îüâ Ï∂îÏ†Å"
          echo "   - DR Îç∞Ïù¥ÌÑ∞ÏÑºÌÑ∞ ÌÜ†Ìè¥Î°úÏßÄ Î¨∏ÏÑúÌôî"
          echo "   - DR Î∞±ÏóÖ Ïä§ÎÉÖÏÉ∑ Ï†ÄÏû• Î∞è 1Ïùº Î≥¥Í¥ÄÏ£ºÍ∏∞ ÏûêÎèô Î°úÌÖåÏù¥ÏÖò"
          echo "   - RPO/RTO, Î≥µÍµ¨ ÌîåÎ†àÏù¥Î∂Å, DR Î≥¥Í¥ÄÏ£ºÍ∏∞(1Ïùº) Í∞êÏÇ¨ Í∏∞Î°ù"
          echo "   - Ïä§ÎÉÖÏÉ∑ tar.gz + Î¨¥Í≤∞ÏÑ± Ìï¥Ïãú + (ÏòµÏÖò) DR Ïô∏Î∂Ä ÏóÖÎ°úÎìú + GitHub Release ÏãúÎèÑ"
          echo "   - HTML Í≤ΩÏòÅ ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ & audit_run_summary.log ÏÉùÏÑ±"
          echo "Î™®Îì† ÏÇ∞Ï∂úÎ¨ºÏù¥ artifact teradata-run-${GITHUB_RUN_ID} Î°ú ÏóÖÎ°úÎìúÎêòÏóàÏäµÎãàÎã§."
          exit 0
