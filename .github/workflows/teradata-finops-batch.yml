name: "üè¶ Teradata FinOps Batch ‚Äî EchoOps + Audit + Snapshot + ISO + DR(1ÏùºÎ≥¥Í¥Ä) + DBA + SafeEcho"

on:
  push:
    branches: [ "main" ]

  # ÏàòÎèô Ïã§Ìñâ (dispatch inputsÎäî 10Í∞ú Ïù¥Ìïò)
  workflow_dispatch:
    inputs:
      mode:
        description: "Ïã§Ìñâ Î™®Îìú(full=Ï†ÑÏ≤¥ / lite=ÏùºÎ∂Ä Îã®Í≥ÑÎßå)"
        type: choice
        options: [full, lite]
        default: full
      gen_rows:
        description: "ÏÉùÏÑ±Ìï† Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞ Ìñâ Ïàò (ÎπàÍ∞íÏù¥Î©¥ Í∏∞Î≥∏ GEN_ROWS)"
        required: false
        default: ""
      expected_rows:
        description: "ÌíàÏßàÍ≤ÄÏ¶ù Í∏∞ÎåÄ Î°úÏö∞ Ïàò (ÎπàÍ∞íÏù¥Î©¥ Í∏∞Î≥∏ EXPECTED_ROWCOUNT)"
        required: false
        default: ""
      dr_backup:
        description: "DR(Ïô∏Î∂Ä) Î∞±ÏóÖ ÏãúÎèÑ Ïó¨Î∂Ä (true/false)"
        type: choice
        options: ["true", "false"]
        default: "false"
      include_masking_audit:
        description: "ÎØºÍ∞êÌïÑÎìú ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Î°úÍ∑∏ Ìè¨Ìï® Ïó¨Î∂Ä"
        type: choice
        options: ["true", "false"]
        default: "true"
      include_healthcheck:
        description: "ÏãúÏä§ÌÖú Ìó¨Ïä§/Î≥¥Ïïà Ï†êÍ≤Ä Î°úÍ∑∏ Ìè¨Ìï® Ïó¨Î∂Ä"
        type: choice
        options: ["true", "false"]
        default: "true"
      include_schema_diff:
        description: "DDL/TPT Î≥ÄÍ≤Ω diff Í∏∞Î°ù Ïó¨Î∂Ä"
        type: choice
        options: ["true", "false"]
        default: "true"
      sla_tracking:
        description: "SLA/SLO ÏãúÍ∞Ñ Ï∏°Ï†ï, Ïû¨ÏãúÎèÑ Í∏∞Î°ù Ïó¨Î∂Ä"
        type: choice
        options: ["true", "false"]
        default: "true"
      echo_trace:
        description: "Î™®Îì† Îã®Í≥ÑÏóêÏÑú echo(set -x) ÌôúÏÑ±Ìôî Ïó¨Î∂Ä"
        type: choice
        options: ["true", "false"]
        default: "false"
      debug_mode:
        description: "ÎîîÎ≤ÑÍ∑∏ Î™®Îìú (lite + echo_trace Í∞ïÏ†ú ON)"
        type: choice
        options: ["true", "false"]
        default: "false"

  # Îß§Ïùº 15:30 UTC == ÌïúÍµ≠ÏãúÍ∞Ñ(KST) ÏÉàÎ≤Ω 00:30 ÏûêÎèô ÏïºÍ∞ÑÎ∞∞Ïπò
  schedule:
    - cron: "30 15 * * *"

permissions:
  contents: write # Î¶¥Î¶¨Ï¶à ÌÉúÍ∑∏/ÏûêÏÇ∞ ÏóÖÎ°úÎìúÏö© (gh release create)

env:
  ###########################################################################
  # Í≥µÌÜµ Í≤ΩÎ°ú / ÎåÄÏÉÅ ÌÖåÏù¥Î∏î
  ###########################################################################
  DATA_ROOT: /home/runner/td_data          # EchoOps Í≥µÌÜµ Î°úÍ∑∏ ÎîîÎ†âÌÜ†Î¶¨ (Î°úÏª¨ ÏàòÏßëÏö©)
  LOG_DIR: .github/echo_logs
  # ÏÑúÎπÑÏä§/ÎèÑÎ©îÏù∏Î≥Ñ ÌÖåÏù¥Î∏î (Staging -> Final -> Audit)
  TBL_STAGE: STG_DATA
  TBL_FINAL: FINAL_DATA
  TBL_AUDIT: LOAD_AUDIT_LOG
  # Î≥ëÌï© ÌÉÄÍπÉ Ïö¥ÏòÅ ÌÖåÏù¥Î∏î
  TARGET_TABLE: FINAL_DATA

  ###########################################################################
  # Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞/ÌíàÏßà Í∏∞Ï§Ä
  ###########################################################################
  GEN_ROWS: "500000"
  EXPECTED_ROWCOUNT: "500000"

  ###########################################################################
  # DR / Î≥¥Í¥ÄÏ£ºÍ∏∞ Ï†ïÏ±Ö
  ###########################################################################
  DR_RETENTION_DAYS: "1" # DR Îç∞Ïù¥ÌÑ∞ÏÑºÌÑ∞ Î≥¥Í¥ÄÏ£ºÍ∏∞ = 1Ïùº
  DR_DIR_NAME: "dr_backup" # DR Ïä§ÎÉÖÏÉ∑ Î°úÏª¨ Î≥¥Í¥Ä ÎîîÎ†âÌÜ†Î¶¨
  DR_TOPOLOGY_DIR: "governance/dr_site"

jobs:
  finops_pipeline:
    runs-on: ubuntu-latest

    steps:
      #######################################################################
      # 0. ÏΩîÎìú Ï≤¥ÌÅ¨ÏïÑÏõÉ
      #######################################################################
      - name: üì• ÏΩîÎìú Ï≤¥ÌÅ¨ÏïÑÏõÉ
        uses: actions/checkout@v4

      #######################################################################
      # 1. Ï†ÑÏó≠ echo_trace/Debug Î™®Îìú ÏÑ§Ï†ï
      #######################################################################
      - name: üîä Ï†ÑÏó≠ echo_trace & Debug Î™®Îìú ÌÜ†Í∏Ä
        run: |
          # debug_mode=true Ïù¥Î©¥ echo_trace Í∞ïÏ†ú ON
          if [ "${{ github.event.inputs.debug_mode || 'false' }}" = "true" ]; then
            echo "ECHO_TRACE_GLOBAL=true" >> "$GITHUB_ENV"
            echo "::notice::Debug mode ON ‚Üí Global echo_trace = ON"
          else
            if [ "${{ github.event.inputs.echo_trace || 'false' }}" = "true" ]; then
              echo "ECHO_TRACE_GLOBAL=true" >> "$GITHUB_ENV"
              echo "::notice::Global echo_trace = ON"
            else
              echo "ECHO_TRACE_GLOBAL=false" >> "$GITHUB_ENV"
              echo "::notice::Global echo_trace = OFF"
            fi
          fi

      #######################################################################
      # 2. EchoOps Helper Î∂ÄÌä∏Ïä§Ìä∏Îû© (/tmp/echo_helpers.sh ÏÉùÏÑ±)
      #######################################################################
      - name: ‚öôÔ∏è Bootstrap EchoOps Helper
        run: |
          set -e
          mkdir -p "${DATA_ROOT}"/{logs,tmp,release,history,quality,landing_zone,governance,health,extract_out}
          mkdir -p "${DATA_ROOT}/${DR_DIR_NAME}"
          mkdir -p "${DATA_ROOT}/${DR_TOPOLOGY_DIR}"
          mkdir -p "${LOG_DIR}"

          cat > /tmp/echo_helpers.sh <<'EOSH'
          #!/usr/bin/env bash
          set -Eeuo pipefail

          : "${DATA_ROOT:=/home/runner/td_data}"
          : "${LOG_DIR:=.github/echo_logs}"

          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"

          ts() { date +%Y-%m-%dT%H:%M:%S%z; }

          echo_note() {
            printf "‚ñ∂ %s %s\n" "$(ts)" "$*" | tee -a "${LOG_DIR}/echo.log"
          }

          kv_set() {
            local file="$1" key="$2" val="$3"
            mkdir -p "$(dirname "$file")"; touch "$file"
            if grep -qE "^${key}=" "$file" 2>/dev/null; then
              sed -i "s|^${key}=.*|${key}=${val}|g" "$file"
            else
              printf "%s=%s\n" "$key" "$val" >> "$file"
            fi
          }

          kv_get() {
            local file="$1" key="$2"
            if [ -f "$file" ]; then
              local line
              line="$(grep -E "^${key}=" "$file" 2>/dev/null || true)"
              if [ -n "$line" ]; then
                echo "${line#*=}"
                return 0
              fi
            fi
            echo ""
            return 0
          }

          snapshot_set() { kv_set "$SNAP_INFO" "$1" "$2"; }
          snapshot_get() { kv_get "$SNAP_INFO" "$1"; }

          # Ï†ÑÏó≠ echo_trace ÌÜ†Í∏Ä Ïãú trace ÌôúÏÑ±Ìôî
          if [ "${ECHO_TRACE_GLOBAL:-}" = "true" ]; then
            export PS4='+ $(date "+%Y-%m-%dT%H:%M:%S%z") ${BASH_SOURCE##*/}:${LINENO}: '
            set -x
          fi

          trap 'echo_note "‚ö†Ô∏è Error (exit code $?) in ${BASH_SOURCE##*/}:${LINENO}"' ERR
          EOSH

          chmod +x /tmp/echo_helpers.sh
          source /tmp/echo_helpers.sh
          echo_note "EchoOps Helper Ï§ÄÎπÑ ÏôÑÎ£å (/tmp/echo_helpers.sh)"

      #######################################################################
      # 2.5 ÏÇ¨Ï†Ñ ÌïÑÏàò Ï°∞Í±¥ Ï†êÍ≤Ä (Health Check: Secrets/Inputs)
      #######################################################################
      - name: üîé ÏÇ¨Ï†Ñ ÌïÑÏàò Ï°∞Í±¥ Ï†êÍ≤Ä (Secrets/Inputs)
        run: |
          set -e
          source /tmp/echo_helpers.sh

          MISSING=0

          # dr_backup=true Ïù∏Îç∞ DR_BACKUP_KEY ÏóÜÏúºÎ©¥ Ïò§Î•ò
          if [ "${{ github.event.inputs.dr_backup || 'false' }}" = "true" ] && [ -z "${{ secrets.DR_BACKUP_KEY }}" ]; then
            echo_note "‚ùå dr_backup=true Ïù∏Îç∞ DR_BACKUP_KEY secret Ïù¥ ÏóÜÏäµÎãàÎã§."
            MISSING=1
          fi

          # Slack Webhook ÏóÜÎäî Í≤ΩÏö∞Îäî ÌïÑÏàòÎäî ÏïÑÎãàÎØÄÎ°ú Í≤ΩÍ≥†Îßå
          if [ -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
            echo_note "‚ö†Ô∏è SLACK_WEBHOOK_URL secret ÏóÜÏùå ‚Üí Slack ÏïåÎ¶º Îã®Í≥ÑÎäî ÏûêÎèôÏúºÎ°ú Ïã§Ìå®/Ïä§ÌÇµÎê† Ïàò ÏûàÏäµÎãàÎã§."
          fi

          if [ "$MISSING" -ne 0 ]; then
            echo_note "ÌïÑÏàò Ï°∞Í±¥ ÎØ∏Ï∂©Ï°± ‚Üí ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ï§ëÎã®"
            exit 1
          fi

          echo_note "ÏÇ¨Ï†Ñ ÌïÑÏàò Ï°∞Í±¥ Ï†êÍ≤Ä ÌÜµÍ≥º"

      #######################################################################
      # 3. ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏãúÏûë Î°úÍπÖ + SLA ÌÉÄÏù¥Î®∏ ÏãúÏûë
      #######################################################################
      - name: üìù ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏãúÏûë & SLA ÌÉÄÏù¥Î®∏ ÏãúÏûë
        run: |
          set -e
          source /tmp/echo_helpers.sh
          echo_note "Teradata ÌÜµÌï© ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏãúÏûë"
          echo_note "DATA_ROOT=${DATA_ROOT}, TARGET_TABLE=${TARGET_TABLE}, GEN_ROWS=${GEN_ROWS}"
          echo_note "dispatch.mode=${{ github.event.inputs.mode || 'N/A' }}"
          echo_note "dispatch.dr_backup=${{ github.event.inputs.dr_backup || 'N/A' }}"
          echo_note "dispatch.echo_trace=${{ github.event.inputs.echo_trace || 'false' }}"
          echo_note "dispatch.debug_mode=${{ github.event.inputs.debug_mode || 'false' }}"
          mkdir -p "${DATA_ROOT}/logs"
          date +%s > "${DATA_ROOT}/logs/start_epoch.txt"
          { echo "SLA_START_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"; } > "${DATA_ROOT}/logs/sla_timing.log"

      #######################################################################
      # 4. DR Îç∞Ïù¥ÌÑ∞ÏÑºÌÑ∞ Íµ¨Ï°∞ÎèÑ/Ï†ïÏ±Ö Î¨∏ÏÑú ÏÉùÏÑ±
      #######################################################################
      - name: üè¢ DR Îç∞Ïù¥ÌÑ∞ÏÑºÌÑ∞ Íµ¨Ï°∞ÎèÑ & Ï†ïÏ±Ö Í∏∞Î°ù
        run: |
          set -e
          source /tmp/echo_helpers.sh
          TOPO_DIR="${DATA_ROOT}/${DR_TOPOLOGY_DIR}"
          mkdir -p "$TOPO_DIR"
          DR_TOPO_FILE="${TOPO_DIR}/dr_topology.txt"
          {
            echo "=== DR DATACENTER TOPOLOGY ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo
            echo "[Primary Site]"
            echo "- PROD Teradata / FINAL_DATA Ïö¥ÏòÅ"
            echo "- Batch ETL (STG_DATA -> FINAL_DATA)"
            echo "- Compliance / Risk Analytics"
            echo
            echo "[DR Site]"
            echo "- Warm standby Teradata or compatible warehouse"
            echo "- Daily snapshot import of FINAL_DATA partitions"
            echo "- Read-only dashboards for Risk/Compliance"
            echo
            echo "[Network Zones]"
            echo "- prod-etl-zone (private)"
            echo "- dr-recovery-zone (isolated / limited inbound)"
            echo "- mgmt-zone (jump/bastion for DBA+SRE only)"
            echo
            echo "[Replication / Snapshot Flow]"
            echo "1) Batch completes in Primary."
            echo "2) Snapshot tar.gz & ISO are generated."
            echo "3) Copy snapshot to DR storage bucket / ${DR_DIR_NAME}/YYYYMMDD/ (simulated)."
            echo "4) DR can restore that batch partition on demand."
            echo
            echo "[Ownership / Escalation]"
            echo "- BatchSRE: owns RTO (restore time)."
            echo "- Compliance: data approval."
            echo "- DBA_TEAM: partition restore procedure."
            echo
            echo "[RPO / RTO Targets]"
            echo "- RPO_TARGET=15min"
            echo "- RTO_TARGET=30min"
            echo
            echo "[Retention Policy]"
            echo "- DR snapshot retention: ${DR_RETENTION_DAYS} day(s)"
            echo "- Rotation job auto-removes snapshots older than ${DR_RETENTION_DAYS} day(s)"
            echo
            echo "NOTE=Ïù¥ Î¨∏ÏÑúÎäî ÏûêÎèôÏúºÎ°ú ÏÉùÏÑ±ÎêòÎ©∞ Í∞êÏÇ¨Ïóê Ìè¨Ìï®Îê©ÎãàÎã§."
          } > "$DR_TOPO_FILE"
          echo_note "DR Îç∞Ïù¥ÌÑ∞ÏÑºÌÑ∞ ÌÜ†Ìè¥Î°úÏßÄ Í∏∞Î°ù ÏôÑÎ£å -> $DR_TOPO_FILE"
          ls -R "${DATA_ROOT}/${DR_TOPOLOGY_DIR}" || true

      #######################################################################
      # 5. Îü¨ÎÑà ÏãúÏä§ÌÖú ÏóÖÍ∑∏Î†àÏù¥Îìú (mode=full Ïù¥Í≥† debug_mode=false Ïùº ÎïåÎßå)
      #######################################################################
      - name: üîÑ Îü¨ÎÑà Ìå®ÌÇ§ÏßÄ ÏóÖÍ∑∏Î†àÏù¥Îìú (continue-on-error)
        if: ${{ github.event.inputs.mode == 'full' && github.event.inputs.debug_mode != 'true' }}
        continue-on-error: true
        run: |
          set +e
          source /tmp/echo_helpers.sh
          set +e
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          BEFORE_LIST="${DATA_ROOT}/logs/pkg_list_before.txt"
          AFTER_LIST="${DATA_ROOT}/logs/pkg_list_after.txt"
          dpkg -l > "$BEFORE_LIST" 2>/dev/null || true
          {
            echo "===== SYSTEM UPGRADE START ====="
            date
            echo "--- apt-get update ---"
            sudo apt-get update -y || echo "[WARN] apt-get update Ïã§Ìå®"
            echo "--- apt-get dist-upgrade ---"
            sudo apt-get -o Dpkg::Options::="--force-confnew" dist-upgrade -y || echo "[WARN] dist-upgrade Ïã§Ìå®"
            echo "--- apt-get autoremove ---"
            sudo apt-get autoremove -y || true
            echo "--- uname -a ---"
            uname -a
            echo "--- lsb_release -a ---"
            lsb_release -a 2>/dev/null || echo "lsb_release not available"
            echo "===== SYSTEM UPGRADE END ====="
          } > "$UPG_LOG" 2>&1
          dpkg -l > "$AFTER_LIST" 2>/dev/null || true
          diff -u "$BEFORE_LIST" "$AFTER_LIST" > "${DATA_ROOT}/logs/pkg_upgrade_diff.txt" || true
          echo_note "ÏãúÏä§ÌÖú ÏóÖÍ∑∏Î†àÏù¥Îìú(Ìå®ÌÇ§ÏßÄ ÏµúÏã†Ìôî) ÏãúÎèÑ ÏôÑÎ£å. ÏÉÅÏÑ∏ ${UPG_LOG}"

      #######################################################################
      # 6. ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞ & Î≥¥Ï°¥ Ï†ïÏ±Ö Î°úÍ∑∏
      #######################################################################
      - name: üìÇ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ± Î∞è Ï†ïÏ±Ö Í∏∞Î°ù
        run: |
          set -e
          source /tmp/echo_helpers.sh
          echo_note "ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞ ÏÉùÏÑ± Î∞è ÌôïÏù∏"
          mkdir -p "${DATA_ROOT}"/{source_files,landing_zone,logs,archive,extract_out,tmp,quality,release,history,governance,health,"${DR_DIR_NAME}"}
          ls -R "${DATA_ROOT}" || true
          cat > "${DATA_ROOT}/logs/retention_policy.log" <<'EOF_RETENTION'
          [RETENTION POLICY SIMULATION]
          - STG_DATA: ÏùºÏûê ÌååÌã∞ÏÖò, 30Ïùº Ï¥àÍ≥º ÌååÌã∞ÏÖòÏùÄ ÏïÑÏπ¥Ïù¥Î∏å ÌõÑ ÏÇ≠Ï†ú ÎåÄÏÉÅ
          - FINAL_DATA: ÏòÅÍµ¨ Î≥¥Ï°¥, Îã® Í∞úÏù∏Ï†ïÎ≥¥ ÌïÑÎìúÎäî ÎßàÏä§ÌÇπ ÏÉÅÌÉúÎßå Ïú†ÏßÄ
          - LOAD_AUDIT_LOG: 1ÎÖÑ Î≥¥Ï°¥ ÌõÑ ÏΩúÎìúÏä§ÌÜ†Î¶¨ÏßÄ Ïù¥Ï†Ñ
          - DR_BACKUP/*: DR_RETENTION_DAYS Ïù¥ÌõÑ Î°úÌÖåÏù¥ÏÖò ÏÇ≠Ï†ú
          EOF_RETENTION

      #######################################################################
      # 7. Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞ ÎåÄÎüâ ÏÉùÏÑ±
      #######################################################################
      - name: üèó Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞ ÎåÄÎüâ ÏÉùÏÑ±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          ROWS_INPUT="${{ github.event.inputs.gen_rows || '' }}"
          if [ -n "$ROWS_INPUT" ]; then
            ROWS="$ROWS_INPUT"
          else
            ROWS="${GEN_ROWS}"
          fi
          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"
          echo "COL1,COL2,AMOUNT,LOAD_TS" > "$SRC_FILE"
          echo_note "Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞ ${ROWS}Ìñâ ÏÉùÏÑ± ÏãúÏûë -> $SRC_FILE"
          i=1
          while [ $i -le $ROWS ]; do
            AMT=$(( (RANDOM % 999800) + 100 ))
            MERCH=$(( (RANDOM % 9000) + 1000 ))
            if [ $AMT -gt 900000 ]; then
              NOTE="MERCHANT_${MERCH}_FLAG-HIGH_${i}"
            else
              NOTE="MERCHANT_${MERCH}_NOTE_${i}"
            fi
            echo "${i},${NOTE},${AMT},$(date +%Y-%m-%dT%H:%M:%S%z)" >> "$SRC_FILE"
            i=$((i+1))
          done
          ls -lh "$SRC_FILE"
          head -n 5 "$SRC_FILE"
          tail -n 5 "$SRC_FILE"
          echo "${ROWS}" > "${DATA_ROOT}/tmp/generated_rowcount.txt"
          echo_note "Í∞ÄÏÉÅ CSV ÏÉùÏÑ± ÏôÑÎ£å (${ROWS} rows)"

      #######################################################################
      # 8. Îü∞ÌÉÄÏûÑ ÌôòÍ≤Ω Ïä§ÎÉÖÏÉ∑
      #######################################################################
      - name: üîç Îü∞ÌÉÄÏûÑ ÌôòÍ≤Ω Ïä§ÎÉÖÏÉ∑ Ï†ÄÏû•
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SNAP="${DATA_ROOT}/logs/env_snapshot.txt"
          {
            echo "===== ENV SNAPSHOT ====="
            date
            uname -a
            whoami
            echo "--- PATH ---"
            echo "$PATH"
            echo "--- DISK (df -h) ---"
            df -h
            echo "--- MEMORY (free -m) ---"
            free -m || true
            echo "--- GITHUB CONTEXT ---"
            echo "RUN_ID=$GITHUB_RUN_ID"
            echo "RUN_NUMBER=$GITHUB_RUN_NUMBER"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "SHA=$GITHUB_SHA"
            echo "BRANCH=$GITHUB_REF_NAME"
          } > "$SNAP"
          echo_note "ÌôòÍ≤Ω Ïä§ÎÉÖÏÉ∑ Í∏∞Î°ù ÏôÑÎ£å -> $SNAP"

      #######################################################################
      # 9. ÏãúÏä§ÌÖú Ìó¨Ïä§ / Î≥¥Ïïà Ï†êÍ≤Ä
      #######################################################################
      - name: ü©∫ ÏãúÏä§ÌÖú Ìó¨Ïä§ Î∞è Î≥¥Ïïà Ï†êÍ≤Ä
        if: ${{ github.event.inputs.include_healthcheck != 'false' }}
        run: |
          set -e
          source /tmp/echo_helpers.sh
          HEALTH_LOG="${DATA_ROOT}/health/system_health.log"
          {
            echo "===== SYSTEM HEALTH CHECK ====="
            date
            echo "--- dmesg (tail 50) ---"
            dmesg | tail -n 50 || true
            echo
            echo "--- TCP/UDP ÏÜåÏºì ÏÉÅÌÉú (ss -tuna head 20) ---"
            ss -tuna | head -n 20 || true
            echo
            echo "--- CPU/MEM load (top -b -n1 head 20) ---"
            top -b -n1 | head -n 20 || true
            echo
            echo "--- I/O stat (iostat if available) ---"
            iostat 2>/dev/null || echo "iostat not available"
            echo
            echo "--- Ïô∏Î∂Ä Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ (curl example.com) ---"
            curl -I https://example.com 2>&1 | head -n 5 || echo "curl external check failed or blocked"
          } > "$HEALTH_LOG"
          echo_note "ÏãúÏä§ÌÖú Ìó¨Ïä§ Ï≤¥ÌÅ¨ Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $HEALTH_LOG"

      #######################################################################
      # 10. DDL/TPT/ÏøºÎ¶¨ Ï†ïÏùò Î∞±ÏóÖ
      #######################################################################
      - name: üß± DDL/TPT/ÏøºÎ¶¨ Ïä§ÌÅ¨Î¶ΩÌä∏ Î∞±ÏóÖ
        run: |
          set -e
          source /tmp/echo_helpers.sh
          mkdir -p "${DATA_ROOT}/source_files/sql"
          mkdir -p "${DATA_ROOT}/source_files/tpt"

          cat > "${DATA_ROOT}/source_files/sql/create_tables.sql" <<'SQL_CREATE'
          -- [STAGING TABLE]
          CREATE TABLE STG_DATA (
            COL1 INTEGER,
            COL2 VARCHAR(100),
            AMOUNT BIGINT,
            LOAD_TS TIMESTAMP,
            BATCH_ID VARCHAR(40)
          );

          -- [FINAL TABLE]
          CREATE TABLE FINAL_DATA (
            COL1 INTEGER,
            COL2 VARCHAR(100),
            AMOUNT BIGINT,
            LOAD_TS TIMESTAMP,
            ETL_TS TIMESTAMP,
            SRC_BATCH VARCHAR(40)
          );

          -- [AUDIT TABLE]
          CREATE TABLE LOAD_AUDIT_LOG (
            AUDIT_TS TIMESTAMP,
            BATCH_ID VARCHAR(40),
            SRC_FILE VARCHAR(255),
            ROW_LOADED INTEGER,
            ROW_EXPECTED INTEGER,
            STATUS_CODE INTEGER,
            STATUS_MESSAGE VARCHAR(2000),
            OPERATOR VARCHAR(128)
          );

          CREATE INDEX IDX_FINAL_DATA_COL1 ON FINAL_DATA (COL1);
          CREATE INDEX IDX_AUDIT_BATCH ON LOAD_AUDIT_LOG (BATCH_ID);
          SQL_CREATE

          cat > "${DATA_ROOT}/source_files/sql/proc_LOAD_AND_MERGE.sql" <<'SQL_PROC'
          REPLACE PROCEDURE LOAD_AND_MERGE (
            IN p_batch_id VARCHAR(40),
            IN p_src_file VARCHAR(255)
          )
          BEGIN
            INSERT INTO FINAL_DATA (COL1, COL2, AMOUNT, LOAD_TS, ETL_TS, SRC_BATCH)
            SELECT COL1, TRIM(COL2), AMOUNT, LOAD_TS, CURRENT_TIMESTAMP, p_batch_id
            FROM STG_DATA
            WHERE BATCH_ID = p_batch_id;

            INSERT INTO LOAD_AUDIT_LOG (
              AUDIT_TS, BATCH_ID, SRC_FILE, ROW_LOADED, ROW_EXPECTED,
              STATUS_CODE, STATUS_MESSAGE, OPERATOR
            )
            VALUES (
              CURRENT_TIMESTAMP, p_batch_id, p_src_file,
              NULL, NULL, 0, 'LOAD_AND_MERGE executed', USER
            );
          END;
          SQL_PROC

          cat > "${DATA_ROOT}/source_files/tpt/load_stg_data.tpt" <<'TPT_LOAD'
          DEFINE JOB LOAD_STG_DATA
          (
            DEFINE SCHEMA STG_SCHEMA
            (
              COL1   INTEGER,
              COL2   VARCHAR(100),
              AMOUNT BIGINT,
              LOAD_TS VARCHAR(30)
            );

            DEFINE OPERATOR FILE_READER
            TYPE DATACONNECTOR PRODUCER
            SCHEMA STG_SCHEMA
            ATTRIBUTES
            (
              FileName = '/home/runner/td_data/landing_zone/input_YYYYMMDD.csv',
              Format   = 'Delimited'
            );

            DEFINE OPERATOR TPT_INSERTER
            TYPE STREAM
            TARGET TABLE STG_DATA
            ATTRIBUTES
            (
              TdpId      = 'TERADATA_SID',
              UserName   = 'ETL_USER',
              UserPassword = 'ETL_PASS',
              LogTable   = 'ETL_LOG_TABLE'
            );

            APPLY
            (
              'INSERT INTO STG_DATA (COL1, COL2, AMOUNT, LOAD_TS, BATCH_ID)
               VALUES (:COL1, :COL2, :AMOUNT, TIMESTAMP :LOAD_TS, ''BATCH_PLACEHOLDER'');'
            )
            TO OPERATOR (TPT_INSERTER[1])
            SELECT COL1, COL2, AMOUNT, LOAD_TS
            FROM OPERATOR (FILE_READER[1]);
          );
          TPT_LOAD

          cat > "${DATA_ROOT}/source_files/sql/quality_queries.sql" <<'SQL_QUALITY'
          SELECT COUNT(*) AS CNT_STG FROM STG_DATA WHERE BATCH_ID = :BATCH_ID;

          SELECT COUNT(*) AS CNT_FINAL FROM FINAL_DATA WHERE SRC_BATCH = :BATCH_ID;

          SELECT
            SUM(CASE WHEN COL2 IS NULL OR TRIM(COL2) = '' THEN 1 ELSE 0 END) AS NULL_MEMO_ROWS,
            SUM(CASE WHEN AMOUNT > 900000 THEN 1 ELSE 0 END) AS HIGH_AMOUNT_ROWS,
            COUNT(*) AS TOTAL_ROWS,
            SUM(AMOUNT) AS SUM_AMOUNT,
            AVG(AMOUNT) AS AVG_AMOUNT
          FROM FINAL_DATA WHERE SRC_BATCH = :BATCH_ID;

          SELECT *
          FROM LOAD_AUDIT_LOG
          QUALIFY ROW_NUMBER() OVER (PARTITION BY BATCH_ID ORDER BY AUDIT_TS DESC) = 1
          ORDER BY AUDIT_TS DESC;
          SQL_QUALITY

          echo_note "DDL/TPT/ÏøºÎ¶¨ Ïä§ÌÅ¨Î¶ΩÌä∏ Ï†ÄÏû• ÏôÑÎ£å -> ${DATA_ROOT}/source_files"

      #######################################################################
      # 11. Ïä§ÌÇ§Îßà Î≥ÄÍ≤Ω diff
      #######################################################################
      - name: üßæ Ïä§ÌÇ§Îßà Î≥ÄÍ≤Ω diff Í∏∞Î°ù
        if: ${{ github.event.inputs.include_schema_diff != 'false' }}
        run: |
          set -e
          source /tmp/echo_helpers.sh
          PREV_SCHEMA="${DATA_ROOT}/history/last_create_tables.sql"
          CURR_SCHEMA="${DATA_ROOT}/source_files/sql/create_tables.sql"
          DIFF_LOG="${DATA_ROOT}/logs/schema_diff.log"
          if [ -f "$PREV_SCHEMA" ]; then
            diff -u "$PREV_SCHEMA" "$CURR_SCHEMA" > "$DIFF_LOG" || true
          else
            echo "[first run or no prev schema]" > "$DIFF_LOG"
          fi
          cp "$CURR_SCHEMA" "$PREV_SCHEMA" 2>/dev/null || cp "$CURR_SCHEMA" "$PREV_SCHEMA"
          echo_note "Ïä§ÌÇ§Îßà diff Í≤∞Í≥º -> $DIFF_LOG"
          head -n 200 "$DIFF_LOG" || true

      #######################################################################
      # 12. Îç∞Ïù¥ÌÑ∞ Ï†ÅÏû¨ & Î≥ëÌï© ÏãúÎÆ¨ (continue-on-error)
      #######################################################################
      - name: üîÑ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Î≥ëÌï© ÏãúÎÆ¨
        continue-on-error: true
        run: |
          set +e
          source /tmp/echo_helpers.sh
          set +e
          mkdir -p "${DATA_ROOT}/logs"
          BATCH_ID="BATCH_$(date +%Y%m%d_%H%M%S)"
          echo "$BATCH_ID" > "${DATA_ROOT}/logs/batch_id.txt"
          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"
          PIPELINE_LOG="${DATA_ROOT}/logs/pipeline_load_merge.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          GENERATED_COUNT=$(cat "${DATA_ROOT}/tmp/generated_rowcount.txt" 2>/dev/null || echo "${GEN_ROWS}")
          EXPECTED_IN="${{ github.event.inputs.expected_rows || '' }}"
          if [ -n "$EXPECTED_IN" ]; then
            EXPECTED_LOCAL="$EXPECTED_IN"
          else
            EXPECTED_LOCAL="${EXPECTED_ROWCOUNT}"
          fi
          {
            echo "=== LOAD+MERGE START ==="
            echo "BATCH_ID=$BATCH_ID"
            echo "SRC_FILE=$SRC_FILE"
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "TBL_STAGE=${TBL_STAGE}"
            echo "TBL_FINAL=${TBL_FINAL}"
            echo "TBL_AUDIT=${TBL_AUDIT}"
            echo
            echo "[1] (ÏãúÎÆ¨) TPT Bulk Load ${GENERATED_COUNT}Ìñâ -> ${TBL_STAGE}"
            echo "    FROM ${SRC_FILE}"
            echo "[2] (ÏãúÎÆ¨) CALL LOAD_AND_MERGE('${BATCH_ID}','${SRC_FILE}')"
            echo "[3] (ÏãúÎÆ¨) Í∞êÏÇ¨Î°úÍ∑∏(${TBL_AUDIT}) insert"
            echo "=== LOAD+MERGE END ==="
          } | tee "$PIPELINE_LOG"

          STATUS_CODE=0
          STATUS_MSG="OK(vdata-bulk-load)"
          SLA_ON="${{ github.event.inputs.sla_tracking || 'true' }}"
          if [ "$SLA_ON" != "false" ]; then
            echo "SLA: first attempt success" | tee -a "$PIPELINE_LOG"
            echo "SLA_RETRY_COUNT=0" > "${DATA_ROOT}/logs/sla_retry.log"
          else
            echo "SLA tracking disabled" > "${DATA_ROOT}/logs/sla_retry.log"
          fi

          {
            echo "STATUS_CODE=${STATUS_CODE}"
            echo "STATUS_MSG=${STATUS_MSG}"
            echo "BATCH_ID=${BATCH_ID}"
            echo "SRC_FILE=${SRC_FILE}"
            echo "ROW_EXPECTED=${EXPECTED_LOCAL}"
            echo "ROW_LOADED=${GENERATED_COUNT}"
          } > "$STATUS_FILE"

          echo_note "Îç∞Ïù¥ÌÑ∞ Ï†ÅÏû¨/Î≥ëÌï© ÏãúÎÆ¨ Ï¢ÖÎ£å (ÎåÄÎüâ Í∞ÄÏÉÅÎç∞Ïù¥ÌÑ∞). ÌååÏù¥ÌîÑÎùºÏù∏ Í≥ÑÏÜç ÏßÑÌñâ."

      #######################################################################
      # 13. DBA: Long-Running Query (LRQ) Í∞êÏßÄ ÏãúÎÆ¨
      #######################################################################
      - name: ‚è±Ô∏è Ïû•Í∏∞ Ïã§Ìñâ ÏøºÎ¶¨(LRQ) Í∞êÏßÄ Î°úÍ∑∏ ÏÉùÏÑ±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          LRQ_LOG="${DATA_ROOT}/logs/dba_lrq_check.log"
          echo_note "DBA: Ïû•Í∏∞ Ïã§Ìñâ ÏøºÎ¶¨(LRQ) Í∞êÏßÄ ÏãúÏûë"
          {
            echo "=== LONG-RUNNING QUERY (LRQ) AUDIT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "LRQ_THRESHOLD_SEC=300 (Simulated)"
            echo
            echo "--- Query Log Analysis (Example Rows > 300sec) ---"
            echo "SELECT QueryText, TotalIOCount, AMPCPUTime, StartTime, FirstRespTime"
            echo "FROM DBC.QryLogV"
            echo "WHERE StartTime >= CURRENT_DATE AND (AMPCPUTime/100.0) > 300"
            echo "ORDER BY AMPCPUTime DESC;"
            echo
            echo "[RESULT SIMULATION]"
            echo "Status: OK. No query exceeded 300 seconds."
            echo "Note: Watch MERGE step (CALL LOAD_AND_MERGE) in next batch."
          } > "$LRQ_LOG"
          echo_note "Ïû•Í∏∞ Ïã§Ìñâ ÏøºÎ¶¨(LRQ) Í∞êÏßÄ Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $LRQ_LOG"

      #######################################################################
      # 14. DBA: ÌååÌã∞ÏÖò / ÎùΩ / ÌîåÎûú / ÌäúÎãù
      #######################################################################
      - name: üß† DBA ÏÑ±Îä•/ÎùΩ/ÌååÌã∞ÏÖò Î∂ÑÏÑù Î°úÍ∑∏ ÏÉùÏÑ±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          BATCH_ID_VAL="$(cat "${DATA_ROOT}/logs/batch_id.txt" 2>/dev/null || echo 'UNKNOWN_BATCH')"

          PARTITION_LOG="${DATA_ROOT}/logs/partition_access.log"
          {
            echo "=== PARTITION ACCESS REPORT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "STG_DATA partition key: LOAD_TS (daily range)"
            echo "FINAL_DATA partition key: SRC_BATCH (batch_id range/hash)"
            echo "TODAY_ACCESSED_PARTITIONS: LOAD_TS=$(date +%Y-%m-%d) , SRC_BATCH=${BATCH_ID_VAL}"
            echo "NOTE=Only today's partition scanned (simulated)."
          } > "$PARTITION_LOG"

          LOCK_LOG="${DATA_ROOT}/logs/lock_contention.log"
          {
            echo "=== LOCK / CONTENTION REPORT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TABLE=${TBL_FINAL}"
            echo "LOCK_MODE=WriteLockDuringMerge (simulated)"
            echo "CONTENTIONS=0"
            echo "MAX_WAIT_SEC=0"
          } > "$LOCK_LOG"

          PLAN_LOG="${DATA_ROOT}/logs/query_plan_sample.log"
          {
            echo "=== SAMPLE EXPLAIN PLAN (SIMULATED) ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "QUERY=SELECT COUNT(*) FROM ${TBL_FINAL} WHERE SRC_BATCH='${BATCH_ID_VAL}';"
            echo "PLAN_STEP_1=Lock FINAL_DATA for access"
            echo "PLAN_STEP_2=Partitioned access on SRC_BATCH only"
            echo "PLAN_STEP_3=Aggregation (COUNT(*)) on partition"
            echo "PLAN_STEP_4=End transaction"
          } > "$PLAN_LOG"

          TUNE_LOG="${DATA_ROOT}/logs/tuning_recommendations.log"
          {
            echo "=== TUNING RECOMMENDATIONS ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TABLE=${TBL_FINAL}"
            echo "RECOMMENDATION_1=Add Secondary Index on (SRC_BATCH, LOAD_TS)"
            echo "WHY=Frequent WHERE SRC_BATCH=? AND LOAD_TS>=? in reporting queries"
            echo "ESTIMATED_BENEFIT=Faster dashboards, less full scan"
            echo "ACTION_OWNER=DBA_TEAM"
          } > "$TUNE_LOG"

          echo_note "DBA ÏÑ±Îä•/ÎùΩ/ÌååÌã∞ÏÖò/ÌäúÎãù Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å"

      #######################################################################
      # 15. ÌíàÏßà Í≤ÄÏ¶ù
      #######################################################################
      - name: ‚úÖ ÌíàÏßà Í≤ÄÏ¶ù Î∞è QC Î°úÍ∑∏ ÏÉùÏÑ±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          EXPECTED="$(grep '^ROW_EXPECTED=' "$STATUS_FILE" 2>/dev/null | cut -d= -f2 || true)"
          ACTUAL="$(grep '^ROW_LOADED=' "$STATUS_FILE" 2>/dev/null | cut -d= -f2 || true)"
          [ -z "$EXPECTED" ] && EXPECTED="${EXPECTED_ROWCOUNT}"
          [ -z "$ACTUAL" ] && ACTUAL="0"

          ROW_COUNT_OK="false"
          if [ "$EXPECTED" = "$ACTUAL" ]; then ROW_COUNT_OK="true"; fi

          HIGH_COUNT=$(grep 'FLAG-HIGH' "${DATA_ROOT}/landing_zone"/input_*.csv 2>/dev/null | wc -l || echo "0")
          if [ "$ACTUAL" -gt 0 ]; then
            HIGH_RATIO=$(echo "$HIGH_COUNT * 100 / $ACTUAL" | bc 2>/dev/null || echo "0")
          else
            HIGH_RATIO="0"
          fi

          LAST_HIST="${DATA_ROOT}/history/last_run_stats.txt"
          PREV_ROWS="N/A"; GROWTH="N/A"
          if [ -f "$LAST_HIST" ]; then
            PREV_ROWS=$(grep '^ACTUAL_ROWCOUNT=' "$LAST_HIST" | cut -d= -f2)
            if [ -n "$PREV_ROWS" ] && [ "$PREV_ROWS" != "N/A" ] && [ "$PREV_ROWS" -gt 0 ]; then
              GROWTH=$(echo "($ACTUAL-$PREV_ROWS)*100/$PREV_ROWS" | bc 2>/dev/null || echo "N/A")
            fi
          fi

          {
            echo "=== QUALITY CHECK ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "EXPECTED_ROWCOUNT=$EXPECTED"
            echo "ACTUAL_ROWCOUNT=$ACTUAL"
            echo "ROW_COUNT_OK=$ROW_COUNT_OK"
            echo
            echo "--- HIGH AMOUNT (FLAG-HIGH) ---"
            echo "HIGH_COUNT=$HIGH_COUNT"
            echo "HIGH_RATIO_PERCENT=$HIGH_RATIO"
            echo
            echo "--- GROWTH vs PREV RUN ---"
            echo "PREV_ACTUAL_ROWCOUNT=$PREV_ROWS"
            echo "ROW_GROWTH_PERCENT=$GROWTH"
            echo
            echo "--- SIMPLE SANITY RULES ---"
            echo "1) Í∏∞ÎåÄ Î°úÏö∞ Ïàò ÏùºÏπò? -> $ROW_COUNT_OK"
            echo "2) Ï¥àÍ≥†Ïï° Í±∞Îûò ÎπÑÏú® Í≥ºÎèÑ? -> check HIGH_RATIO_PERCENT"
            echo "3) Ï†ÑÏùº ÎåÄÎπÑ Í∏âÏ¶ù/Í∏âÍ∞ê? -> $GROWTH"
          } > "$QUALITY_LOG"

          {
            echo "RUN_ID=$GITHUB_RUN_ID"
            echo "ACTUAL_ROWCOUNT=$ACTUAL"
            echo "HIGH_RATIO_PERCENT=$HIGH_RATIO"
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$LAST_HIST"

          echo_note "ÌíàÏßà Í≤ÄÏ¶ù Î°úÍ∑∏ ÏûëÏÑ± ÏôÑÎ£å -> $QUALITY_LOG"
          head -n 50 "$QUALITY_LOG" || true

      #######################################################################
      # 16. DBA: Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Î∂àÍ∑†Ìòï(Skewness) Î∂ÑÏÑù
      #######################################################################
      - name: ‚öñÔ∏è Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Î∂àÍ∑†Ìòï(Skewness) Î∂ÑÏÑù
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SKEW_LOG="${DATA_ROOT}/logs/dba_skewness_check.log"
          echo_note "DBA: Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Î∂àÍ∑†Ìòï(Skewness) Î∂ÑÏÑù ÏãúÏûë"
          CURR_ROWS=$(cat "${DATA_ROOT}/tmp/generated_rowcount.txt" 2>/dev/null || echo "0")
          AMPS=6
          AVG_AMP_ROWS=$(( (CURR_ROWS + AMPS - 1) / AMPS ))
          MAX_AMP_ROWS=$(( AVG_AMP_ROWS + (AVG_AMP_ROWS * 20 / 100) ))
          if [ "$AVG_AMP_ROWS" -gt 0 ]; then
            SKEW_PERCENT=$(( (MAX_AMP_ROWS - AVG_AMP_ROWS) * 100 / AVG_AMP_ROWS ))
          else
            SKEW_PERCENT=0
          fi
          {
            echo "=== DATA SKEWNESS AUDIT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "PI_DEFINITION=COL1 (Primary Index)"
            echo "TOTAL_ROWS=$CURR_ROWS"
            echo "SIMULATED_AMPS=$AMPS"
            echo "AVG_ROWS_PER_AMP=$AVG_AMP_ROWS"
            echo "MAX_ROWS_PER_AMP=$MAX_AMP_ROWS (Simulated)"
            echo "SKEW_PERCENTAGE=${SKEW_PERCENT}%"
            echo
            echo "[ANALYSIS]"
            if [ "$SKEW_PERCENT" -gt 30 ]; then
              echo "Status: WARNING. Skewness exceeds 30%."
              echo "Action: Consider PI/Partition review or Columnar/Hash changes."
            else
              echo "Status: OK. Skewness within acceptable range."
            fi
          } > "$SKEW_LOG"
          echo_note "Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Î∂àÍ∑†Ìòï(Skewness) Î∂ÑÏÑù Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $SKEW_LOG"

      #######################################################################
      # 17. DBA: Ïã§Ìñâ Í≥ÑÌöç Î≥ÄÍ≤Ω Diff Í∏∞Î°ù
      #######################################################################
      - name: üßæ Ïã§Ìñâ Í≥ÑÌöç Î≥ÄÍ≤Ω Diff Í∏∞Î°ù
        run: |
          set -e
          source /tmp/echo_helpers.sh
          PREV_PLAN="${DATA_ROOT}/history/last_query_plan.txt"
          CURR_PLAN="${DATA_ROOT}/logs/current_query_plan.txt"
          PLAN_DIFF_LOG="${DATA_ROOT}/logs/plan_diff.log"
          echo_note "DBA: Ïã§Ìñâ Í≥ÑÌöç Î≥ÄÍ≤Ω Diff ÏãúÏûë"
          QUERY="SELECT COUNT(*) FROM ${TARGET_TABLE} WHERE SRC_BATCH = 'BATCH_PLACEHOLDER';"
          {
            echo "=== CURRENT QUERY PLAN ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "QUERY: $QUERY"
            echo "STEP_1: Lock ${TARGET_TABLE} for access"
            echo "STEP_2: Partitioned access on SRC_BATCH only"
            echo "STEP_3: Aggregation (COUNT(*)) on partition"
            echo "STEP_4: End transaction"
          } > "$CURR_PLAN"
          if [ -f "$PREV_PLAN" ]; then
            echo_note "Ïù¥Ï†Ñ ÌîåÎûúÍ≥º ÌòÑÏû¨ ÌîåÎûú ÎπÑÍµê Ï§ë..."
            diff -u "$PREV_PLAN" "$CURR_PLAN" > "$PLAN_DIFF_LOG" || true
          else
            echo "[first run or no previous plan]" > "$PLAN_DIFF_LOG"
          fi
          cp "$CURR_PLAN" "$PREV_PLAN" 2>/dev/null || cp "$CURR_PLAN" "$PREV_PLAN"
          echo_note "Ïã§Ìñâ Í≥ÑÌöç Diff Í≤∞Í≥º -> $PLAN_DIFF_LOG"
          head -n 200 "$PLAN_DIFF_LOG" || true

      #######################################################################
      # 18. DBA Ïú†ÏßÄÎ≥¥Ïàò / DR Î≥µÍµ¨ ÌîåÎûú / Ïö©Îüâ / RPO-RTO
      #######################################################################
      - name: üßÆ DBA Ïú†ÏßÄÎ≥¥Ïàò/DR/Ïö©Îüâ/RPO-RTO Î°úÍ∑∏ ÏÉùÏÑ±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          BATCH_ID_VAL="$(cat "${DATA_ROOT}/logs/batch_id.txt" 2>/dev/null || echo 'UNKNOWN_BATCH')"

          STATS_LOG="${DATA_ROOT}/logs/stats_maintenance.log"
          {
            echo "=== STATS MAINTENANCE REPORT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TABLE ${TBL_FINAL}: COLLECT STATS on (COL1, SRC_BATCH, LOAD_TS) (simulated)"
            echo "IMPACT=Optimizer cardinality accuracy ‚Üë"
            echo "NEXT_REVIEW=+1 day or rowcount_delta > 20%"
          } > "$STATS_LOG"

          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          LAST_CAP="${DATA_ROOT}/history/last_capacity.txt"
          CURR_STG_MB=$(( (RANDOM % 4000) + 1000 ))
          CURR_FINAL_MB=$(( (RANDOM % 9000) + 2000 ))
          PREV_STG_MB="N/A"
          PREV_FINAL_MB="N/A"
          if [ -f "$LAST_CAP" ]; then
            PREV_STG_MB=$(grep '^STG_MB=' "$LAST_CAP" | cut -d= -f2)
            PREV_FINAL_MB=$(grep '^FINAL_MB=' "$LAST_CAP" | cut -d= -f2)
          fi
          {
            echo "=== CAPACITY / GROWTH ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "STG_MB_CURR=$CURR_STG_MB"
            echo "FINAL_MB_CURR=$CURR_FINAL_MB"
            echo "STG_MB_PREV=$PREV_STG_MB"
            echo "FINAL_MB_PREV=$PREV_FINAL_MB"
            echo "NOTE=Track daily growth for capacity planning & cost mgmt."
          } > "$CAP_LOG"
          {
            echo "STG_MB=$CURR_STG_MB"
            echo "FINAL_MB=$CURR_FINAL_MB"
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$LAST_CAP"

          DRPLAY="${DATA_ROOT}/governance/recovery_playbook.txt"
          {
            echo "=== RECOVERY PLAYBOOK ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "BATCH_ID=${BATCH_ID_VAL}"
            echo "1) Pause downstream reporting jobs."
            echo "2) Restore FINAL_DATA partition for BATCH_ID=${BATCH_ID_VAL} from latest DR snapshot (.tar.gz or .iso)."
            echo "3) Validate rowcount vs EXPECTED_ROWCOUNT."
            echo "4) Re-run LOAD_AND_MERGE for failed batch only."
            echo "5) Notify BatchSRE and Compliance if discrepancy > 0.5%."
            echo
            echo "RPO_TARGET=15min"
            echo "RTO_TARGET=30min"
            echo "RPO_ESTIMATE=15min_ok (simulated)"
            echo "RTO_ESTIMATE=25min_ok (simulated)"
            echo
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            echo "POLICY=Snapshots older than ${DR_RETENTION_DAYS} day(s) are purged from ${DR_DIR_NAME}"
          } > "$DRPLAY"

          CLASS_LOG="${DATA_ROOT}/governance/table_classification.log"
          {
            echo "=== TABLE CLASSIFICATION MAP ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "STG_DATA: SENSITIVITY=HIGH_RAW (pre-mask), OWNER=ETL_TEAM, RETENTION=30d"
            echo "FINAL_DATA: SENSITIVITY=MASKED_CONFIDENTIAL, OWNER=RISK_ANALYTICS, RETENTION=indef(masked)"
            echo "LOAD_AUDIT_LOG: SENSITIVITY=OPERATIONS_AUDIT, OWNER=COMPLIANCE_TEAM, RETENTION=365d"
            echo "DR_BACKUP (FINAL_DATA subset): SENSITIVITY=MASKED_CONFIDENTIAL, OWNER=DBA_TEAM"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
          } > "$CLASS_LOG"

          echo_note "DBA Ïú†ÏßÄÎ≥¥Ïàò/DR/RPO-RTO/Ïö©Îüâ/Î∂ÑÎ•ò Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å"

      #######################################################################
      # 18.5 FinOps: Ïö©Îüâ Ï∂îÏÑ∏ CSV ÏóÖÎç∞Ïù¥Ìä∏
      #######################################################################
      - name: üíæ Ïö©Îüâ Ï∂îÏÑ∏ CSV ÏóÖÎç∞Ïù¥Ìä∏
        run: |
          set -e
          source /tmp/echo_helpers.sh
          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          CSV_FILE="${DATA_ROOT}/logs/capacity_growth.csv"

          TS=$(date +%Y-%m-%dT%H:%M:%S%z)
          STG_MB_CURR=$(grep 'STG_MB_CURR=' "$CAP_LOG" | cut -d= -f2)
          FINAL_MB_CURR=$(grep 'FINAL_MB_CURR=' "$CAP_LOG" | cut -d= -f2)

          if [ ! -f "$CSV_FILE" ]; then
            echo "TS,STG_MB,FINAL_MB" > "$CSV_FILE"
          fi

          echo "${TS},${STG_MB_CURR},${FINAL_MB_CURR}" >> "$CSV_FILE"
          echo_note "Ïö©Îüâ Ï∂îÏÑ∏ CSV Í∞±Ïã† -> $CSV_FILE"

      #######################################################################
      # 19. ÎØºÍ∞ê Îç∞Ïù¥ÌÑ∞ ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Í∞êÏÇ¨
      #######################################################################
      - name: üõ° ÎØºÍ∞ê Îç∞Ïù¥ÌÑ∞ ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Í∞êÏÇ¨ Î°úÍ∑∏
        if: ${{ github.event.inputs.include_masking_audit != 'false' }}
        run: |
          set -e
          source /tmp/echo_helpers.sh
          MASK_LOG="${DATA_ROOT}/logs/masking_audit.log"
          {
            echo "[MASKING POLICY SIMULATION]"
            echo "- ÎØºÍ∞ê ÌïÑÎìú Ïòà: CARD_NO, SSN, ACCOUNT_ID Îì±"
            echo "- FINAL_DATA ÏóêÏÑúÎäî Ìï¥Îãπ ÎØºÍ∞ê ÌïÑÎìúÎäî SHA256 ÎòêÎäî TOKEN_ID Î°úÎßå Ï†ÄÏû•"
            echo "- STG_DATA ÏõêÎ≥∏ÌòïÏãùÏùÄ 24ÏãúÍ∞Ñ ÎÇ¥ ÌååÌã∞ÏÖò ÏïÑÏπ¥Ïù¥Î∏å ÌõÑ Ï†ëÍ∑ºÏ∞®Îã®"
            echo "- ACCESS CONTROL: ANALYST_ROLE ÏùÄ ÎßàÏä§ÌÇπÎêú Ïª¨ÎüºÎßå SELECT Í∞ÄÎä•"
            echo "- ANALYST_ROLE ÏùÄ ÏßÄÏ†êÎ≥Ñ row-level filterÎ°ú Ï†úÌïú"
            echo "- Î≥ÄÍ≤Ω ÏäπÏù∏Ïûê(DATA_OWNER_X) ÏäπÏù∏Ïùº=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$MASK_LOG"
          echo_note "ÎØºÍ∞ê Îç∞Ïù¥ÌÑ∞ ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $MASK_LOG"
          head -n 40 "$MASK_LOG" || true

      #######################################################################
      # 20. Í∂åÌïú/Ï†ëÍ∑ºÏ†úÏñ¥ Í∞êÏÇ¨
      #######################################################################
      - name: üîê Í∂åÌïú/Ï†ëÍ∑ºÏ†úÏñ¥ Í∞êÏÇ¨ Í∏∞Î°ù ÏÉùÏÑ±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          ACL_LOG="${DATA_ROOT}/logs/acl_audit.sql"
          {
            echo "-- Í∂åÌïú Í∞êÏÇ¨ Î°úÍ∑∏ (ÏãúÎÆ¨Î†àÏù¥ÏÖò)"
            echo "-- FINAL_DATA Ï°∞Ìöå/Ïì∞Í∏∞ Í∂åÌïú ÏÑ§Ï†ï ÏòàÏãú"
            echo "GRANT SELECT ON ${TBL_FINAL} TO ROLE ANALYST_ROLE;"
            echo "GRANT INSERT,UPDATE ON ${TBL_FINAL} TO ROLE ETL_LOADER_ROLE;"
            echo "REVOKE INSERT ON ${TBL_FINAL} FROM ROLE ANALYST_ROLE;"
            echo
            echo "-- AUDIT LOG ÌÖåÏù¥Î∏î Ï†ëÍ∑º ÌÜµÏ†ú"
            echo "GRANT SELECT ON ${TBL_AUDIT} TO ROLE AUDIT_READER_ROLE;"
            echo
            echo "-- Column-level masking / Row-level filtering (Î¨∏ÏÑúÌôîÏö©)"
            echo "-- ANALYST_ROLE ÏùÄ FINAL_DATA.COL2(Î©îÎ™®)Îäî ÎßàÏä§ÌÇπ Î≤ÑÏ†ÑÎßå Ï°∞Ìöå Í∞ÄÎä•"
            echo "-- ANALYST_ROLE ÏùÄ ÏûêÏã†Ïùò ÏßÄÏ†ê Îç∞Ïù¥ÌÑ∞Îßå Ï†ëÍ∑º (row filter)"
            echo
            echo "-- Ïã§Ìñâ Î©îÌÉÄ"
            echo "-- AUDIT_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "-- EXEC_BY=$GITHUB_ACTOR"
            echo "-- COMMIT_SHA=$GITHUB_SHA"
            echo "-- BRANCH=$GITHUB_REF_NAME"
          } > "$ACL_LOG"
          echo_note "Í∂åÌïú Í∞êÏÇ¨ Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $ACL_LOG"
          head -n 60 "$ACL_LOG" || true

      #######################################################################
      # 21. tar.gz Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± + snapshot_set Í∏∞Î°ù
      #######################################################################
      - name: üóú Í≤∞Í≥ºÎ¨º tar.gz Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SNAP_TAG="td-snapshot-$(date +%Y%m%d-%H%M%S)-${GITHUB_SHA:0:8}"
          SNAP_DIR="${DATA_ROOT}/release"
          SNAP_FILE="${SNAP_DIR}/${SNAP_TAG}.tar.gz"
          mkdir -p "$SNAP_DIR"
          tar -czf "$SNAP_FILE" \
            -C "${DATA_ROOT}" \
            logs \
            quality \
            extract_out \
            source_files \
            landing_zone \
            tmp \
            health \
            governance \
            history \
            || true
          snapshot_set SNAP_TAG "$SNAP_TAG"
          snapshot_set SNAP_FILE "$SNAP_FILE"
          sha256sum "$SNAP_FILE" > "${DATA_ROOT}/logs/snapshot_hash.txt" 2>/dev/null || echo "hash_failed" > "${DATA_ROOT}/logs/snapshot_hash.txt"
          echo_note "tar.gz Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± -> $SNAP_FILE"
          ls -lh "$SNAP_FILE" || true
          head -n 5 "${DATA_ROOT}/logs/snapshot_hash.txt" || true

      #######################################################################
      # 22. ISO Ïä§ÎÉÖÏÉ∑ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± (mode=full & debug_mode=false Ïùº ÎïåÎßå)
      #######################################################################
      - name: üßä ISO Ïä§ÎÉÖÏÉ∑ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±
        if: ${{ github.event.inputs.mode == 'full' && github.event.inputs.debug_mode != 'true' }}
        continue-on-error: true
        run: |
          set +e
          source /tmp/echo_helpers.sh
          set +e
          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"
          ISO_BUILD_LOG="${DATA_ROOT}/logs/iso_build.log"
          echo "ISO_BUILD_TS=$(date +%Y-%m-%dT%H:%M:%S%z)" > "$ISO_BUILD_LOG"
          echo "SNAP_TAG=$SNAP_TAG" >> "$ISO_BUILD_LOG"
          echo "SNAP_FILE=$SNAP_FILE" >> "$ISO_BUILD_LOG"
          echo "ISO_FILE=$ISO_FILE" >> "$ISO_BUILD_LOG"
          if [ -z "$SNAP_TAG" ] || [ -z "$SNAP_FILE" ]; then
            echo_note "SNAP_INFO ÏóÜÏùå ‚Üí ISO ÏÉùÏÑ± Ïä§ÌÇµ"
            exit 0
          fi
          ISO_SRC_DIR="${DATA_ROOT}/tmp/iso_src_${SNAP_TAG}"
          mkdir -p "$ISO_SRC_DIR"
          cat > "${ISO_SRC_DIR}/audit_summary_seed.txt" <<EOF_ISO
          Snapshot Tag: $SNAP_TAG
          Generated: $(date +%Y-%m-%dT%H:%M:%S%z)
          Contains: logs, quality, governance, health, DR topology, classification, recovery playbook, etc.
          RPO_TARGET=15min
          RTO_TARGET=30min
          DR_RETENTION_DAYS=${DR_RETENTION_DAYS}
          EOF_ISO
          cp -r "${DATA_ROOT}/logs" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/quality" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/governance" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/health" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/source_files" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/landing_zone" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/history" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/${DR_TOPOLOGY_DIR}" "${ISO_SRC_DIR}/dr_topology" 2>>"$ISO_BUILD_LOG" || true

          echo "[TRY] installing genisoimage/xorriso" >> "$ISO_BUILD_LOG"
          sudo apt-get update -y >> "$ISO_BUILD_LOG" 2>&1 || true
          sudo apt-get install -y genisoimage xorriso >> "$ISO_BUILD_LOG" 2>&1 || true

          if command -v genisoimage >/dev/null 2>&1 ; then
            echo "[USE] genisoimage" >> "$ISO_BUILD_LOG"
            genisoimage -quiet -J -r -o "$ISO_FILE" "$ISO_SRC_DIR" >> "$ISO_BUILD_LOG" 2>&1 || { echo "[WARN] genisoimage failed" >> "$ISO_BUILD_LOG"; }
          fi

          if [ ! -s "$ISO_FILE" ] && command -v xorriso >/dev/null 2>&1 ; then
            echo "[USE] xorriso -as mkisofs" >> "$ISO_BUILD_LOG"
            xorriso -as mkisofs -J -r -o "$ISO_FILE" "$ISO_SRC_DIR" >> "$ISO_BUILD_LOG" 2>&1 || { echo "[WARN] xorriso mkisofs failed" >> "$ISO_BUILD_LOG"; }
          fi

          if [ -s "$ISO_FILE" ]; then
            sha256sum "$ISO_FILE" > "${DATA_ROOT}/logs/iso_hash.txt" 2>/dev/null || echo "hash_failed" > "${DATA_ROOT}/logs/iso_hash.txt"
            echo_note "ISO Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± ÏôÑÎ£å -> $ISO_FILE"
            ls -lh "$ISO_FILE" || true
          else
            echo_note "ISO Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± Ïã§Ìå®(ÌôòÍ≤Ω Ï†úÏïΩ). ÌååÏù¥ÌîÑÎùºÏù∏ Í≥ÑÏÜç."
            echo "ISO_CREATION_FAILED=1" >> "$ISO_BUILD_LOG"
          fi

      #######################################################################
      # 23. DR(Ïû¨Ìï¥Î≥µÍµ¨) Î∞±ÏóÖ Î∞è 1Ïùº Î≥¥Í¥Ä Î°úÌÖåÏù¥ÏÖò (mode=full & debug_mode=false & dr_backup=true)
      #######################################################################
      - name: üåê DR(Ïû¨Ìï¥Î≥µÍµ¨) Î∞±ÏóÖ Î∞è 1Ïùº Î≥¥Í¥Ä Î°úÌÖåÏù¥ÏÖò
        if: ${{ github.event.inputs.mode == 'full' && github.event.inputs.debug_mode != 'true' }}
        continue-on-error: true
        env:
          DUMMY_KEY: ${{ secrets.DR_BACKUP_KEY }}
        run: |
          set +e
          source /tmp/echo_helpers.sh
          set +e
          if [ "${{ github.event.inputs.dr_backup || 'false' }}" != "true" ]; then
            echo_note "dispatch.dr_backup=false ‚Üí Ïô∏Î∂Ä DR Î∞±ÏóÖ Îã®Í≥Ñ Ïä§ÌÇµ"
            exit 0
          fi

          if [ -n "$DUMMY_KEY" ]; then
            echo "::add-mask::$DUMMY_KEY"
          fi

          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"
          ISO_HASH_FILE="${DATA_ROOT}/logs/iso_hash.txt"

          if [ -z "$SNAP_TAG" ] || [ -z "$SNAP_FILE" ]; then
            echo_note "SNAP_INFO ÏóÜÏùå ‚Üí DR Î∞±ÏóÖ/Î°úÌÖåÏù¥ÏÖò Ïä§ÌÇµ"
            exit 0
          fi

          mkdir -p "${DATA_ROOT}/${DR_DIR_NAME}"
          DR_DATE_DIR="${DATA_ROOT}/${DR_DIR_NAME}/$(date +%Y-%m-%d)"
          mkdir -p "$DR_DATE_DIR"
          DR_COPY_TAR="${DR_DATE_DIR}/${SNAP_TAG}.tar.gz"
          DR_COPY_ISO="${DR_DATE_DIR}/${SNAP_TAG}.iso"
          DR_META_PATH="${DR_DATE_DIR}/${SNAP_TAG}.meta.txt"

          cp -f "$SNAP_FILE" "$DR_COPY_TAR" 2>/dev/null || true
          if [ -s "$ISO_FILE" ]; then cp -f "$ISO_FILE" "$DR_COPY_ISO" 2>/dev/null || true; fi

          {
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE_LOCAL=$SNAP_FILE"
            echo "DR_STORED_TAR=$DR_COPY_TAR"
            if [ -s "$ISO_FILE" ]; then echo "DR_STORED_ISO=$DR_COPY_ISO"; else echo "DR_STORED_ISO=(none)"; fi
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            if [ -n "$DUMMY_KEY" ]; then
              echo "DR_KEY_PRESENT=yes"
              echo "EXTERNAL_UPLOAD_STATUS=SUCCESS(simulated)"
            else
              echo "DR_KEY_PRESENT=no"
              echo "EXTERNAL_UPLOAD_STATUS=FAILED(no credentials)"
            fi
            echo "NOTE=Î°úÏª¨ DR ÏòÅÏó≠Ïóê tar.gz/iso Ï†ÄÏû• ÌõÑ, Î≥¥Í¥ÄÏ£ºÍ∏∞ ÏßÄÎÇú Ïä§ÎÉÖÏÉ∑ÏùÄ ÏÇ≠Ï†ú"
            if [ -f "$ISO_HASH_FILE" ]; then
              echo "--- ISO HASH ---"
              cat "$ISO_HASH_FILE"
            fi
          } > "$DR_META_PATH"

          ROTATE_LOG="${DATA_ROOT}/logs/dr_rotation.log"
          {
            echo "=== DR ROTATION START ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "POLICY=keep ${DR_RETENTION_DAYS} day(s)"
            echo "ACTION=find ${DATA_ROOT}/${DR_DIR_NAME} -type f -mtime +${DR_RETENTION_DAYS} -delete"
            find "${DATA_ROOT}/${DR_DIR_NAME}" -type f -mtime +${DR_RETENTION_DAYS} -print
            find "${DATA_ROOT}/${DR_DIR_NAME}" -type f -mtime +${DR_RETENTION_DAYS} -delete || true
            echo "=== DR ROTATION END ==="
          } > "$ROTATE_LOG"

          DR_LOG="${DATA_ROOT}/logs/dr_backup_attempt.log"
          {
            echo "[DR BACKUP SIMULATION]"
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE=$SNAP_FILE"
            echo "DR_LOCAL_TAR=$DR_COPY_TAR"
            echo "DR_LOCAL_ISO=$DR_COPY_ISO"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            echo "RPO_TARGET=15min"
            echo "RTO_TARGET=30min"
            if [ -n "$DUMMY_KEY" ]; then
              echo "DR_KEY_PRESENT=yes (simulated external upload ok)"
            else
              echo "DR_KEY_PRESENT=no (external upload skipped/fail)"
            fi
          } > "$DR_LOG"

          echo_note "DR Î∞±ÏóÖ/Î°úÌÖåÏù¥ÏÖò Ï≤òÎ¶¨ ÏôÑÎ£å"
          head -n 80 "$DR_LOG" || true
          head -n 80 "$ROTATE_LOG" || true

      #######################################################################
      # 24. GitHub Release ÏóÖÎ°úÎìú ÏãúÎèÑ (mode=full & debug_mode=false)
      #######################################################################
      - name: üöÄ GitHub Release(Ïä§ÎÉÖÏÉ∑) ÏóÖÎ°úÎìú ÏãúÎèÑ
        if: ${{ github.event.inputs.mode == 'full' && github.event.inputs.debug_mode != 'true' }}
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set +e
          source /tmp/echo_helpers.sh
          set +e
          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"

          if [ -z "$SNAP_TAG" ] || [ -z "$SNAP_FILE" ]; then
            echo_note "SNAP_INFO ÏóÜÏùå ‚Üí GitHub Release ÏóÖÎ°úÎìú Ïä§ÌÇµ"
            exit 0
          fi

          echo_note "Î¶¥Î¶¨Ï¶à ÌÉúÍ∑∏ ${SNAP_TAG} ÏóÖÎ°úÎìú ÏãúÎèÑ"
          if [ -s "$ISO_FILE" ]; then
            gh release create "$SNAP_TAG" "$SNAP_FILE" "$ISO_FILE" \
              --title "$SNAP_TAG" \
              --notes "ÏûêÎèô Teradata Î∞∞Ïπò Ïä§ÎÉÖÏÉ∑ (tar.gz + iso). Ìè¨Ìï®ÎÇ¥Ïö©: ÎåÄÎüâÎç∞Ïù¥ÌÑ∞, DBAÎ°úÍ∑∏, DR ÌÜ†Ìè¥Î°úÏßÄ, RPO/RTO, ÎßàÏä§ÌÇπ/Í∂åÌïú Í∞êÏÇ¨, ÏãúÏä§ÌÖú ÏóÖÍ∑∏Î†àÏù¥Îìú Î°úÍ∑∏ Îì±." \
              || echo "::warning::gh release create Ïã§Ìå® (Í∂åÌïú Î∂ÄÏ°± ÎòêÎäî ÌÉúÍ∑∏ Ï§ëÎ≥µ Í∞ÄÎä•)"
          else
            gh release create "$SNAP_TAG" "$SNAP_FILE" \
              --title "$SNAP_TAG" \
              --notes "ÏûêÎèô Teradata Î∞∞Ïπò Ïä§ÎÉÖÏÉ∑ (tar.gz). Ìè¨Ìï®ÎÇ¥Ïö©: ÎåÄÎüâÎç∞Ïù¥ÌÑ∞, DBAÎ°úÍ∑∏, DR ÌÜ†Ìè¥Î°úÏßÄ, RPO/RTO, ÎßàÏä§ÌÇπ/Í∂åÌïú Í∞êÏÇ¨, ÏãúÏä§ÌÖú ÏóÖÍ∑∏Î†àÏù¥Îìú Î°úÍ∑∏ Îì±." \
              || echo "::warning::gh release create Ïã§Ìå® (Í∂åÌïú Î∂ÄÏ°± ÎòêÎäî ÌÉúÍ∑∏ Ï§ëÎ≥µ Í∞ÄÎä•)"
          fi

      #######################################################################
      # 25. Í±∞Î≤ÑÎÑåÏä§/ÏäπÏù∏/Ïò®ÏΩú Í∏∞Î°ù
      #######################################################################
      - name: üßë‚Äçüíº Í±∞Î≤ÑÎÑåÏä§ ÏäπÏù∏ & Îã¥ÎãπÏûê Í∏∞Î°ù
        run: |
          set -e
          source /tmp/echo_helpers.sh
          GOV_LOG="${DATA_ROOT}/governance/governance_approval.log"
          {
            echo "=== GOVERNANCE / APPROVAL LOG ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "DATA_OWNER=DATA_OWNER_X"
            echo "APPROVER=COMPLIANCE_TEAM"
            echo "ONCALL_TEAM=ETL_OnCall"
            echo "ONCALL_CONTACT=etloncall@example.local"
            echo "LAST_SCHEMA_CHANGE=Ïª¨Îüº AMOUNT Ï∂îÍ∞Ä / ÎØºÍ∞êÌïÑÎìú ÎßàÏä§ÌÇπ Ï†ïÏ±Ö Î∞òÏòÅ"
            echo "SLA_CONTACT=BatchSRE"
            echo "BUSINESS_SLA=KST 06:00 Î¶¨Ìè¨Ìä∏ ÎßàÍ∞êÍπåÏßÄ ÏôÑÎ£å"
            echo "BUSINESS_IMPACT=ÏßÄÏó∞ Ïãú Î¶¨Ïä§ÌÅ¨ Ïä§ÏΩîÏñ¥ Î≥¥Í≥† ÏßÄÏó∞"
            echo "DR_POLICY=DR snapshot retention ${DR_RETENTION_DAYS} day(s) under ${DR_DIR_NAME}/"
            echo "DR_TOPOLOGY_DOC=${DR_TOPOLOGY_DIR}/dr_topology.txt"
          } > "$GOV_LOG"
          echo_note "Í±∞Î≤ÑÎÑåÏä§/ÏäπÏù∏/Ïò®ÏΩú Ï†ïÎ≥¥ Í∏∞Î°ù ÏôÑÎ£å -> $GOV_LOG"
          head -n 50 "$GOV_LOG" || true

      #######################################################################
      # 26. HTML ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± (DBA Í≥†Í∏â Ìï≠Î™© Ìè¨Ìï®)
      #######################################################################
      - name: üñ® HTML ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"
          : "${SNAP_TAG:=N/A}"
          : "${SNAP_FILE:=N/A}"

          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          ACL_LOG="${DATA_ROOT}/logs/acl_audit.sql"
          HASH_LOG="${DATA_ROOT}/logs/snapshot_hash.txt"
          ISO_HASH_FILE="${DATA_ROOT}/logs/iso_hash.txt"
          PARTITION_LOG="${DATA_ROOT}/logs/partition_access.log"
          LOCK_LOG="${DATA_ROOT}/logs/lock_contention.log"
          PLAN_LOG="${DATA_ROOT}/logs/query_plan_sample.log"
          TUNE_LOG="${DATA_ROOT}/logs/tuning_recommendations.log"
          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          STATS_LOG="${DATA_ROOT}/logs/stats_maintenance.log"
          CLASS_LOG="${DATA_ROOT}/governance/table_classification.log"
          DRPLAY="${DATA_ROOT}/governance/recovery_playbook.txt"
          DR_TOPO_FILE="${DATA_ROOT}/${DR_TOPOLOGY_DIR}/dr_topology.txt"
          ROTATE_LOG="${DATA_ROOT}/logs/dr_rotation.log"
          LRQ_LOG="${DATA_ROOT}/logs/dba_lrq_check.log"
          SKEW_LOG="${DATA_ROOT}/logs/dba_skewness_check.log"
          PLAN_DIFF_LOG="${DATA_ROOT}/logs/plan_diff.log"
          REPORT_HTML="${DATA_ROOT}/release/report_${GITHUB_RUN_ID}.html"

          {
            echo "<html><body style='font-family:monospace;'>"
            echo "<h1>Teradata FinOps Batch Summary</h1>"
            echo "<p><b>Timestamp:</b> $(date +%Y-%m-%dT%H:%M:%S%z)</p>"
            echo "<p><b>Repo:</b> $GITHUB_REPOSITORY</p>"
            echo "<p><b>Actor:</b> $GITHUB_ACTOR</p>"
            echo "<p><b>Branch:</b> $GITHUB_REF_NAME</p>"
            echo "<p><b>Commit:</b> $GITHUB_SHA</p>"
            echo "<p><b>Batch Target Table:</b> ${TARGET_TABLE}</p>"
            echo "<p><b>Tables Used:</b> ${TBL_STAGE}, ${TBL_FINAL}, ${TBL_AUDIT}</p>"
            echo "<p><b>Snapshot Tag:</b> $SNAP_TAG</p>"
            echo "<p><b>Snapshot File (tar.gz):</b> $SNAP_FILE</p>"
            echo "<p><b>Snapshot ISO:</b> $ISO_FILE</p>"
            echo "<p><b>DR Retention Policy:</b> ${DR_RETENTION_DAYS} day(s) in ${DR_DIR_NAME}/ (auto-rotation)</p>"

            echo "<hr /><h2>Pipeline Status</h2><pre>"
            cat "$STATUS_FILE" 2>/dev/null || echo "(no pipeline_status.log)"
            echo "</pre><h2>Quality Check</h2><pre>"
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo "</pre><h2>System Upgrade (head)</h2><pre>"
            head -n 40 "$UPG_LOG" 2>/dev/null || echo "(no system_upgrade.log)"

            echo "</pre><h2>ACL / Access Control</h2><pre>"
            head -n 80 "$ACL_LOG" 2>/dev/null || echo "(no acl_audit.sql)"

            echo "</pre><h2>DBA Performance & Capacity</h2><pre>"
            echo "--- Long-Running Query Check (LRQ) ---"
            head -n 80 "$LRQ_LOG" 2>/dev/null || echo "(no dba_lrq_check.log)"
            echo
            echo "--- Skewness / Distribution Check ---"
            head -n 80 "$SKEW_LOG" 2>/dev/null || echo "(no dba_skewness_check.log)"
            echo
            echo "--- Query Plan Diff ---"
            head -n 80 "$PLAN_DIFF_LOG" 2>/dev/null || echo "(no plan_diff.log)"
            echo
            echo "--- Partition Access ---"
            head -n 80 "$PARTITION_LOG" 2>/dev/null || echo "(no partition_access.log)"
            echo
            echo "--- Lock Contention ---"
            head -n 80 "$LOCK_LOG" 2>/dev/null || echo "(no lock_contention.log)"
            echo
            echo "--- Query Plan Sample ---"
            head -n 80 "$PLAN_LOG" 2>/dev/null || echo "(no query_plan_sample.log)"
            echo
            echo "--- Tuning Recommendations ---"
            head -n 80 "$TUNE_LOG" 2>/dev/null || echo "(no tuning_recommendations.log)"
            echo
            echo "--- Stats Maintenance ---"
            head -n 80 "$STATS_LOG" 2>/dev/null || echo "(no stats_maintenance.log)"
            echo
            echo "--- Capacity Growth ---"
            head -n 80 "$CAP_LOG" 2>/dev/null || echo "(no capacity_growth.log)"

            echo "</pre><h2>Classification / DR / Snapshot Integrity</h2><pre>"
            echo "--- Table Classification ---"
            head -n 80 "$CLASS_LOG" 2>/dev/null || echo "(no table_classification.log)"
            echo
            echo "--- Recovery Playbook (RPO/RTO) ---"
            head -n 80 "$DRPLAY" 2>/dev/null || echo "(no recovery_playbook.txt)"
            echo
            echo "--- DR Topology ---"
            head -n 80 "$DR_TOPO_FILE" 2>/dev/null || echo "(no dr_topology.txt)"
            echo
            echo "--- DR Rotation ---"
            head -n 80 "$ROTATE_LOG" 2>/dev/null || echo "(no dr_rotation.log)"
            echo
            echo "--- Snapshot Integrity Hash (tar.gz) ---"
            cat "$HASH_LOG" 2>/dev/null || echo "(no snapshot_hash.txt)"
            echo
            echo "--- Snapshot ISO Hash ---"
            cat "$ISO_HASH_FILE" 2>/dev/null || echo "(no iso_hash.txt)"
            echo "</pre>"

            echo "<p>-- End of Report --</p>"
            echo "</body></html>"
          } > "$REPORT_HTML"

          echo_note "HTML ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± -> $REPORT_HTML"
          head -n 40 "$REPORT_HTML" || true

      #######################################################################
      # 27. SLA ÌÉÄÏù¥Î®∏ Ï¢ÖÎ£å (ÏòµÏÖò)
      #######################################################################
      - name: ‚è± SLA ÌÉÄÏù¥Î®∏ Ï¢ÖÎ£å Î∞è Ïã§ÌñâÏãúÍ∞Ñ Í∏∞Î°ù
        if: ${{ github.event.inputs.sla_tracking != 'false' }}
        run: |
          set -e
          source /tmp/echo_helpers.sh
          END_EPOCH=$(date +%s)
          START_EPOCH=$(cat "${DATA_ROOT}/logs/start_epoch.txt" 2>/dev/null || echo "$END_EPOCH")
          DURATION_SEC=$((END_EPOCH-START_EPOCH))
          {
            echo "SLA_END_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SLA_DURATION_SEC=$DURATION_SEC"
            echo "BUSINESS_SLA=KST 06:00 Î¶¨Ìè¨Ìä∏ ÎßàÍ∞êÍπåÏßÄ ÏôÑÎ£å"
            echo "BUSINESS_IMPACT=ÏßÄÏó∞ Ïãú Î¶¨Ïä§ÌÅ¨ Ïä§ÏΩîÏñ¥ Î≥¥Í≥† ÏßÄÏó∞"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
          } >> "${DATA_ROOT}/logs/sla_timing.log"
          echo_note "SLA_DURATION_SEC=${DURATION_SEC}s Í∏∞Î°ù ÏôÑÎ£å"

      #######################################################################
      # 27.5 Delta Summary (Ïù¥Ï†Ñ Ïã§ÌñâÍ≥º ÎπÑÍµê ÏöîÏïΩ)
      #######################################################################
      - name: üìà Delta Summary ÏöîÏïΩ Î°úÍ∑∏ ÏÉùÏÑ±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          DELTA_LOG="${DATA_ROOT}/logs/run_delta_summary.log"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          LAST_HIST="${DATA_ROOT}/history/last_run_stats.txt"
          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          LAST_CAP="${DATA_ROOT}/history/last_capacity.txt"
          PLAN_DIFF_LOG="${DATA_ROOT}/logs/plan_diff.log"
          SCHEMA_DIFF_LOG="${DATA_ROOT}/logs/schema_diff.log"

          CURR_ACTUAL=$(grep '^ACTUAL_ROWCOUNT=' "$QUALITY_LOG" 2>/dev/null | head -n1 | cut -d= -f2)
          PREV_ACTUAL=$(grep '^ACTUAL_ROWCOUNT=' "$LAST_HIST" 2>/dev/null | head -n1 | cut -d= -f2)

          CURR_FINAL_MB=$(grep 'FINAL_MB_CURR=' "$CAP_LOG" 2>/dev/null | head -n1 | cut -d= -f2)
          PREV_FINAL_MB=$(grep '^FINAL_MB=' "$LAST_CAP" 2>/dev/null | head -n1 | cut -d= -f2)

          PLAN_CHANGED="NO"
          if grep -qv "first run or no previous plan" "$PLAN_DIFF_LOG" 2>/dev/null; then
            if [ -s "$PLAN_DIFF_LOG" ]; then
              PLAN_CHANGED="YES"
            fi
          fi

          SCHEMA_CHANGED="NO"
          if grep -qv "first run or no prev schema" "$SCHEMA_DIFF_LOG" 2>/dev/null; then
            if [ -s "$SCHEMA_DIFF_LOG" ]; then
              SCHEMA_CHANGED="YES"
            fi
          fi

          {
            echo "=== RUN DELTA SUMMARY ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "CURR_ACTUAL_ROWCOUNT=${CURR_ACTUAL:-N/A}"
            echo "PREV_ACTUAL_ROWCOUNT=${PREV_ACTUAL:-N/A}"
            echo "CURR_FINAL_MB=${CURR_FINAL_MB:-N/A}"
            echo "PREV_FINAL_MB=${PREV_FINAL_MB:-N/A}"
            echo "PLAN_CHANGED=${PLAN_CHANGED}"
            echo "SCHEMA_CHANGED=${SCHEMA_CHANGED}"
          } > "$DELTA_LOG"

          echo_note "Delta Summary Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $DELTA_LOG"
          cat "$DELTA_LOG" || true

      #######################################################################
      # 28. Í∞êÏÇ¨ ÏöîÏïΩ Î°úÍ∑∏ (audit_run_summary.log)
      #######################################################################
      - name: üßæ Í∞êÏÇ¨ ÏöîÏïΩ Î°úÍ∑∏ ÏÉùÏÑ± (audit_run_summary.log)
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"
          : "${SNAP_TAG:=N/A}"
          : "${SNAP_FILE:=N/A}"

          AUDIT_FILE="${DATA_ROOT}/logs/audit_run_summary.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          SLA_FILE="${DATA_ROOT}/logs/sla_timing.log"
          RETENTION_LOG="${DATA_ROOT}/logs/retention_policy.log"
          DIFF_LOG="${DATA_ROOT}/logs/schema_diff.log"
          DR_LOG="${DATA_ROOT}/logs/dr_backup_attempt.log"
          GOV_LOG="${DATA_ROOT}/governance/governance_approval.log"
          HASH_LOG="${DATA_ROOT}/logs/snapshot_hash.txt"
          ISO_HASH_FILE="${DATA_ROOT}/logs/iso_hash.txt"
          PARTITION_LOG="${DATA_ROOT}/logs/partition_access.log"
          LOCK_LOG="${DATA_ROOT}/logs/lock_contention.log"
          PLAN_LOG="${DATA_ROOT}/logs/query_plan_sample.log"
          TUNE_LOG="${DATA_ROOT}/logs/tuning_recommendations.log"
          STATS_LOG="${DATA_ROOT}/logs/stats_maintenance.log"
          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          CLASS_LOG="${DATA_ROOT}/governance/table_classification.log"
          DRPLAY="${DATA_ROOT}/governance/recovery_playbook.txt"
          DR_TOPO_FILE="${DATA_ROOT}/${DR_TOPOLOGY_DIR}/dr_topology.txt"
          ROTATE_LOG="${DATA_ROOT}/logs/dr_rotation.log"
          LRQ_LOG="${DATA_ROOT}/logs/dba_lrq_check.log"
          SKEW_LOG="${DATA_ROOT}/logs/dba_skewness_check.log"
          PLAN_DIFF_LOG="${DATA_ROOT}/logs/plan_diff.log"
          DELTA_LOG="${DATA_ROOT}/logs/run_delta_summary.log"

          {
            echo "=== PIPELINE AUDIT SUMMARY ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "BRANCH=$GITHUB_REF_NAME"
            echo "COMMIT_SHA=$GITHUB_SHA"
            echo
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "TABLES_USED=${TBL_STAGE},${TBL_FINAL},${TBL_AUDIT}"
            echo "GEN_ROWS=${GEN_ROWS}"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            echo
            echo "--- SNAPSHOT ARTIFACTS ---"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_TAR=$SNAP_FILE"
            echo "SNAP_ISO=$ISO_FILE"
            echo
            echo "--- STATUS_FILE ---"
            cat "$STATUS_FILE" 2>/dev/null || echo "(no pipeline_status.log)"
            echo
            echo "--- QUALITY_LOG ---"
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo
            echo "--- SLA_TIMING ---"
            cat "$SLA_FILE" 2>/dev/null || echo "(no sla_timing.log)"
            echo
            echo "--- SYSTEM UPGRADE LOG (head 40) ---"
            head -n 40 "$UPG_LOG" 2>/dev/null || echo "(no system_upgrade.log)"
            echo
            echo "--- RETENTION POLICY ---"
            cat "$RETENTION_LOG" 2>/dev/null || echo "(no retention_policy.log)"
            echo
            echo "--- SCHEMA DIFF ---"
            head -n 80 "$DIFF_LOG" 2>/dev/null || echo "(no schema_diff.log)"
            echo
            echo "--- DR BACKUP & ROTATION (${DR_RETENTION_DAYS}d) ---"
            head -n 60 "$DR_LOG" 2>/dev/null || echo "(no dr_backup_attempt.log)"
            echo ">>> DR ROTATION LOG"
            head -n 60 "$ROTATE_LOG" 2>/dev/null || echo "(no dr_rotation.log)"
            echo
            echo "--- GOVERNANCE / APPROVAL ---"
            head -n 60 "$GOV_LOG" 2>/dev/null || echo "(no governance_approval.log)"
            echo
            echo "--- DBA ADVANCED (LRQ / Skew / Plan Diff) ---"
            echo ">>> dba_lrq_check.log"
            head -n 60 "$LRQ_LOG" 2>/dev/null || echo "(no dba_lrq_check.log)"
            echo ">>> dba_skewness_check.log"
            head -n 60 "$SKEW_LOG" 2>/dev/null || echo "(no dba_skewness_check.log)"
            echo ">>> plan_diff.log"
            head -n 60 "$PLAN_DIFF_LOG" 2>/dev/null || echo "(no plan_diff.log)"
            echo
            echo "--- DBA PERFORMANCE / PARTITION / LOCK / PLAN / TUNING ---"
            echo ">>> partition_access.log"
            head -n 60 "$PARTITION_LOG" 2>/dev/null || echo "(no partition_access.log)"
            echo ">>> lock_contention.log"
            head -n 60 "$LOCK_LOG" 2>/dev/null || echo "(no lock_contention.log)"
            echo ">>> query_plan_sample.log"
            head -n 60 "$PLAN_LOG" 2>/dev/null || echo "(no query_plan_sample.log)"
            echo ">>> tuning_recommendations.log"
            head -n 60 "$TUNE_LOG" 2>/dev/null || echo "(no tuning_recommendations.log)"
            echo ">>> stats_maintenance.log"
            head -n 60 "$STATS_LOG" 2>/dev/null || echo "(no stats_maintenance.log)"
            echo ">>> capacity_growth.log"
            head -n 60 "$CAP_LOG" 2>/dev/null || echo "(no capacity_growth.log)"
            echo
            echo "--- CLASSIFICATION / DR PLAYBOOK / DR TOPOLOGY ---"
            echo ">>> table_classification.log"
            head -n 60 "$CLASS_LOG" 2>/dev/null || echo "(no table_classification.log)"
            echo ">>> recovery_playbook.txt (RPO/RTO)"
            head -n 60 "$DRPLAY" 2>/dev/null || echo "(no recovery_playbook.txt)"
            echo ">>> dr_topology.txt"
            head -n 60 "$DR_TOPO_FILE" 2>/dev/null || echo "(no dr_topology.txt)"
            echo
            echo "--- SNAPSHOT HASH (tar.gz) ---"
            cat "$HASH_LOG" 2>/dev/null || echo "(no snapshot_hash.txt)"
            echo
            echo "--- SNAPSHOT ISO HASH ---"
            cat "$ISO_HASH_FILE" 2>/dev/null || echo "(no iso_hash.txt)"
            echo
            echo "--- RUN DELTA SUMMARY ---"
            cat "$DELTA_LOG" 2>/dev/null || echo "(no run_delta_summary.log)"
          } > "$AUDIT_FILE"

          echo_note "Í∞êÏÇ¨ ÏöîÏïΩ Î°úÍ∑∏ ÏÉùÏÑ± ÏôÑÎ£å -> $AUDIT_FILE"
          head -n 200 "$AUDIT_FILE" || true

      #######################################################################
      # 29. Artifact ÏóÖÎ°úÎìú (Ï†ÑÏ≤¥ Ï¶ùÏ†Å Î¨∂Ïùå)
      #######################################################################
      - name: üì¶ Ïã§Ìñâ ÏÇ∞Ï∂úÎ¨º ÏóÖÎ°úÎìú (logs / quality / source / snapshot Îì±)
        uses: actions/upload-artifact@v4
        with:
          name: teradata-run-${{ github.run_id }}
          path: |
            ${{ env.DATA_ROOT }}/logs/**
            ${{ env.DATA_ROOT }}/quality/**
            ${{ env.DATA_ROOT }}/source_files/**
            ${{ env.DATA_ROOT }}/release/**
            ${{ env.DATA_ROOT }}/landing_zone/**
            ${{ env.DATA_ROOT }}/${{ env.DR_DIR_NAME }}/**
            ${{ env.DATA_ROOT }}/${{ env.DR_TOPOLOGY_DIR }}/**
            ${{ env.DATA_ROOT }}/extract_out/**
            ${{ env.DATA_ROOT }}/tmp/**
            ${{ env.DATA_ROOT }}/health/**
            ${{ env.DATA_ROOT }}/governance/**
            ${{ env.DATA_ROOT }}/history/**
            ${{ env.LOG_DIR }}/**
          if-no-files-found: warn
          retention-days: 14

      #######################################################################
      # 29.5 DBA Ï†ÑÏö© Artifact ÏóÖÎ°úÎìú
      #######################################################################
      - name: üì¶ DBA Î°úÍ∑∏ Ï†ÑÏö© Artifact ÏóÖÎ°úÎìú
        uses: actions/upload-artifact@v4
        with:
          name: teradata-dba-logs-${{ github.run_id }}
          path: |
            ${{ env.DATA_ROOT }}/logs/dba_*.log
            ${{ env.DATA_ROOT }}/logs/partition_access.log
            ${{ env.DATA_ROOT }}/logs/lock_contention.log
            ${{ env.DATA_ROOT }}/logs/query_plan_sample.log
            ${{ env.DATA_ROOT }}/logs/tuning_recommendations.log
            ${{ env.DATA_ROOT }}/logs/stats_maintenance.log
            ${{ env.DATA_ROOT }}/logs/dba_skewness_check.log
            ${{ env.DATA_ROOT }}/logs/dba_lrq_check.log
            ${{ env.DATA_ROOT }}/logs/plan_diff.log
          if-no-files-found: warn
          retention-days: 30

      #######################################################################
      # 30. QC/SLA ÏúÑÎ∞ò Ïãú GitHub Issue ÏûêÎèô ÏÉùÏÑ±
      #######################################################################
      - name: üêû QC/SLA ÏúÑÎ∞ò Ïãú GitHub Issue ÏûêÎèô ÏÉùÏÑ±
        if: always()
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -e
          source /tmp/echo_helpers.sh

          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          SLA_LOG="${DATA_ROOT}/logs/sla_timing.log"

          QC_FLAG="OK"
          SLA_FLAG="OK"

          if grep -q "ROW_COUNT_OK=false" "$QUALITY_LOG" 2>/dev/null; then
            QC_FLAG="FAIL"
          fi

          if grep -q "SLA_DURATION_SEC=" "$SLA_LOG" 2>/dev/null; then
            DURATION=$(grep 'SLA_DURATION_SEC=' "$SLA_LOG" | tail -n1 | cut -d= -f2)
            if [ "$DURATION" -gt 900 ]; then
              SLA_FLAG="FAIL"
            fi
          fi

          if [ "$QC_FLAG" = "OK" ] && [ "$SLA_FLAG" = "OK" ]; then
            echo_note "QC/SLA Î™®Îëê Ï†ïÏÉÅ ‚Üí Issue ÏÉùÏÑ± Ïä§ÌÇµ"
            exit 0
          fi

          TITLE="Teradata FinOps Batch: QC/SLA Ïù¥Ïäà (Run $GITHUB_RUN_ID)"
          BODY_FILE="${DATA_ROOT}/tmp/issue_body.txt"
          mkdir -p "$(dirname "$BODY_FILE")"

          {
            echo "Îü∞ ID: $GITHUB_RUN_ID"
            echo "Ïª§Î∞ã: $GITHUB_SHA"
            echo "Î∏åÎûúÏπò: $GITHUB_REF_NAME"
            echo
            echo "QC ÏÉÅÌÉú: $QC_FLAG"
            echo "SLA ÏÉÅÌÉú: $SLA_FLAG"
            echo
            echo "=== QUALITY ==="
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo
            echo "=== SLA TIMING ==="
            cat "$SLA_LOG" 2>/dev/null || echo "(no sla_timing.log)"
          } > "$BODY_FILE"

          echo_note "QC/SLA Ïù¥Ïäà Í∞êÏßÄ ‚Üí GitHub Issue ÏÉùÏÑ± ÏãúÎèÑ"
          gh issue create \
            --title "$TITLE" \
            --body-file "$BODY_FILE" \
            --label "finops" \
            --label "batch-issue" \
            || echo "::warning::Issue ÏÉùÏÑ± Ïã§Ìå® (Í∂åÌïú Î∂ÄÏ°± Îì±)"

      #######################################################################
      # 31. Ïã§Ìå® Ïãú Slack ÏïåÎ¶º
      #######################################################################
      - name: üö® Ïã§Ìå® ÏïåÎ¶º (Slack)
        if: failure()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "‚ùå Teradata FinOps Batch ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§Ìå®",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§Ìñâ Ïã§Ìå®\n*Run ID:* ${{ github.run_id }}\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|Ïã§Ìñâ Î°úÍ∑∏ Ïó¥Í∏∞>"
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      #######################################################################
      # 32. ÏÑ±Í≥µ Ïãú Slack ÏöîÏïΩ ÏïåÎ¶º
      #######################################################################
      - name: üìä ÏÑ±Í≥µ ÏöîÏïΩ ÏïåÎ¶º (Slack)
        if: success()
        uses: slackapi/slack-github-action@v1.26.0
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        with:
          payload: |
            {
              "text": "‚úÖ Teradata FinOps Batch ÏÑ±Í≥µ",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Teradata FinOps Batch ÏÑ±Í≥µ*\n*Run ID:* ${{ github.run_id }}\n*Branch:* `${{ github.ref_name }}`\n*Commit:* `${{ github.sha }}`"
                  }
                }
              ]
            }

      #######################################################################
      # 33. ÌååÏù¥ÌîÑÎùºÏù∏ Ï¢ÖÎ£å (Ìï≠ÏÉÅ ÏÑ±Í≥µ)
      #######################################################################
      - name: ‚úÖ ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å (Ìï≠ÏÉÅ ÏÑ±Í≥µ)
        if: always()
        run: |
          source /tmp/echo_helpers.sh || true
          echo_note "Teradata FinOps Batch ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ÑÏ≤¥ Îã®Í≥Ñ ÏàòÌñâ ÏôÑÎ£å"
          echo_note "- EchoOps Î°úÍπÖ (/tmp/echo_helpers.sh Í∏∞Î∞ò)"
          echo_note "- ÎåÄÎüâ Í∞ÄÏÉÅÎç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± / ÌíàÏßàÍ≤ÄÏ¶ù / Ï¶ùÍ∞êÎ•† / Í≥†ÏúÑÌóò Í±∞ÎûòÎπÑÏú®"
          echo_note "- ÏãúÏä§ÌÖú ÏóÖÍ∑∏Î†àÏù¥Îìú & pkg diff Í∏∞Î°ù (mode=full & debug_mode=false)"
          echo_note "- DBA Í¥ÄÏ†ê(ÌååÌã∞ÏÖò ÌîÑÎ£®Îãù, ÎùΩ Í≤ΩÌï©, ÏøºÎ¶¨ ÌîåÎûú, ÌäúÎãù, ÌÜµÍ≥ÑÏàòÏßë, Ïö©Îüâ ÏÑ±Ïû•)"
          echo_note "- DBA Advanced(LRQ, Skewness, Plan Diff) Ìè¨Ìï®"
          echo_note "- ÎØºÍ∞êÎç∞Ïù¥ÌÑ∞ ÎßàÏä§ÌÇπ/Í∂åÌïú Í∞êÏÇ¨/Í±∞Î≤ÑÎÑåÏä§/Ïò®ÏΩú/RPO¬∑RTO Î¨∏ÏÑúÌôî"
          echo_note "- tar.gz Ïä§ÎÉÖÏÉ∑ + (ÏòµÏÖò) ISO Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ± (hash Ìè¨Ìï®)"
          echo_note "- DR ÌÜ†Ìè¥Î°úÏßÄ/DR Î≥¥Í¥Ä/1Ïùº Î°úÌÖåÏù¥ÏÖò/DR Î≥µÍµ¨ ÌîåÎ†àÏù¥Î∂Å"
          echo_note "- GitHub Release ÏóÖÎ°úÎìú(ÏòµÏÖò, mode=full & debug_mode=false)"
          echo_note "- HTML Î¶¨Ìè¨Ìä∏ / run_delta_summary / audit_run_summary / artifact ÏóÖÎ°úÎìú ÏôÑÎ£å"
          echo "‚úÖ Î™®Îì† ÏÇ∞Ï∂úÎ¨ºÏùÄ artifact teradata-run-${GITHUB_RUN_ID} Î∞è teradata-dba-logs-${GITHUB_RUN_ID} Î°ú ÏóÖÎ°úÎìúÎêòÏóàÏäµÎãàÎã§."
