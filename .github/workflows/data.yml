name: "ğŸ¦ Teradata FinOps Batch â€” EchoOps + Audit + Snapshot + Schedule (ë§Œì „ëŒ€ë¹„ + ëŒ€ëŸ‰ê°€ìƒë°ì´í„° + ì‹œìŠ¤í…œì—…ê·¸ë ˆì´ë“œ)"

on:
  push:
    branches: [ "main" ]
  # ìˆ˜ë™ ì‹¤í–‰ (dispatch inputsëŠ” 10ê°œ ì´í•˜)
  workflow_dispatch:
    inputs:
      mode:                 # 1
        description: "ì‹¤í–‰ ëª¨ë“œ(full=ì „ì²´ / lite=ì¼ë¶€ ë‹¨ê³„ë§Œ)"
        type: choice
        options: [full, lite]
        default: full

      gen_rows:             # 2
        description: "ìƒì„±í•  ê°€ìƒ ë°ì´í„° í–‰ ìˆ˜ (ë¹ˆê°’ì´ë©´ ê¸°ë³¸ GEN_ROWS)"
        required: false
        default: ""

      expected_rows:        # 3
        description: "í’ˆì§ˆê²€ì¦ ê¸°ëŒ€ ë¡œìš° ìˆ˜ (ë¹ˆê°’ì´ë©´ ê¸°ë³¸ EXPECTED_ROWCOUNT)"
        required: false
        default: ""

      dr_backup:            # 4
        description: "DR(ì™¸ë¶€) ë°±ì—… ì‹œë„ ì—¬ë¶€ (true/false)"
        type: choice
        options: ["true", "false"]
        default: "false"

      include_masking_audit:  # 5
        description: "ë¯¼ê°í•„ë“œ ë§ˆìŠ¤í‚¹ ì •ì±… ë¡œê·¸ í¬í•¨ ì—¬ë¶€"
        type: choice
        options: ["true", "false"]
        default: "true"

      include_healthcheck:    # 6
        description: "ì‹œìŠ¤í…œ í—¬ìŠ¤/ë³´ì•ˆ ì ê²€ ë¡œê·¸ í¬í•¨ ì—¬ë¶€"
        type: choice
        options: ["true", "false"]
        default: "true"

      include_schema_diff:    # 7
        description: "DDL/TPT ë³€ê²½ diff ê¸°ë¡ ì—¬ë¶€"
        type: choice
        options: ["true", "false"]
        default: "true"

      sla_tracking:           # 8
        description: "SLA/SLO ì‹œê°„ ì¸¡ì •, ì¬ì‹œë„ ê¸°ë¡ ì—¬ë¶€"
        type: choice
        options: ["true", "false"]
        default: "true"

  schedule:
    # ë§¤ì¼ 15:30 UTC == í•œêµ­ì‹œê°„(KST) ìƒˆë²½ 00:30 ìë™ ì•¼ê°„ë°°ì¹˜
    - cron: "30 15 * * *"

permissions:
  contents: write   # ë¦´ë¦¬ì¦ˆ íƒœê·¸/ìì‚° ì—…ë¡œë“œìš© (gh release create)

env:
  ###########################################################################
  # ê³µí†µ ê²½ë¡œ / ëŒ€ìƒ í…Œì´ë¸”
  ###########################################################################
  DATA_ROOT: /home/runner/td_data

  # ì„œë¹„ìŠ¤/ë„ë©”ì¸ë³„ í…Œì´ë¸” (Staging -> Final -> Audit)
  TBL_STAGE: STG_DATA
  TBL_FINAL: FINAL_DATA
  TBL_AUDIT: LOAD_AUDIT_LOG

  # ë³‘í•© íƒ€ê¹ƒ ìš´ì˜ í…Œì´ë¸”
  TARGET_TABLE: FINAL_DATA

  ###########################################################################
  # ê°€ìƒ ë°ì´í„°/í’ˆì§ˆ ê¸°ì¤€
  #
  # GEN_ROWS: ì´ë²ˆ ë°°ì¹˜ì—ì„œ "ìƒì„±/ì ì¬"í•œë‹¤ê³  ê°€ì •í•˜ëŠ” ê°€ìƒ ë ˆì½”ë“œ ìˆ˜
  # EXPECTED_ROWCOUNT: í’ˆì§ˆ ê²€ì¦ ì‹œ ê¸°ëŒ€ ë¡œìš° ìˆ˜ë¡œ ì‚¬ìš©
  ###########################################################################
  GEN_ROWS: "500000"
  EXPECTED_ROWCOUNT: "500000"

jobs:
  teradata_pipeline_job:
    runs-on: ubuntu-latest

    steps:
      #######################################################################
      # 0. ì½”ë“œ ì²´í¬ì•„ì›ƒ + ì‹œì‘ ì•Œë¦¼
      #######################################################################
      - name: "ğŸ“¥ ì½”ë“œ ì²´í¬ì•„ì›ƒ"
        uses: actions/checkout@v5

      - name: "ğŸ“ ì›Œí¬í”Œë¡œìš° ì‹œì‘ ë¡œê¹…"
        run: |
          echo "::notice::Teradata í†µí•© ì›Œí¬í”Œë¡œìš° ì‹œì‘ (ì•¼ê°„ë°°ì¹˜/ìˆ˜ë™ì‹¤í–‰/Push ê³µí†µ)."
          echo "::debug::DATA_ROOT=${DATA_ROOT}, TARGET_TABLE=${TARGET_TABLE}, GEN_ROWS=${GEN_ROWS}"
          echo "::debug::dispatch.mode=${{ github.event.inputs.mode || 'N/A' }}"
          echo "::debug::dispatch.dr_backup=${{ github.event.inputs.dr_backup || 'N/A' }}"

      #######################################################################
      # 0.2 SLA íƒ€ì´ë¨¸ ì‹œì‘ (SLA/SLO ì¶”ì ìš©)
      #######################################################################
      - name: "â± SLA íƒ€ì´ë¨¸ ì‹œì‘"
        run: |
          mkdir -p "${DATA_ROOT}/logs" "${DATA_ROOT}/history"
          date +%s > "${DATA_ROOT}/logs/start_epoch.txt"
          echo "SLA_START_TS=$(date +%Y-%m-%dT%H:%M:%S%z)" > "${DATA_ROOT}/logs/sla_timing.log"

      #######################################################################
      # 0.5 ëŸ¬ë„ˆ í™˜ê²½ ì—…ê·¸ë ˆì´ë“œ (apt upgrade ë“±)
      #     ì‹¤íŒ¨í•´ë„ ê³„ì†. ê²°ê³¼ëŠ” ì—…ê·¸ë ˆì´ë“œ ë¡œê·¸ë¡œ ë‚¨ê¹€
      #######################################################################
      - name: "ğŸ”„ ëŸ¬ë„ˆ íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ (ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ & ì—…ê·¸ë ˆì´ë“œ)"
        continue-on-error: true
        run: |
          set +e
          mkdir -p "${DATA_ROOT}/logs"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          BEFORE_LIST="${DATA_ROOT}/logs/pkg_list_before.txt"
          AFTER_LIST="${DATA_ROOT}/logs/pkg_list_after.txt"

          dpkg -l > "$BEFORE_LIST" 2>/dev/null || true

          {
            echo "===== SYSTEM UPGRADE START ====="
            date
            echo "--- apt-get update ---"
            sudo apt-get update -y || echo "[WARN] apt-get update ì‹¤íŒ¨"
            echo "--- apt-get dist-upgrade ---"
            sudo apt-get -o Dpkg::Options::="--force-confnew" dist-upgrade -y || echo "[WARN] dist-upgrade ì‹¤íŒ¨"
            echo "--- apt-get autoremove ---"
            sudo apt-get autoremove -y || true
            echo "--- uname -a ---"
            uname -a
            echo "--- lsb_release -a (ê°€ëŠ¥í•˜ë©´) ---"
            lsb_release -a 2>/dev/null || echo "lsb_release not available"
            echo "===== SYSTEM UPGRADE END ====="
          } > "$UPG_LOG" 2>&1

          dpkg -l > "$AFTER_LIST" 2>/dev/null || true
          diff -u "$BEFORE_LIST" "$AFTER_LIST" > "${DATA_ROOT}/logs/pkg_upgrade_diff.txt" || true

          echo "::notice::ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ(íŒ¨í‚¤ì§€ ìµœì‹ í™”) ì‹œë„ ì™„ë£Œ. ìƒì„¸ ë‚´ìš©ì€ $UPG_LOG ë° pkg_upgrade_diff.txt ì°¸ê³ ."
          set -e

      #######################################################################
      # 1. ë””ë ‰í† ë¦¬ ì¤€ë¹„ + êµ¬ì¡° ìƒì„±
      #######################################################################
      - name: "ğŸ“‚ ë””ë ‰í† ë¦¬ ìƒì„± ë° í™˜ê²½ ì´ˆê¸°í™”"
        run: |
          set -e
          echo "ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì¤‘..."
          mkdir -p "${DATA_ROOT}"/{source_files,landing_zone,logs,archive,extract_out,tmp,quality,release,history,governance,health,dr_backup}
          echo "ìƒì„±ëœ ë””ë ‰í† ë¦¬ í™•ì¸:"
          ls -R "${DATA_ROOT}" || true

          # íŒŒí‹°ì…˜/ë³´ì¡´ ì •ì±… ì‹œë®¬ ë¡œê·¸(ë°ì´í„° ë³´ì¡´/ì•„ì¹´ì´ë¸Œ ì „ëµ ê¸°ë¡)
          cat > "${DATA_ROOT}/logs/retention_policy.log" <<'EOF'
          [RETENTION POLICY SIMULATION]
          - STG_DATA: ì¼ì íŒŒí‹°ì…˜, 30ì¼ ì´ˆê³¼ íŒŒí‹°ì…˜ì€ ì•„ì¹´ì´ë¸Œ í›„ ì‚­ì œ ëŒ€ìƒ
          - FINAL_DATA: ì˜êµ¬ ë³´ì¡´, ë‹¨ ê°œì¸ì •ë³´ í•„ë“œëŠ” ë§ˆìŠ¤í‚¹ ìƒíƒœë§Œ ìœ ì§€
          - LOAD_AUDIT_LOG: 1ë…„ ë³´ì¡´ í›„ ì½œë“œìŠ¤í† ë¦¬ì§€ ì´ì „
          - ì•„ì¹´ì´ë¸Œ ê²½ë¡œ: /home/runner/td_data/archive/YYYY/MM/DD/*.gz
          These are simulated operational policies for audit/compliance.
          EOF

      #######################################################################
      # 1.5 ê°€ìƒ ë°ì´í„° ëŒ€ëŸ‰ ìƒì„± (landing_zone ì— ëŒ€ìš©ëŸ‰ CSV ìƒì„±)
      #     dispatch ì…ë ¥ê°’(gen_rows)ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
      #######################################################################
      - name: "ğŸ— ê°€ìƒ ë°ì´í„° ëŒ€ëŸ‰ ìƒì„±"
        run: |
          set -e
          # rows ê²°ì • (dispatch override ìš°ì„ )
          ROWS_INPUT="${{ github.event.inputs.gen_rows || '' }}"
          if [ -n "$ROWS_INPUT" ]; then
            ROWS="$ROWS_INPUT"
          else
            ROWS="${GEN_ROWS}"
          fi

          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"

          echo "COL1,COL2,AMOUNT,LOAD_TS" > "$SRC_FILE"
          echo "ê°€ìƒ ë°ì´í„° ${ROWS}í–‰ ìƒì„± ì¤‘..."
          i=1
          while [ $i -le $ROWS ]; do
            # ëœë¤ ê¸ˆì•¡ (100 ~ 999900 ì‚¬ì´)
            AMT=$(( (RANDOM % 999800) + 100 ))
            MERCH=$(( (RANDOM % 9000) + 1000 ))

            # ì´ìƒ ê±°ë˜ ì‹œë®¬: ì•„ì£¼ í° ê¸ˆì•¡ì€ 'FLAG-HIGH'
            if [ $AMT -gt 900000 ]; then
              NOTE="MERCHANT_${MERCH}_FLAG-HIGH_${i}"
            else
              NOTE="MERCHANT_${MERCH}_NOTE_${i}"
            fi

            echo "${i},${NOTE},${AMT},$(date +%Y-%m-%dT%H:%M:%S%z)" >> "$SRC_FILE"
            i=$((i+1))
          done

          ls -lh "$SRC_FILE"
          head -n 5 "$SRC_FILE"
          tail -n 5 "$SRC_FILE"

          echo "${ROWS}" > "${DATA_ROOT}/tmp/generated_rowcount.txt"
          echo "::notice::ê°€ìƒ CSV ìƒì„± ì™„ë£Œ (${ROWS} rows) -> $SRC_FILE"

      #######################################################################
      # 2. í™˜ê²½ ìŠ¤ëƒ…ìƒ· (ë³´ì•ˆ/ê°ì‚¬ìš© ëŸ°íƒ€ì„ ì¦ë¹™)
      #######################################################################
      - name: "ğŸ” ëŸ°íƒ€ì„ í™˜ê²½ ìŠ¤ëƒ…ìƒ· ì €ì¥"
        run: |
          SNAP="${DATA_ROOT}/logs/env_snapshot.txt"
          {
            echo "===== ENV SNAPSHOT ====="
            date
            uname -a
            whoami
            echo "--- PATH ---"
            echo "$PATH"
            echo "--- DISK (df -h) ---"
            df -h
            echo "--- MEMORY (free -m) ---"
            free -m || true
            echo "--- GITHUB CONTEXT ---"
            echo "RUN_ID=$GITHUB_RUN_ID"
            echo "RUN_NUMBER=$GITHUB_RUN_NUMBER"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "SHA=$GITHUB_SHA"
            echo "BRANCH=$GITHUB_REF_NAME"
          } > "$SNAP"
          echo "::debug::í™˜ê²½ ìŠ¤ëƒ…ìƒ· ê¸°ë¡ ì™„ë£Œ -> $SNAP"

      #######################################################################
      # 2.5 ì‹œìŠ¤í…œ í—¬ìŠ¤/ë³´ì•ˆ ì ê²€ (ë„¤íŠ¸ì›Œí¬, dmesg, í¬íŠ¸ ë“±)
      #######################################################################
      - name: "ğŸ©º ì‹œìŠ¤í…œ í—¬ìŠ¤ ë° ë³´ì•ˆ ì ê²€"
        if: ${{ github.event.inputs.include_healthcheck != 'false' }}
        run: |
          HEALTH_LOG="${DATA_ROOT}/health/system_health.log"
          {
            echo "===== SYSTEM HEALTH CHECK ====="
            date
            echo "--- dmesg (tail 50) ---"
            dmesg | tail -n 50 || true
            echo
            echo "--- TCP/UDP ì†Œì¼“ ìƒíƒœ (ss -tuna head 20) ---"
            ss -tuna | head -n 20 || true
            echo
            echo "--- CPU/MEM load (top -b -n1 head 20) ---"
            top -b -n1 | head -n 20 || true
            echo
            echo "--- I/O stat (iostat if available) ---"
            iostat 2>/dev/null || echo "iostat not available"
            echo
            echo "--- ë„¤íŠ¸ì›Œí¬ ì™¸ë¶€ ì—°ê²° í…ŒìŠ¤íŠ¸ ---"
            curl -I https://example.com 2>&1 | head -n 5 || echo "curl external check failed or blocked"
          } > "$HEALTH_LOG"
          echo "::notice::ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $HEALTH_LOG"

      #######################################################################
      # 3. DDL / TPT / ì¿¼ë¦¬ ì •ì˜ ë°±ì—…
      #######################################################################
      - name: "ğŸ§± DDL/TPT/ì¿¼ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ë°±ì—… (ê° ì„œë¹„ìŠ¤ë³„ table / query)"
        run: |
          set -e
          mkdir -p "${DATA_ROOT}/source_files/sql"
          mkdir -p "${DATA_ROOT}/source_files/tpt"

          # í…Œì´ë¸” ì •ì˜
          cat > "${DATA_ROOT}/source_files/sql/create_tables.sql" <<'SQL'
          -- [STAGING TABLE] (ì›ì²œ ë¤í”„ë¥¼ ê·¸ëŒ€ë¡œ ì ì¬)
          CREATE TABLE STG_DATA (
            COL1       INTEGER,
            COL2       VARCHAR(100),
            AMOUNT     BIGINT,
            LOAD_TS    TIMESTAMP,
            BATCH_ID   VARCHAR(40)
          );

          -- [FINAL TABLE] (ì •ì œ/ë³€í™˜/ì¤‘ë³µì œê±° í›„ ìš´ì˜ì˜ì—­)
          CREATE TABLE FINAL_DATA (
            COL1       INTEGER,
            COL2       VARCHAR(100),
            AMOUNT     BIGINT,
            LOAD_TS    TIMESTAMP,
            ETL_TS     TIMESTAMP,
            SRC_BATCH  VARCHAR(40)
          );

          -- [AUDIT TABLE] (ë°°ì¹˜ ì ì¬ ì´ë ¥/í’ˆì§ˆ ê²€ì¦ ê²°ê³¼)
          CREATE TABLE LOAD_AUDIT_LOG (
            AUDIT_TS        TIMESTAMP,
            BATCH_ID        VARCHAR(40),
            SRC_FILE        VARCHAR(255),
            ROW_LOADED      INTEGER,
            ROW_EXPECTED    INTEGER,
            STATUS_CODE     INTEGER,
            STATUS_MESSAGE  VARCHAR(2000),
            OPERATOR        VARCHAR(128)
          );

          -- ì¸ë±ìŠ¤ ì˜ˆì‹œ
          CREATE INDEX IDX_FINAL_DATA_COL1 ON FINAL_DATA (COL1);
          CREATE INDEX IDX_AUDIT_BATCH     ON LOAD_AUDIT_LOG (BATCH_ID);
          SQL

          # ë³‘í•©/ì •ì œ í”„ë¡œì‹œì € ì˜ˆì‹œ
          cat > "${DATA_ROOT}/source_files/sql/proc_LOAD_AND_MERGE.sql" <<'SQL'
          REPLACE PROCEDURE LOAD_AND_MERGE (
            IN p_batch_id VARCHAR(40),
            IN p_src_file VARCHAR(255)
          )
          BEGIN
            /* 1) STG_DATA -> FINAL_DATA ë¡œ ë³‘í•©/ì •ì œ */
            INSERT INTO FINAL_DATA (COL1, COL2, AMOUNT, LOAD_TS, ETL_TS, SRC_BATCH)
            SELECT
              COL1,
              TRIM(COL2),
              AMOUNT,
              LOAD_TS,
              CURRENT_TIMESTAMP,
              p_batch_id
            FROM STG_DATA
            WHERE BATCH_ID = p_batch_id;

            /* 2) AUDIT LOG ì— ì´ë ¥ ì¶•ì  */
            INSERT INTO LOAD_AUDIT_LOG (
              AUDIT_TS, BATCH_ID, SRC_FILE,
              ROW_LOADED, ROW_EXPECTED,
              STATUS_CODE, STATUS_MESSAGE, OPERATOR
            )
            VALUES (
              CURRENT_TIMESTAMP,
              p_batch_id,
              p_src_file,
              NULL,   -- ì‚¬í›„ UPDATE
              NULL,   -- ì‚¬í›„ UPDATE
              0,
              'LOAD_AND_MERGE executed',
              USER
            );
          END;
          SQL

          # Teradata TPT Bulk Load í…œí”Œë¦¿
          cat > "${DATA_ROOT}/source_files/tpt/load_stg_data.tpt" <<'TPT'
          DEFINE JOB LOAD_STG_DATA
          (
            DEFINE SCHEMA STG_SCHEMA
            (
              COL1       INTEGER,
              COL2       VARCHAR(100),
              AMOUNT     BIGINT,
              LOAD_TS    VARCHAR(30)
            );

            DEFINE OPERATOR FILE_READER
            TYPE DATACONNECTOR PRODUCER
            SCHEMA STG_SCHEMA
            ATTRIBUTES
            (
              FileName = '/home/runner/td_data/landing_zone/input_YYYYMMDD.csv',
              Format   = 'Delimited'
            );

            DEFINE OPERATOR TPT_INSERTER
            TYPE STREAM
            TARGET TABLE STG_DATA
            ATTRIBUTES
            (
              TdpId        = 'TERADATA_SID',
              UserName     = 'ETL_USER',
              UserPassword = 'ETL_PASS',
              LogTable     = 'ETL_LOG_TABLE'
            );

            APPLY
            (
              'INSERT INTO STG_DATA (COL1, COL2, AMOUNT, LOAD_TS, BATCH_ID)
               VALUES (:COL1, :COL2, :AMOUNT, TIMESTAMP :LOAD_TS, ''BATCH_PLACEHOLDER'');'
            )
            TO OPERATOR (TPT_INSERTER[1])
            SELECT
              COL1, COL2, AMOUNT, LOAD_TS
            FROM OPERATOR (FILE_READER[1]);
          );
          TPT

          # í’ˆì§ˆ ê²€ì¦ / í†µê³„ / ê°ì‚¬ ì¡°íšŒ ì¿¼ë¦¬ ëª¨ìŒ
          cat > "${DATA_ROOT}/source_files/sql/quality_queries.sql" <<'SQL'
          -- íŠ¹ì • ë°°ì¹˜ì— ëŒ€í•´ STG_DATA ì ì¬ ê±´ìˆ˜
          SELECT COUNT(*) AS CNT_STG
          FROM STG_DATA
          WHERE BATCH_ID = :BATCH_ID;

          -- ìµœì¢… FINAL_DATA ë°˜ì˜ ê±´ìˆ˜
          SELECT COUNT(*) AS CNT_FINAL
          FROM FINAL_DATA
          WHERE SRC_BATCH = :BATCH_ID;

          -- í’ˆì§ˆ: NULL ë©”ëª¨ ë¹„ìœ¨, ê³ ì•¡ ê±°ë˜ ë¹„ìœ¨ ë“±
          SELECT
            SUM(CASE WHEN COL2 IS NULL OR TRIM(COL2) = '' THEN 1 ELSE 0 END) AS NULL_MEMO_ROWS,
            SUM(CASE WHEN AMOUNT > 900000 THEN 1 ELSE 0 END) AS HIGH_AMOUNT_ROWS,
            COUNT(*) AS TOTAL_ROWS,
            SUM(AMOUNT) AS SUM_AMOUNT,
            AVG(AMOUNT) AS AVG_AMOUNT
          FROM FINAL_DATA
          WHERE SRC_BATCH = :BATCH_ID;

          -- ìµœê·¼ AUDIT ë¡œê·¸ (ë°°ì¹˜ë³„ ìµœì‹  íˆìŠ¤í† ë¦¬)
          SELECT *
          FROM LOAD_AUDIT_LOG
          QUALIFY ROW_NUMBER()
            OVER (PARTITION BY BATCH_ID ORDER BY AUDIT_TS DESC) = 1
          ORDER BY AUDIT_TS DESC;
          SQL

          echo "::notice::DDL/TPT/ì¿¼ë¦¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ${DATA_ROOT}/source_files ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
          ls -R "${DATA_ROOT}/source_files" || true

      #######################################################################
      # 3.5 ìŠ¤í‚¤ë§ˆ ë³€ê²½ ê°ì‹œ (schema diff)
      #######################################################################
      - name: "ğŸ§¾ ìŠ¤í‚¤ë§ˆ ë³€ê²½ diff ê¸°ë¡"
        if: ${{ github.event.inputs.include_schema_diff != 'false' }}
        run: |
          PREV_SCHEMA="${DATA_ROOT}/history/last_create_tables.sql"
          CURR_SCHEMA="${DATA_ROOT}/source_files/sql/create_tables.sql"
          DIFF_LOG="${DATA_ROOT}/logs/schema_diff.log"

          if [ -f "$PREV_SCHEMA" ]; then
            diff -u "$PREV_SCHEMA" "$CURR_SCHEMA" > "$DIFF_LOG" || true
          else
            echo "[first run or no prev schema]" > "$DIFF_LOG"
          fi

          cp "$CURR_SCHEMA" "$PREV_SCHEMA" 2>/dev/null || cp "$CURR_SCHEMA" "$PREV_SCHEMA"

          echo "::notice::ìŠ¤í‚¤ë§ˆ diff ê²°ê³¼ -> $DIFF_LOG"
          head -n 200 "$DIFF_LOG" || true

      #######################################################################
      # 4. ë°ì´í„° ì ì¬ & ë³‘í•© ì‹œë®¬
      #######################################################################
      - name: "ğŸ”„ ë°ì´í„° ë¡œë“œ ë° ë³€í™˜/ë³‘í•© ì‹¤í–‰ (ëŒ€ëŸ‰ ê°€ìƒë°ì´í„° / ì‹¤íŒ¨í•´ë„ ê³„ì†)"
        continue-on-error: true
        run: |
          set +e
          BATCH_ID="BATCH_$(date +%Y%m%d_%H%M%S)"
          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"
          PIPELINE_LOG="${DATA_ROOT}/logs/pipeline_load_merge.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"

          GENERATED_COUNT=$(cat "${DATA_ROOT}/tmp/generated_rowcount.txt" 2>/dev/null || echo "${GEN_ROWS}")

          EXPECTED_IN="${{ github.event.inputs.expected_rows || '' }}"
          if [ -n "$EXPECTED_IN" ]; then
            EXPECTED_LOCAL="$EXPECTED_IN"
          else
            EXPECTED_LOCAL="${EXPECTED_ROWCOUNT}"
          fi

          echo "=== LOAD+MERGE START ==="           | tee "$PIPELINE_LOG"
          echo "BATCH_ID=$BATCH_ID"               | tee -a "$PIPELINE_LOG"
          echo "SRC_FILE=$SRC_FILE"               | tee -a "$PIPELINE_LOG"
          echo "TARGET_TABLE=${TARGET_TABLE}"     | tee -a "$PIPELINE_LOG"
          echo "TBL_STAGE=${TBL_STAGE}"           | tee -a "$PIPELINE_LOG"
          echo "TBL_FINAL=${TBL_FINAL}"           | tee -a "$PIPELINE_LOG"
          echo "TBL_AUDIT=${TBL_AUDIT}"           | tee -a "$PIPELINE_LOG"
          echo
          echo "[1] (ì‹œë®¬) TPT Bulk Load ${GENERATED_COUNT}í–‰ -> ${TBL_STAGE}" | tee -a "$PIPELINE_LOG"
          echo "    FROM ${SRC_FILE}"                                   | tee -a "$PIPELINE_LOG"
          echo "[2] (ì‹œë®¬) CALL LOAD_AND_MERGE('${BATCH_ID}','${SRC_FILE}')" | tee -a "$PIPELINE_LOG"
          echo "[3] (ì‹œë®¬) ê°ì‚¬ë¡œê·¸(${TBL_AUDIT}) insert"               | tee -a "$PIPELINE_LOG"
          echo "=== LOAD+MERGE END ==="                                 | tee -a "$PIPELINE_LOG"

          STATUS_CODE=0
          STATUS_MSG="OK(vdata-bulk-load)"

          # SLA / ì¬ì‹œë„ ì‹œë®¬
          SLA_ON="${{ github.event.inputs.sla_tracking || 'true' }}"
          if [ "$SLA_ON" != "false" ]; then
            echo "SLA: first attempt success" | tee -a "$PIPELINE_LOG"
            echo "SLA_RETRY_COUNT=0" > "${DATA_ROOT}/logs/sla_retry.log"
          else
            echo "SLA tracking disabled" >> "${DATA_ROOT}/logs/sla_retry.log"
          fi

          {
            echo "STATUS_CODE=${STATUS_CODE}"
            echo "STATUS_MSG=${STATUS_MSG}"
            echo "BATCH_ID=${BATCH_ID}"
            echo "SRC_FILE=${SRC_FILE}"
            echo "ROW_EXPECTED=${EXPECTED_LOCAL}"
            echo "ROW_LOADED=${GENERATED_COUNT}"
          } > "$STATUS_FILE"

          echo "::notice::ë°ì´í„° ì ì¬/ë³‘í•©(ëŒ€ìš©ëŸ‰ ê°€ìƒ ë°ì´í„°) ë‹¨ê³„ ì™„ë£Œ. íŒŒì´í”„ë¼ì¸ì€ ê³„ì†ë©ë‹ˆë‹¤."
          set -e

      #######################################################################
      # 5. í’ˆì§ˆ ê²€ì¦ / QC ë¡œê·¸ ìƒì„±
      #######################################################################
      - name: "âœ… í’ˆì§ˆ ê²€ì¦ ë° QC ë¡œê·¸ ìƒì„±"
        run: |
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"

          EXPECTED="$(grep '^ROW_EXPECTED=' "$STATUS_FILE" 2>/dev/null | cut -d= -f2)"
          ACTUAL="$(grep '^ROW_LOADED='   "$STATUS_FILE" 2>/dev/null | cut -d= -f2)"

          [ -z "$EXPECTED" ] && EXPECTED="${EXPECTED_ROWCOUNT}"
          [ -z "$ACTUAL" ] && ACTUAL="0"

          ROW_COUNT_OK="false"
          if [ "$EXPECTED" = "$ACTUAL" ]; then
            ROW_COUNT_OK="true"
          fi

          # ê³ ì•¡ ê±°ë˜ ë¹„ìœ¨
          HIGH_COUNT=$(grep 'FLAG-HIGH' "${DATA_ROOT}/landing_zone"/input_*.csv | wc -l || echo "0")
          if [ "$ACTUAL" -gt 0 ]; then
            HIGH_RATIO=$(echo "$HIGH_COUNT * 100 / $ACTUAL" | bc 2>/dev/null || echo "0")
          else
            HIGH_RATIO="0"
          fi

          # ì „ì¼ ëŒ€ë¹„ ì¦ê°ë¥ 
          LAST_HIST="${DATA_ROOT}/history/last_run_stats.txt"
          PREV_ROWS="N/A"
          GROWTH="N/A"
          if [ -f "$LAST_HIST" ]; then
            PREV_ROWS=$(grep '^ACTUAL_ROWCOUNT=' "$LAST_HIST" | cut -d= -f2)
            if [ -n "$PREV_ROWS" ] && [ "$PREV_ROWS" != "N/A" ] && [ "$PREV_ROWS" -gt 0 ]; then
              GROWTH=$(echo "($ACTUAL-$PREV_ROWS)*100/$PREV_ROWS" | bc 2>/dev/null || echo "N/A")
            fi
          fi

          {
            echo "=== QUALITY CHECK ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "EXPECTED_ROWCOUNT=$EXPECTED"
            echo "ACTUAL_ROWCOUNT=$ACTUAL"
            echo "ROW_COUNT_OK=$ROW_COUNT_OK"
            echo
            echo "--- HIGH AMOUNT (FLAG-HIGH) ---"
            echo "HIGH_COUNT=$HIGH_COUNT"
            echo "HIGH_RATIO_PERCENT=$HIGH_RATIO"
            echo
            echo "--- GROWTH vs PREV RUN ---"
            echo "PREV_ACTUAL_ROWCOUNT=$PREV_ROWS"
            echo "ROW_GROWTH_PERCENT=$GROWTH"
            echo
            echo "--- SIMPLE SANITY RULES ---"
            echo "1) ì ì¬ ê±´ìˆ˜ ê¸°ëŒ€ì¹˜ì™€ ë™ì¼í•œê°€? -> $ROW_COUNT_OK"
            echo "2) ê³ ìœ„í—˜(ì´ˆê³ ì•¡) ê±°ë˜ ë¹„ìœ¨ì€ ê³¼ë„í•˜ì§€ ì•Šì€ê°€? -> threshold check later"
            echo "3) ì „ì¼ ëŒ€ë¹„ ê¸‰ì¦/ê¸‰ê°? -> $GROWTH"
          } > "$QUALITY_LOG"

          # ë‹¤ìŒ ì‹¤í–‰ ë¹„êµìš© history ì €ì¥
          {
            echo "RUN_ID=$GITHUB_RUN_ID"
            echo "ACTUAL_ROWCOUNT=$ACTUAL"
            echo "HIGH_RATIO_PERCENT=$HIGH_RATIO"
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$LAST_HIST"

          echo "::notice::í’ˆì§ˆ ê²€ì¦ ë¡œê·¸ ì‘ì„± ì™„ë£Œ -> $QUALITY_LOG"
          head -n 50 "$QUALITY_LOG" || true

      #######################################################################
      # 5.5 ë¯¼ê° í•„ë“œ ë§ˆìŠ¤í‚¹/í† í°í™” ì •ì±… ê°ì‚¬ ë¡œê·¸
      #######################################################################
      - name: "ğŸ›¡ ë¯¼ê° ë°ì´í„° ë§ˆìŠ¤í‚¹ ì •ì±… ê°ì‚¬ ë¡œê·¸"
        if: ${{ github.event.inputs.include_masking_audit != 'false' }}
        run: |
          MASK_LOG="${DATA_ROOT}/logs/masking_audit.log"
          {
            echo "[MASKING POLICY SIMULATION]"
            echo "- ë¯¼ê° í•„ë“œ ì˜ˆ: CARD_NO, SSN, ACCOUNT_ID ë“±"
            echo "- FINAL_DATA ì—ì„œëŠ” í•´ë‹¹ ë¯¼ê° í•„ë“œëŠ” SHA256 ë˜ëŠ” TOKEN_ID ë¡œë§Œ ì €ì¥"
            echo "- STG_DATA ì›ë³¸í˜•ì‹ì€ 24ì‹œê°„ ë‚´ íŒŒí‹°ì…˜ ì•„ì¹´ì´ë¸Œ í›„ ì ‘ê·¼ì°¨ë‹¨"
            echo "- ACCESS CONTROL: ANALYST_ROLE ì€ ë§ˆìŠ¤í‚¹ëœ ì»¬ëŸ¼ë§Œ SELECT ê°€ëŠ¥"
            echo "- ë³€ê²½ ìŠ¹ì¸ì(DATA_OWNER_X) ìŠ¹ì¸ì¼=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$MASK_LOG"

          echo "::notice::ë¯¼ê° ë°ì´í„° ë§ˆìŠ¤í‚¹ ì •ì±… ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $MASK_LOG"
          head -n 40 "$MASK_LOG" || true

      #######################################################################
      # 6. ê¶Œí•œ/ì ‘ê·¼ì œì–´ ê°ì‚¬ ê¸°ë¡
      #######################################################################
      - name: "ğŸ” ê¶Œí•œ/ì ‘ê·¼ì œì–´ ê°ì‚¬ ê¸°ë¡ ìƒì„±"
        run: |
          ACL_LOG="${DATA_ROOT}/logs/acl_audit.sql"
          {
            echo "-- ê¶Œí•œ ê°ì‚¬ ë¡œê·¸ (ì‹œë®¬ë ˆì´ì…˜)"
            echo "-- FINAL_DATA ì¡°íšŒ/ì“°ê¸° ê¶Œí•œ ì„¤ì • ì˜ˆì‹œ"
            echo "GRANT SELECT ON ${TBL_FINAL} TO ROLE ANALYST_ROLE;"
            echo "GRANT INSERT,UPDATE ON ${TBL_FINAL} TO ROLE ETL_LOADER_ROLE;"
            echo "REVOKE INSERT ON ${TBL_FINAL} FROM ROLE ANALYST_ROLE;"
            echo
            echo "-- AUDIT LOG í…Œì´ë¸” ì ‘ê·¼ í†µì œ"
            echo "GRANT SELECT ON ${TBL_AUDIT} TO ROLE AUDIT_READER_ROLE;"
            echo
            echo "-- Column-level masking / Row-level filtering (ë¬¸ì„œí™”ìš©)"
            echo "-- ANALYST_ROLE ì€ FINAL_DATA.COL2(ë©”ëª¨)ëŠ” ë§ˆìŠ¤í‚¹ ë²„ì „ë§Œ ì¡°íšŒ ê°€ëŠ¥"
            echo "-- BRANCH ì œí•œ: ANALYST_ROLE ì€ ìì‹ ì˜ ì§€ì  ë°ì´í„°ë§Œ ì ‘ê·¼ (row filter)"
            echo
            echo "-- ì‹¤í–‰ ë©”íƒ€"
            echo "-- AUDIT_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "-- EXEC_BY=$GITHUB_ACTOR"
            echo "-- COMMIT_SHA=$GITHUB_SHA"
            echo "-- BRANCH=$GITHUB_REF_NAME"
          } > "$ACL_LOG"

          echo "::debug::ê¶Œí•œ ê°ì‚¬ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $ACL_LOG"
          head -n 60 "$ACL_LOG" || true

      #######################################################################
      # 7. ë¦´ë¦¬ì¦ˆ ìŠ¤ëƒ…ìƒ· & ì••ì¶•ë³¸ ìƒì„± (tar.gz) + ë¬´ê²°ì„± í•´ì‹œ
      #######################################################################
      - name: "ğŸ—œ ê²°ê³¼ë¬¼ tar.gz ìŠ¤ëƒ…ìƒ· ìƒì„±"
        run: |
          SNAP_TAG="td-snapshot-$(date +%Y%m%d-%H%M%S)-${GITHUB_SHA:0:8}"
          SNAP_DIR="${DATA_ROOT}/release"
          SNAP_FILE="${SNAP_DIR}/${SNAP_TAG}.tar.gz"

          echo "SNAP_TAG=$SNAP_TAG"    | tee "${DATA_ROOT}/logs/snapshot_tag.txt"
          echo "SNAP_FILE=$SNAP_FILE"  | tee -a "${DATA_ROOT}/logs/snapshot_tag.txt"

          mkdir -p "$SNAP_DIR"
          tar -czf "$SNAP_FILE" \
            -C "${DATA_ROOT}" \
            logs \
            quality \
            extract_out \
            source_files \
            landing_zone \
            tmp \
            health \
            governance \
            || true

          # ë¬´ê²°ì„± í•´ì‹œ ê¸°ë¡
          sha256sum "$SNAP_FILE" > "${DATA_ROOT}/logs/snapshot_hash.txt" 2>/dev/null || echo "hash_failed" > "${DATA_ROOT}/logs/snapshot_hash.txt"

          echo "::notice::ìŠ¤ëƒ…ìƒ· ì•„ì¹´ì´ë¸Œ ìƒì„± -> $SNAP_FILE"
          ls -lh "$SNAP_FILE" || true
          head -n 5 "${DATA_ROOT}/logs/snapshot_hash.txt" || true

      #######################################################################
      # 7.5 DR(ì¬í•´ë³µêµ¬) ì™¸ë¶€ ì—…ë¡œë“œ ì‹œë®¬ (ì„ íƒ)
      #######################################################################
      - name: "ğŸŒ DR(ì¬í•´ë³µêµ¬) ë°±ì—… ì‹œë„"
        if: ${{ github.event.inputs.dr_backup == 'true' }}
        continue-on-error: true
        env:
          # ì‹¤ì œë¼ë©´ ì—¬ê¸°ì— AWS/GCP/Azure ìê²©ì¦ëª…(or OIDC í™œìš©) ì‚¬ìš©
          DUMMY_KEY: ${{ secrets.DR_BACKUP_KEY }}
        run: |
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
          SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)

          DR_LOG="${DATA_ROOT}/logs/dr_backup_attempt.log"
          {
            echo "[DR BACKUP SIMULATION]"
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE=$SNAP_FILE"
            echo "Attempting offsite backup (S3/Azure/GCS etc.)"
            if [ -n "$DUMMY_KEY" ]; then
              echo "DR_KEY_PRESENT=yes (simulated upload)"
              echo "UPLOAD_STATUS=SUCCESS(simulated)"
            else
              echo "DR_KEY_PRESENT=no"
              echo "UPLOAD_STATUS=FAILED(no credentials)"
            fi
          } > "$DR_LOG"

          echo "::warning::DR ë°±ì—… ì‹œë®¬ ê²°ê³¼ -> $DR_LOG"
          head -n 50 "$DR_LOG" || true

      #######################################################################
      # 8. GitHub Release ì—…ë¡œë“œ ì‹œë„ (ì˜µì…˜)
      #######################################################################
      - name: "ğŸš€ GitHub Release(ìŠ¤ëƒ…ìƒ·) ì—…ë¡œë“œ ì‹œë„"
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          if [ -f "$SNAP_INFO" ]; then
            SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
            SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)
          fi

          if [ -n "${SNAP_TAG:-}" ] && [ -f "${SNAP_FILE:-}" ]; then
            echo "::notice::ë¦´ë¦¬ì¦ˆ íƒœê·¸ $SNAP_TAG, íŒŒì¼ $SNAP_FILE ì—…ë¡œë“œ ì‹œë„ ì¤‘"
            gh release create "$SNAP_TAG" "$SNAP_FILE" \
              --title "$SNAP_TAG" \
              --notes "ìë™ Teradata ë°°ì¹˜ ìŠ¤ëƒ…ìƒ· (ê°€ìƒ ëŒ€ëŸ‰ë°ì´í„°, ì—…ê·¸ë ˆì´ë“œ ë¡œê·¸, í…Œì´ë¸”/ê¶Œí•œ ì •ì˜, í’ˆì§ˆê²€ì¦, ê°ì‚¬ë¡œê·¸ í¬í•¨)" \
              || echo "::warning::gh release create ì‹¤íŒ¨ (ê¶Œí•œ ë¶€ì¡± ë˜ëŠ” íƒœê·¸ ì¤‘ë³µ ê°€ëŠ¥)"
          else
            echo "::warning::ìŠ¤ëƒ…ìƒ· ì •ë³´ê°€ ì—†ì–´ ë¦´ë¦¬ì¦ˆ ìƒëµ"
          fi

      #######################################################################
      # 8.5 ê±°ë²„ë„ŒìŠ¤/ì±…ì„ì¶”ì  ë¬¸ì„œ (ìŠ¹ì¸ì, ì˜¨ì½œ ë‹´ë‹¹ì ë“±)
      #######################################################################
      - name: "ğŸ§‘â€ğŸ’¼ ê±°ë²„ë„ŒìŠ¤ ìŠ¹ì¸ & ë‹´ë‹¹ì ê¸°ë¡"
        run: |
          GOV_LOG="${DATA_ROOT}/governance/governance_approval.log"
          {
            echo "=== GOVERNANCE / APPROVAL LOG ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "DATA_OWNER=DATA_OWNER_X"
            echo "APPROVER=COMPLIANCE_TEAM"
            echo "ONCALL_TEAM=ETL_OnCall"
            echo "ONCALL_CONTACT=etloncall@example.local"
            echo "LAST_SCHEMA_CHANGE=ì»¬ëŸ¼ AMOUNT ì¶”ê°€ / ë¯¼ê°í•„ë“œ ë§ˆìŠ¤í‚¹ ì •ì±… ë°˜ì˜"
            echo "SLA_CONTACT=BatchSRE"
            echo "NOTES=ë°°ì¹˜ ì‹¤íŒ¨ì‹œ ì¦‰ì‹œ BatchSRE í˜¸ì¶œ"
          } > "$GOV_LOG"

          echo "::notice::ê±°ë²„ë„ŒìŠ¤/ìŠ¹ì¸/ì˜¨ì½œ ì •ë³´ ê¸°ë¡ ì™„ë£Œ -> $GOV_LOG"
          head -n 50 "$GOV_LOG" || true

      #######################################################################
      # 9. HTML ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± (ê²½ì˜/ê°ì‚¬ìš© í•œ ì¥ì§œë¦¬)
      #######################################################################
      - name: "ğŸ–¨ HTML ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"
        run: |
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          SNAP_TAG="N/A"
          SNAP_FILE="N/A"

          if [ -f "$SNAP_INFO" ]; then
            SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
            SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)
          fi

          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          ACL_LOG="${DATA_ROOT}/logs/acl_audit.sql"
          HASH_LOG="${DATA_ROOT}/logs/snapshot_hash.txt"

          REPORT_HTML="${DATA_ROOT}/release/report_${GITHUB_RUN_ID}.html"
          {
            echo "<html><body style='font-family:monospace;'>"
            echo "<h1>Teradata FinOps Batch Summary</h1>"
            echo "<p><b>Timestamp:</b> $(date +%Y-%m-%dT%H:%M:%S%z)</p>"
            echo "<p><b>Repo:</b> $GITHUB_REPOSITORY</p>"
            echo "<p><b>Actor:</b> $GITHUB_ACTOR</p>"
            echo "<p><b>Branch:</b> $GITHUB_REF_NAME</p>"
            echo "<p><b>Commit:</b> $GITHUB_SHA</p>"
            echo "<p><b>Batch Target Table:</b> ${TARGET_TABLE}</p>"
            echo "<p><b>Tables Used:</b> ${TBL_STAGE}, ${TBL_FINAL}, ${TBL_AUDIT}</p>"
            echo "<p><b>Snapshot Tag:</b> $SNAP_TAG</p>"
            echo "<p><b>Snapshot File:</b> $SNAP_FILE</p>"
            echo "<hr />"

            echo "<h2>Pipeline Status</h2><pre>"
            cat "$STATUS_FILE" 2>/dev/null || echo "(no pipeline_status.log)"
            echo "</pre>"

            echo "<h2>Quality Check</h2><pre>"
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo "</pre>"

            echo "<h2>System Upgrade (head)</h2><pre>"
            head -n 40 "$UPG_LOG" 2>/dev/null || echo "(no system_upgrade.log)"
            echo "</pre>"

            echo "<h2>ACL / Access Control</h2><pre>"
            head -n 80 "$ACL_LOG" 2>/dev/null || echo "(no acl_audit.sql)"
            echo "</pre>"

            echo "<h2>Snapshot Integrity Hash</h2><pre>"
            cat "$HASH_LOG" 2>/dev/null || echo "(no snapshot_hash.txt)"
            echo "</pre>"

            echo "<p>-- End of Report --</p>"
            echo "</body></html>"
          } > "$REPORT_HTML"

          echo "::notice::HTML ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± -> $REPORT_HTML"
          head -n 40 "$REPORT_HTML" || true

      #######################################################################
      # 9.5 SLA íƒ€ì´ë¨¸ ì¢…ë£Œ ë° ì‹¤í–‰ì‹œê°„ ê¸°ë¡
      #######################################################################
      - name: "â± SLA íƒ€ì´ë¨¸ ì¢…ë£Œ ë° ì‹¤í–‰ì‹œê°„ ê¸°ë¡"
        if: ${{ github.event.inputs.sla_tracking != 'false' }}
        run: |
          END_EPOCH=$(date +%s)
          START_EPOCH=$(cat "${DATA_ROOT}/logs/start_epoch.txt" 2>/dev/null || echo "$END_EPOCH")
          DURATION_SEC=$((END_EPOCH-START_EPOCH))

          {
            echo "SLA_END_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SLA_DURATION_SEC=$DURATION_SEC"
          } >> "${DATA_ROOT}/logs/sla_timing.log"

          echo "::notice::ì‹¤í–‰ ì‹œê°„(SLA_DURATION_SEC=${DURATION_SEC}s) ê¸°ë¡ ì™„ë£Œ"

      #######################################################################
      # 10. ê°ì‚¬ ìš”ì•½ ë¡œê·¸ (audit_run_summary.log)
      #######################################################################
      - name: "ğŸ§¾ ê°ì‚¬ ìš”ì•½ ë¡œê·¸ ìƒì„± (audit_run_summary.log)"
        run: |
          AUDIT_FILE="${DATA_ROOT}/logs/audit_run_summary.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          SLA_FILE="${DATA_ROOT}/logs/sla_timing.log"
          RETENTION_LOG="${DATA_ROOT}/logs/retention_policy.log"
          DIFF_LOG="${DATA_ROOT}/logs/schema_diff.log"
          DR_LOG="${DATA_ROOT}/logs/dr_backup_attempt.log"
          GOV_LOG="${DATA_ROOT}/governance/governance_approval.log"
          HASH_LOG="${DATA_ROOT}/logs/snapshot_hash.txt"

          SNAP_TAG="N/A"
          SNAP_FILE="N/A"
          if [ -f "$SNAP_INFO" ]; then
            SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
            SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)
          fi

          {
            echo "=== PIPELINE AUDIT SUMMARY ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "BRANCH=$GITHUB_REF_NAME"
            echo "COMMIT_SHA=$GITHUB_SHA"
            echo
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "TABLES_USED=${TBL_STAGE},${TBL_FINAL},${TBL_AUDIT}"
            echo "GEN_ROWS=${GEN_ROWS}"
            echo
            echo "--- STATUS_FILE ---"
            cat "$STATUS_FILE" 2>/dev/null || echo "(no pipeline_status.log)"
            echo
            echo "--- QUALITY_LOG ---"
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo
            echo "--- SLA_TIMING ---"
            cat "$SLA_FILE" 2>/dev/null || echo "(no sla_timing.log)"
            echo
            echo "--- SYSTEM UPGRADE LOG (head 40) ---"
            head -n 40 "$UPG_LOG" 2>/dev/null || echo "(no system_upgrade.log)"
            echo
            echo "--- RETENTION POLICY ---"
            cat "$RETENTION_LOG" 2>/dev/null || echo "(no retention_policy.log)"
            echo
            echo "--- SCHEMA DIFF ---"
            head -n 80 "$DIFF_LOG" 2>/dev/null || echo "(no schema_diff.log)"
            echo
            echo "--- DR BACKUP ATTEMPT ---"
            head -n 60 "$DR_LOG" 2>/dev/null || echo "(no dr_backup_attempt.log)"
            echo
            echo "--- GOVERNANCE / APPROVAL ---"
            head -n 60 "$GOV_LOG" 2>/dev/null || echo "(no governance_approval.log)"
            echo
            echo "--- SNAPSHOT INFO ---"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE=$SNAP_FILE"
            echo "DATA_ROOT=$DATA_ROOT"
            echo "--- SNAPSHOT HASH ---"
            cat "$HASH_LOG" 2>/dev/null || echo "(no snapshot_hash.txt)"
          } > "$AUDIT_FILE"

          echo "::notice::ê°ì‚¬ ìš”ì•½ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $AUDIT_FILE"
          head -n 200 "$AUDIT_FILE" || true

      #######################################################################
      # 11. Artifact ì—…ë¡œë“œ
      #######################################################################
      - name: "ğŸ“¦ ì‹¤í–‰ ì‚°ì¶œë¬¼ ì—…ë¡œë“œ (logs / quality / source / snapshot ë“±)"
        uses: actions/upload-artifact@v4
        with:
          name: teradata-run-${{ github.run_id }}
          path: |
            ${{ env.DATA_ROOT }}/logs/**
            ${{ env.DATA_ROOT }}/quality/**
            ${{ env.DATA_ROOT }}/source_files/**
            ${{ env.DATA_ROOT }}/release/**
            ${{ env.DATA_ROOT }}/landing_zone/**
            ${{ env.DATA_ROOT }}/extract_out/**
            ${{ env.DATA_ROOT }}/tmp/**
            ${{ env.DATA_ROOT }}/health/**
            ${{ env.DATA_ROOT }}/governance/**
            ${{ env.DATA_ROOT }}/history/**
          if-no-files-found: warn
          retention-days: 14

      #######################################################################
      # 12. ìµœì¢… ì¢…ë£Œ (í•­ìƒ ì„±ê³µ)
      #######################################################################
      - name: "âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (í•­ìƒ ì„±ê³µ)"
        if: always()
        run: |
          echo "âœ… Teradata FinOps Batch íŒŒì´í”„ë¼ì¸ ì „ì²´ ë‹¨ê³„ ìˆ˜í–‰ ì™„ë£Œ."
          echo "   - ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ & pkg diff ê¸°ë¡"
          echo "   - ëŒ€ëŸ‰ ê°€ìƒë°ì´í„° ìƒì„±(${GEN_ROWS} rows ê¸°ì¤€, dispatch override ë°˜ì˜)"
          echo "   - í’ˆì§ˆê²€ì¦(ì¦ê°ë¥ /ê³ ìœ„í—˜ê±°ë˜ ë¹„ìœ¨), íŒŒí‹°ì…˜/ë³´ì¡´ ì •ì±… ë¡œê·¸"
          echo "   - ë§ˆìŠ¤í‚¹/ê¶Œí•œ/ê±°ë²„ë„ŒìŠ¤/ì˜¨ì½œ/ìŠ¹ì¸ ê¸°ë¡"
          echo "   - ì‹œìŠ¤í…œ í—¬ìŠ¤/ë³´ì•ˆ ì ê²€, SLA ì‹œê°„ ì¸¡ì •"
          echo "   - ìŠ¤ëƒ…ìƒ· tar.gz + ë¬´ê²°ì„± í•´ì‹œ + (ì˜µì…˜) DR ë°±ì—… + GitHub Release ì‹œë„"
          echo "   - HTML ìš”ì•½ ë¦¬í¬íŠ¸ê¹Œì§€ ìƒì„±"
          echo "ëª¨ë“  ì‚°ì¶œë¬¼ì´ artifact teradata-run-${GITHUB_RUN_ID} ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
          exit 0
