name: "ğŸ¦ Teradata FinOps Batch â€” EchoOps + Audit + Snapshot + ISO + DR(1ì¼ë³´ê´€) + DBA + SafeEcho"

on:
  push:
    branches: [ "main" ]
  # ìˆ˜ë™ ì‹¤í–‰ (dispatch inputsëŠ” 10ê°œ ì´í•˜)
  workflow_dispatch:
    inputs:
      mode: # 1
        description: "ì‹¤í–‰ ëª¨ë“œ(full=ì „ì²´ / lite=ì¼ë¶€ ë‹¨ê³„ë§Œ)"
        type: choice
        options: [full, lite]
        default: full
      gen_rows: # 2
        description: "ìƒì„±í•  ê°€ìƒ ë°ì´í„° í–‰ ìˆ˜ (ë¹ˆê°’ì´ë©´ ê¸°ë³¸ GEN_ROWS)"
        required: false
        default: ""
      expected_rows: # 3
        description: "í’ˆì§ˆê²€ì¦ ê¸°ëŒ€ ë¡œìš° ìˆ˜ (ë¹ˆê°’ì´ë©´ ê¸°ë³¸ EXPECTED_ROWCOUNT)"
        required: false
        default: ""
      dr_backup: # 4
        description: "DR(ì™¸ë¶€) ë°±ì—… ì‹œë„ ì—¬ë¶€ (true/false)"
        type: choice
        options: ["true", "false"]
        default: "false"
      include_masking_audit: # 5
        description: "ë¯¼ê°í•„ë“œ ë§ˆìŠ¤í‚¹ ì •ì±… ë¡œê·¸ í¬í•¨ ì—¬ë¶€"
        type: choice
        options: ["true", "false"]
        default: "true"
      include_healthcheck: # 6
        description: "ì‹œìŠ¤í…œ í—¬ìŠ¤/ë³´ì•ˆ ì ê²€ ë¡œê·¸ í¬í•¨ ì—¬ë¶€"
        type: choice
        options: ["true", "false"]
        default: "true"
      include_schema_diff: # 7
        description: "DDL/TPT ë³€ê²½ diff ê¸°ë¡ ì—¬ë¶€"
        type: choice
        options: ["true", "false"]
        default: "true"
      sla_tracking: # 8
        description: "SLA/SLO ì‹œê°„ ì¸¡ì •, ì¬ì‹œë„ ê¸°ë¡ ì—¬ë¶€"
        type: choice
        options: ["true", "false"]
        default: "true"
      echo_trace: # 9
        description: "ëª¨ë“  ì‰˜ ì»¤ë§¨ë“œ ì—ì½”(set -x) í™œì„±í™” ì—¬ë¶€"
        type: choice
        options: ["true", "false"]
        default: "false"

  # ë§¤ì¼ 15:30 UTC == í•œêµ­ì‹œê°„(KST) ìƒˆë²½ 00:30 ìë™ ì•¼ê°„ë°°ì¹˜
  schedule:
    - cron: "30 15 * * *"

permissions:
  contents: write # ë¦´ë¦¬ì¦ˆ íƒœê·¸/ìì‚° ì—…ë¡œë“œìš© (gh release create)

env:
  ###########################################################################
  # ê³µí†µ ê²½ë¡œ / ëŒ€ìƒ í…Œì´ë¸”
  ###########################################################################
  DATA_ROOT: /home/runner/td_data          # EchoOps ê³µí†µ ë¡œê·¸ ë””ë ‰í† ë¦¬ (ë¡œì»¬ ìˆ˜ì§‘ìš©)
  LOG_DIR: .github/echo_logs
  # ì„œë¹„ìŠ¤/ë„ë©”ì¸ë³„ í…Œì´ë¸” (Staging -> Final -> Audit)
  TBL_STAGE: STG_DATA
  TBL_FINAL: FINAL_DATA
  TBL_AUDIT: LOAD_AUDIT_LOG
  # ë³‘í•© íƒ€ê¹ƒ ìš´ì˜ í…Œì´ë¸”
  TARGET_TABLE: FINAL_DATA

  ###########################################################################
  # ê°€ìƒ ë°ì´í„°/í’ˆì§ˆ ê¸°ì¤€
  ###########################################################################
  GEN_ROWS: "500000"
  EXPECTED_ROWCOUNT: "500000"

  ###########################################################################
  # DR / ë³´ê´€ì£¼ê¸° ì •ì±…
  ###########################################################################
  DR_RETENTION_DAYS: "1" # DR ë°ì´í„°ì„¼í„° ë³´ê´€ì£¼ê¸° = 1ì¼
  DR_DIR_NAME: "dr_backup" # DR ìŠ¤ëƒ…ìƒ· ë¡œì»¬ ë³´ê´€ ë””ë ‰í† ë¦¬
  DR_TOPOLOGY_DIR: "governance/dr_site"

jobs:
  teradata_pipeline_job:
    runs-on: ubuntu-latest
    steps:
      #######################################################################
      # 0. ì½”ë“œ ì²´í¬ì•„ì›ƒ
      #######################################################################
      - name: ğŸ“¥ ì½”ë“œ ì²´í¬ì•„ì›ƒ
        uses: actions/checkout@v4

      #######################################################################
      # 0.1 Echo ë¶€íŠ¸ìŠ¤íŠ¸ë© & ë””ë ‰í† ë¦¬ ì¤€ë¹„ + ê³µìš© í—¬í¼ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
      #######################################################################
      - name: ğŸ”§ Echo ë¶€íŠ¸ìŠ¤íŠ¸ë© (/tmp/echo_helpers.sh ìƒì„±)
        run: |
          set -e
          # echo_trace í† ê¸€ ì‹œ ì „ì²´ ì»¤ë§¨ë“œ ì—ì½” + íƒ€ì„ìŠ¤íƒ¬í”„ PS4 ì ìš©
          if [ "${{ github.event.inputs.echo_trace || 'false' }}" = "true" ]; then
            export PS4='+ $(date "+%Y-%m-%dT%H:%M:%S%z") ${BASH_SOURCE##*/}:${LINENO}: '
            set -x
            echo "::notice::echo_trace enabled â†’ set -x"
          fi

          # ê³µí†µ ë””ë ‰í† ë¦¬ ì „ì²˜ë¦¬
          mkdir -p "${DATA_ROOT}"/{logs,quality,release,tmp,landing_zone,governance,history,health,extract_out}
          mkdir -p "${DATA_ROOT}/${DR_DIR_NAME}"
          mkdir -p "${DATA_ROOT}/${DR_TOPOLOGY_DIR}"
          mkdir -p "${LOG_DIR}"

          # EchoOps í—¬í¼ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
          cat > /tmp/echo_helpers.sh <<'EOSH'
          #!/usr/bin/env bash
          set -Eeuo pipefail
          # ê¸°ë³¸ ê²½ë¡œ í™˜ê²½ë³€ìˆ˜ ë³´ì¥
          : "${DATA_ROOT:=/home/runner/td_data}"
          : "${LOG_DIR:=.github/echo_logs}"
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          ts() { date +%Y-%m-%dT%H:%M:%S%z; }
          echo_note() { # echo ë…¸ì´ì¦ˆ í‘œì¤€í™” + ì¤‘ì•™ ë¡œê·¸ ì €ì¥
            local msg="$1"
            printf "â–¶ %s %s\n" "$(ts)" "$msg" | tee -a "${LOG_DIR}/echo.log"
          }
          kv_set() { # key=value í˜•íƒœë¡œ íŒŒì¼ì— ì €ì¥ (ê°±ì‹  ê°€ëŠ¥)
            local file="$1" key="$2" val="$3"
            mkdir -p "$(dirname "$file")"; touch "$file"
            if grep -qE "^${key}=" "$file" 2>/dev/null; then
              sed -i "s|^${key}=.*|${key}=${val}|g" "$file"
            else
              printf "%s=%s\n" "$key" "$val" >> "$file"
            fi
          }
          kv_get() { # fileì—ì„œ key ì¡°íšŒ, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ë¦¬í„´
            local file="$1" key="$2"
            if [ -f "$file" ]; then
              local line
              line="$(grep -E "^${key}=" "$file" 2>/dev/null || true)"
              if [ -n "$line" ]; then echo "${line#*=}"; return 0; fi
            fi
            echo ""; return 0
          }
          snapshot_set() { kv_set "$SNAP_INFO" "$1" "$2"; }
          snapshot_get() { kv_get "$SNAP_INFO" "$1"; }
          trap 'echo_note "trap: last command failed (rc=$?)"' ERR
          EOSH
          chmod +x /tmp/echo_helpers.sh
          echo "::notice::/tmp/echo_helpers.sh ìƒì„± ë° ë””ë ‰í† ë¦¬ ë¶€íŠ¸ìŠ¤íŠ¸ë© ì™„ë£Œ"

      #######################################################################
      # 0.2 ì›Œí¬í”Œë¡œìš° ì‹œì‘ ë¡œê¹… + SLA íƒ€ì´ë¨¸ ì‹œì‘
      #######################################################################
      - name: ğŸ“ ì›Œí¬í”Œë¡œìš° ì‹œì‘ & SLA íƒ€ì´ë¨¸ ì‹œì‘
        run: |
          set -e
          source /tmp/echo_helpers.sh
          echo_note "Teradata í†µí•© ì›Œí¬í”Œë¡œìš° ì‹œì‘"
          echo_note "DATA_ROOT=${DATA_ROOT}, TARGET_TABLE=${TARGET_TABLE}, GEN_ROWS=${GEN_ROWS}"
          echo_note "dispatch.mode=${{ github.event.inputs.mode || 'N/A' }}"
          echo_note "dispatch.dr_backup=${{ github.event.inputs.dr_backup || 'N/A' }}"
          echo_note "dispatch.echo_trace=${{ github.event.inputs.echo_trace || 'false' }}"
          mkdir -p "${DATA_ROOT}/logs"
          date +%s > "${DATA_ROOT}/logs/start_epoch.txt"
          { echo "SLA_START_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"; } > "${DATA_ROOT}/logs/sla_timing.log"

      #######################################################################
      # 0.3 DR ë°ì´í„°ì„¼í„° êµ¬ì¡°ë„/ì •ì±… ë¬¸ì„œ ìƒì„±
      #######################################################################
      - name: ğŸ¢ DR ë°ì´í„°ì„¼í„° êµ¬ì¡°ë„ & ì •ì±… ê¸°ë¡
        run: |
          set -e
          source /tmp/echo_helpers.sh
          TOPO_DIR="${DATA_ROOT}/${DR_TOPOLOGY_DIR}"
          mkdir -p "$TOPO_DIR"
          DR_TOPO_FILE="${TOPO_DIR}/dr_topology.txt"
          {
            echo "=== DR DATACENTER TOPOLOGY ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo
            echo "[Primary Site]"
            echo "- PROD Teradata / FINAL_DATA ìš´ì˜"
            echo "- Batch ETL (STG_DATA -> FINAL_DATA)"
            echo "- Compliance / Risk Analytics"
            echo
            echo "[DR Site]"
            echo "- Warm standby Teradata or compatible warehouse"
            echo "- Daily snapshot import of FINAL_DATA partitions"
            echo "- Read-only dashboards for Risk/Compliance"
            echo
            echo "[Network Zones]"
            echo "- prod-etl-zone (private)"
            echo "- dr-recovery-zone (isolated / limited inbound)"
            echo "- mgmt-zone (jump/bastion for DBA+SRE only)"
            echo
            echo "[Replication / Snapshot Flow]"
            echo "1) Batch completes in Primary."
            echo "2) Snapshot tar.gz & ISO are generated."
            echo "3) Copy snapshot to DR storage bucket / ${DR_DIR_NAME}/YYYYMMDD/ (simulated)."
            echo "4) DR can restore that batch partition on demand."
            echo
            echo "[Ownership / Escalation]"
            echo "- BatchSRE: owns RTO (restore time)."
            echo "- Compliance: data approval."
            echo "- DBA_TEAM: partition restore procedure."
            echo
            echo "[RPO / RTO Targets]"
            echo "- RPO_TARGET=15min"
            echo "- RTO_TARGET=30min"
            echo
            echo "[Retention Policy]"
            echo "- DR snapshot retention: ${DR_RETENTION_DAYS} day(s)"
            echo "- Rotation job auto-removes snapshots older than ${DR_RETENTION_DAYS} day(s)"
            echo
            echo "NOTE=ì´ ë¬¸ì„œëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë˜ë©° ê°ì‚¬ì— í¬í•¨ë©ë‹ˆë‹¤."
          } > "$DR_TOPO_FILE"
          echo_note "DR ë°ì´í„°ì„¼í„° í† í´ë¡œì§€ ê¸°ë¡ ì™„ë£Œ -> $DR_TOPO_FILE"
          ls -R "${DATA_ROOT}/${DR_TOPOLOGY_DIR}" || true

      #######################################################################
      # 0.5 ëŸ¬ë„ˆ ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ (ì‹¤íŒ¨í•´ë„ ê³„ì†) + íŒ¨í‚¤ì§€ ëª©ë¡ diff
      #######################################################################
      - name: ğŸ”„ ëŸ¬ë„ˆ íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ (continue-on-error)
        continue-on-error: true
        run: |
          set +e
          source /tmp/echo_helpers.sh
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          BEFORE_LIST="${DATA_ROOT}/logs/pkg_list_before.txt"
          AFTER_LIST="${DATA_ROOT}/logs/pkg_list_after.txt"
          dpkg -l > "$BEFORE_LIST" 2>/dev/null || true
          {
            echo "===== SYSTEM UPGRADE START ====="
            date
            echo "--- apt-get update ---"
            sudo apt-get update -y || echo "[WARN] apt-get update ì‹¤íŒ¨"
            echo "--- apt-get dist-upgrade ---"
            sudo apt-get -o Dpkg::Options::="--force-confnew" dist-upgrade -y || echo "[WARN] dist-upgrade ì‹¤íŒ¨"
            echo "--- apt-get autoremove ---"
            sudo apt-get autoremove -y || true
            echo "--- uname -a ---"
            uname -a
            echo "--- lsb_release -a ---"
            lsb_release -a 2>/dev/null || echo "lsb_release not available"
            echo "===== SYSTEM UPGRADE END ====="
          } > "$UPG_LOG" 2>&1
          dpkg -l > "$AFTER_LIST" 2>/dev/null || true
          diff -u "$BEFORE_LIST" "$AFTER_LIST" > "${DATA_ROOT}/logs/pkg_upgrade_diff.txt" || true
          echo_note "ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ(íŒ¨í‚¤ì§€ ìµœì‹ í™”) ì‹œë„ ì™„ë£Œ. ìƒì„¸ ${UPG_LOG}"
          set -e

      #######################################################################
      # 1. ë””ë ‰í† ë¦¬ êµ¬ì¡° & ë³´ì¡´ ì •ì±… ë¡œê·¸
      #######################################################################
      - name: ğŸ“‚ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì •ì±… ê¸°ë¡
        run: |
          set -e
          source /tmp/echo_helpers.sh
          echo_note "ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ë° í™•ì¸"
          mkdir -p "${DATA_ROOT}"/{source_files,landing_zone,logs,archive,extract_out,tmp,quality,release,history,governance,health,"${DR_DIR_NAME}"}
          ls -R "${DATA_ROOT}" || true
          cat > "${DATA_ROOT}/logs/retention_policy.log" <<'EOF'
          [RETENTION POLICY SIMULATION]
          - STG_DATA: ì¼ì íŒŒí‹°ì…˜, 30ì¼ ì´ˆê³¼ íŒŒí‹°ì…˜ì€ ì•„ì¹´ì´ë¸Œ í›„ ì‚­ì œ ëŒ€ìƒ
          - FINAL_DATA: ì˜êµ¬ ë³´ì¡´, ë‹¨ ê°œì¸ì •ë³´ í•„ë“œëŠ” ë§ˆìŠ¤í‚¹ ìƒíƒœë§Œ ìœ ì§€
          - LOAD_AUDIT_LOG: 1ë…„ ë³´ì¡´ í›„ ì½œë“œìŠ¤í† ë¦¬ì§€ ì´ì „
          - DR_BACKUP/*: DR_RETENTION_DAYS ì´í›„ ë¡œí…Œì´ì…˜ ì‚­ì œ
          EOF

      #######################################################################
      # 1.5 ê°€ìƒ ë°ì´í„° ëŒ€ëŸ‰ ìƒì„±
      #######################################################################
      - name: ğŸ— ê°€ìƒ ë°ì´í„° ëŒ€ëŸ‰ ìƒì„±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          ROWS_INPUT="${{ github.event.inputs.gen_rows || '' }}"
          if [ -n "$ROWS_INPUT" ]; then
            ROWS="$ROWS_INPUT"
          else
            ROWS="${GEN_ROWS}"
          fi
          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"
          echo "COL1,COL2,AMOUNT,LOAD_TS" > "$SRC_FILE"
          echo_note "ê°€ìƒ ë°ì´í„° ${ROWS}í–‰ ìƒì„± ì‹œì‘ -> $SRC_FILE"
          i=1
          while [ $i -le $ROWS ]; do
            AMT=$(( (RANDOM % 999800) + 100 )) # 100~ì•½100ë§Œ
            MERCH=$(( (RANDOM % 9000) + 1000 ))
            if [ $AMT -gt 900000 ]; then
              NOTE="MERCHANT_${MERCH}_FLAG-HIGH_${i}"
            else
              NOTE="MERCHANT_${MERCH}_NOTE_${i}"
            fi
            echo "${i},${NOTE},${AMT},$(date +%Y-%m-%dT%H:%M:%S%z)" >> "$SRC_FILE"
            i=$((i+1))
          done
          ls -lh "$SRC_FILE"
          head -n 5 "$SRC_FILE"
          tail -n 5 "$SRC_FILE"
          echo "${ROWS}" > "${DATA_ROOT}/tmp/generated_rowcount.txt"
          echo_note "ê°€ìƒ CSV ìƒì„± ì™„ë£Œ (${ROWS} rows)"

      #######################################################################
      # 2. í™˜ê²½ ìŠ¤ëƒ…ìƒ·
      #######################################################################
      - name: ğŸ” ëŸ°íƒ€ì„ í™˜ê²½ ìŠ¤ëƒ…ìƒ· ì €ì¥
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SNAP="${DATA_ROOT}/logs/env_snapshot.txt"
          {
            echo "===== ENV SNAPSHOT ====="
            date
            uname -a
            whoami
            echo "--- PATH ---"
            echo "$PATH"
            echo "--- DISK (df -h) ---"
            df -h
            echo "--- MEMORY (free -m) ---"
            free -m || true
            echo "--- GITHUB CONTEXT ---"
            echo "RUN_ID=$GITHUB_RUN_ID"
            echo "RUN_NUMBER=$GITHUB_RUN_NUMBER"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "SHA=$GITHUB_SHA"
            echo "BRANCH=$GITHUB_REF_NAME"
          } > "$SNAP"
          echo_note "í™˜ê²½ ìŠ¤ëƒ…ìƒ· ê¸°ë¡ ì™„ë£Œ -> $SNAP"

      #######################################################################
      # 2.5 ì‹œìŠ¤í…œ í—¬ìŠ¤ / ë³´ì•ˆ ì ê²€
      #######################################################################
      - name: ğŸ©º ì‹œìŠ¤í…œ í—¬ìŠ¤ ë° ë³´ì•ˆ ì ê²€
        if: ${{ github.event.inputs.include_healthcheck != 'false' }}
        run: |
          set -e
          source /tmp/echo_helpers.sh
          HEALTH_LOG="${DATA_ROOT}/health/system_health.log"
          {
            echo "===== SYSTEM HEALTH CHECK ====="
            date
            echo "--- dmesg (tail 50) ---"
            dmesg | tail -n 50 || true
            echo
            echo "--- TCP/UDP ì†Œì¼“ ìƒíƒœ (ss -tuna head 20) ---"
            ss -tuna | head -n 20 || true
            echo
            echo "--- CPU/MEM load (top -b -n1 head 20) ---"
            top -b -n1 | head -n 20 || true
            echo
            echo "--- I/O stat (iostat if available) ---"
            iostat 2>/dev/null || echo "iostat not available"
            echo
            echo "--- ì™¸ë¶€ ì—°ê²° í…ŒìŠ¤íŠ¸ (curl example.com) ---"
            curl -I https://example.com 2>&1 | head -n 5 || echo "curl external check failed or blocked"
          } > "$HEALTH_LOG"
          echo_note "ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $HEALTH_LOG"

      #######################################################################
      # 3. DDL/TPT/ì¿¼ë¦¬ ì •ì˜ ë°±ì—…
      #######################################################################
      - name: ğŸ§± DDL/TPT/ì¿¼ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ë°±ì—…
        run: |
          set -e
          source /tmp/echo_helpers.sh
          mkdir -p "${DATA_ROOT}/source_files/sql"
          mkdir -p "${DATA_ROOT}/source_files/tpt"
          cat > "${DATA_ROOT}/source_files/sql/create_tables.sql" <<'SQL'
          -- [STAGING TABLE]
          CREATE TABLE STG_DATA (
            COL1 INTEGER,
            COL2 VARCHAR(100),
            AMOUNT BIGINT,
            LOAD_TS TIMESTAMP,
            BATCH_ID VARCHAR(40)
          );
          -- [FINAL TABLE]
          CREATE TABLE FINAL_DATA (
            COL1 INTEGER,
            COL2 VARCHAR(100),
            AMOUNT BIGINT,
            LOAD_TS TIMESTAMP,
            ETL_TS TIMESTAMP,
            SRC_BATCH VARCHAR(40)
          );
          -- [AUDIT TABLE]
          CREATE TABLE LOAD_AUDIT_LOG (
            AUDIT_TS TIMESTAMP,
            BATCH_ID VARCHAR(40),
            SRC_FILE VARCHAR(255),
            ROW_LOADED INTEGER,
            ROW_EXPECTED INTEGER,
            STATUS_CODE INTEGER,
            STATUS_MESSAGE VARCHAR(2000),
            OPERATOR VARCHAR(128)
          );
          CREATE INDEX IDX_FINAL_DATA_COL1 ON FINAL_DATA (COL1);
          CREATE INDEX IDX_AUDIT_BATCH ON LOAD_AUDIT_LOG (BATCH_ID);
          SQL
          cat > "${DATA_ROOT}/source_files/sql/proc_LOAD_AND_MERGE.sql" <<'SQL'
          REPLACE PROCEDURE LOAD_AND_MERGE (
            IN p_batch_id VARCHAR(40),
            IN p_src_file VARCHAR(255)
          )
          BEGIN
            INSERT INTO FINAL_DATA (COL1, COL2, AMOUNT, LOAD_TS, ETL_TS, SRC_BATCH)
            SELECT COL1, TRIM(COL2), AMOUNT, LOAD_TS, CURRENT_TIMESTAMP, p_batch_id
            FROM STG_DATA
            WHERE BATCH_ID = p_batch_id;
            INSERT INTO LOAD_AUDIT_LOG (
              AUDIT_TS, BATCH_ID, SRC_FILE, ROW_LOADED, ROW_EXPECTED,
              STATUS_CODE, STATUS_MESSAGE, OPERATOR
            )
            VALUES (
              CURRENT_TIMESTAMP, p_batch_id, p_src_file,
              NULL, NULL, 0, 'LOAD_AND_MERGE executed', USER
            );
          END;
          SQL
          cat > "${DATA_ROOT}/source_files/tpt/load_stg_data.tpt" <<'TPT'
          DEFINE JOB LOAD_STG_DATA
          (
            DEFINE SCHEMA STG_SCHEMA
            (
              COL1   INTEGER,
              COL2   VARCHAR(100),
              AMOUNT BIGINT,
              LOAD_TS VARCHAR(30)
            );
            DEFINE OPERATOR FILE_READER
            TYPE DATACONNECTOR PRODUCER
            SCHEMA STG_SCHEMA
            ATTRIBUTES
            (
              FileName = '/home/runner/td_data/landing_zone/input_YYYYMMDD.csv',
              Format   = 'Delimited'
            );
            DEFINE OPERATOR TPT_INSERTER
            TYPE STREAM
            TARGET TABLE STG_DATA
            ATTRIBUTES
            (
              TdpId      = 'TERADATA_SID',
              UserName   = 'ETL_USER',
              UserPassword = 'ETL_PASS',
              LogTable   = 'ETL_LOG_TABLE'
            );
            APPLY
            (
              'INSERT INTO STG_DATA (COL1, COL2, AMOUNT, LOAD_TS, BATCH_ID)
               VALUES (:COL1, :COL2, :AMOUNT, TIMESTAMP :LOAD_TS, ''BATCH_PLACEHOLDER'');'
            )
            TO OPERATOR (TPT_INSERTER[1])
            SELECT COL1, COL2, AMOUNT, LOAD_TS
            FROM OPERATOR (FILE_READER[1]);
          );
          TPT
          cat > "${DATA_ROOT}/source_files/sql/quality_queries.sql" <<'SQL'
          SELECT COUNT(*) AS CNT_STG FROM STG_DATA WHERE BATCH_ID = :BATCH_ID;
          SELECT COUNT(*) AS CNT_FINAL FROM FINAL_DATA WHERE SRC_BATCH = :BATCH_ID;
          SELECT
            SUM(CASE WHEN COL2 IS NULL OR TRIM(COL2) = '' THEN 1 ELSE 0 END) AS NULL_MEMO_ROWS,
            SUM(CASE WHEN AMOUNT > 900000 THEN 1 ELSE 0 END) AS HIGH_AMOUNT_ROWS,
            COUNT(*) AS TOTAL_ROWS,
            SUM(AMOUNT) AS SUM_AMOUNT,
            AVG(AMOUNT) AS AVG_AMOUNT
          FROM FINAL_DATA WHERE SRC_BATCH = :BATCH_ID;
          SELECT *
          FROM LOAD_AUDIT_LOG
          QUALIFY ROW_NUMBER() OVER (PARTITION BY BATCH_ID ORDER BY AUDIT_TS DESC) = 1
          ORDER BY AUDIT_TS DESC;
          SQL
          echo_note "DDL/TPT/ì¿¼ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì™„ë£Œ -> ${DATA_ROOT}/source_files"

      #######################################################################
      # 3.5 ìŠ¤í‚¤ë§ˆ ë³€ê²½ diff
      #######################################################################
      - name: ğŸ§¾ ìŠ¤í‚¤ë§ˆ ë³€ê²½ diff ê¸°ë¡
        if: ${{ github.event.inputs.include_schema_diff != 'false' }}
        run: |
          set -e
          source /tmp/echo_helpers.sh
          PREV_SCHEMA="${DATA_ROOT}/history/last_create_tables.sql"
          CURR_SCHEMA="${DATA_ROOT}/source_files/sql/create_tables.sql"
          DIFF_LOG="${DATA_ROOT}/logs/schema_diff.log"
          if [ -f "$PREV_SCHEMA" ]; then
            diff -u "$PREV_SCHEMA" "$CURR_SCHEMA" > "$DIFF_LOG" || true
          else
            echo "[first run or no prev schema]" > "$DIFF_LOG"
          fi
          cp "$CURR_SCHEMA" "$PREV_SCHEMA" 2>/dev/null || cp "$CURR_SCHEMA" "$PREV_SCHEMA"
          echo_note "ìŠ¤í‚¤ë§ˆ diff ê²°ê³¼ -> $DIFF_LOG"
          head -n 200 "$DIFF_LOG" || true

      #######################################################################
      # 4. ë°ì´í„° ì ì¬ & ë³‘í•© ì‹œë®¬ (continue-on-error)
      #######################################################################
      - name: ğŸ”„ ë°ì´í„° ë¡œë“œ ë° ë³‘í•© ì‹œë®¬
        continue-on-error: true
        run: |
          set +e
          source /tmp/echo_helpers.sh
          mkdir -p "${DATA_ROOT}/logs"
          BATCH_ID="BATCH_$(date +%Y%m%d_%H%M%S)"
          echo "$BATCH_ID" > "${DATA_ROOT}/logs/batch_id.txt"
          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"
          PIPELINE_LOG="${DATA_ROOT}/logs/pipeline_load_merge.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          GENERATED_COUNT=$(cat "${DATA_ROOT}/tmp/generated_rowcount.txt" 2>/dev/null || echo "${GEN_ROWS}")
          EXPECTED_IN="${{ github.event.inputs.expected_rows || '' }}"
          if [ -n "$EXPECTED_IN" ]; then
            EXPECTED_LOCAL="$EXPECTED_IN"
          else
            EXPECTED_LOCAL="${EXPECTED_ROWCOUNT}"
          fi
          {
            echo "=== LOAD+MERGE START ==="
            echo "BATCH_ID=$BATCH_ID"
            echo "SRC_FILE=$SRC_FILE"
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "TBL_STAGE=${TBL_STAGE}"
            echo "TBL_FINAL=${TBL_FINAL}"
            echo "TBL_AUDIT=${TBL_AUDIT}"
            echo
            echo "[1] (ì‹œë®¬) TPT Bulk Load ${GENERATED_COUNT}í–‰ -> ${TBL_STAGE}"
            echo "    FROM ${SRC_FILE}"
            echo "[2] (ì‹œë®¬) CALL LOAD_AND_MERGE('${BATCH_ID}','${SRC_FILE}')"
            echo "[3] (ì‹œë®¬) ê°ì‚¬ë¡œê·¸(${TBL_AUDIT}) insert"
            echo "=== LOAD+MERGE END ==="
          } | tee "$PIPELINE_LOG"
          STATUS_CODE=0
          STATUS_MSG="OK(vdata-bulk-load)"
          SLA_ON="${{ github.event.inputs.sla_tracking || 'true' }}"
          if [ "$SLA_ON" != "false" ]; then
            echo "SLA: first attempt success" | tee -a "$PIPELINE_LOG"
            echo "SLA_RETRY_COUNT=0" > "${DATA_ROOT}/logs/sla_retry.log"
          else
            echo "SLA tracking disabled" > "${DATA_ROOT}/logs/sla_retry.log"
          fi
          {
            echo "STATUS_CODE=${STATUS_CODE}"
            echo "STATUS_MSG=${STATUS_MSG}"
            echo "BATCH_ID=${BATCH_ID}"
            echo "SRC_FILE=${SRC_FILE}"
            echo "ROW_EXPECTED=${EXPECTED_LOCAL}"
            echo "ROW_LOADED=${GENERATED_COUNT}"
          } > "$STATUS_FILE"
          echo_note "ë°ì´í„° ì ì¬/ë³‘í•© ì‹œë®¬ ì¢…ë£Œ (ëŒ€ëŸ‰ ê°€ìƒë°ì´í„°). íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰."
          set -e

      #######################################################################
      # 4.3 DBA: Long-Running Query (LRQ) ê°ì§€ ì‹œë®¬
      #######################################################################
      - name: â±ï¸ ì¥ê¸° ì‹¤í–‰ ì¿¼ë¦¬(LRQ) ê°ì§€ ë¡œê·¸ ìƒì„±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          LRQ_LOG="${DATA_ROOT}/logs/dba_lrq_check.log"
          echo_note "DBA: ì¥ê¸° ì‹¤í–‰ ì¿¼ë¦¬(LRQ) ê°ì§€ ì‹œì‘"
          {
            echo "=== LONG-RUNNING QUERY (LRQ) AUDIT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "LRQ_THRESHOLD_SEC=300 (Simulated)"
            echo
            echo "--- Query Log Analysis (Example Rows > 300sec) ---"
            echo "SELECT QueryText, TotalIOCount, AMPCPUTime, StartTime, FirstRespTime"
            echo "FROM DBC.QryLogV"
            echo "WHERE StartTime >= CURRENT_DATE AND (AMPCPUTime/100.0) > 300"
            echo "ORDER BY AMPCPUTime DESC;"
            echo
            echo "[RESULT SIMULATION]"
            echo "Status: OK. No query exceeded 300 seconds."
            echo "Note: Watch MERGE step (CALL LOAD_AND_MERGE) in next batch."
          } > "$LRQ_LOG"
          echo_note "ì¥ê¸° ì‹¤í–‰ ì¿¼ë¦¬(LRQ) ê°ì§€ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $LRQ_LOG"

      #######################################################################
      # 4.5 DBA: íŒŒí‹°ì…˜ / ë½ / í”Œëœ / íŠœë‹
      #######################################################################
      - name: ğŸ§  DBA ì„±ëŠ¥/ë½/íŒŒí‹°ì…˜ ë¶„ì„ ë¡œê·¸ ìƒì„±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          BATCH_ID_VAL="$(cat "${DATA_ROOT}/logs/batch_id.txt" 2>/dev/null || echo 'UNKNOWN_BATCH')"
          PARTITION_LOG="${DATA_ROOT}/logs/partition_access.log"
          {
            echo "=== PARTITION ACCESS REPORT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "STG_DATA partition key: LOAD_TS (daily range)"
            echo "FINAL_DATA partition key: SRC_BATCH (batch_id range/hash)"
            echo "TODAY_ACCESSED_PARTITIONS: LOAD_TS=$(date +%Y-%m-%d) , SRC_BATCH=${BATCH_ID_VAL}"
            echo "NOTE=Only today's partition scanned (simulated)."
          } > "$PARTITION_LOG"
          LOCK_LOG="${DATA_ROOT}/logs/lock_contention.log"
          {
            echo "=== LOCK / CONTENTION REPORT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TABLE=${TBL_FINAL}"
            echo "LOCK_MODE=WriteLockDuringMerge (simulated)"
            echo "CONTENTIONS=0"
            echo "MAX_WAIT_SEC=0"
          } > "$LOCK_LOG"
          PLAN_LOG="${DATA_ROOT}/logs/query_plan_sample.log"
          {
            echo "=== SAMPLE EXPLAIN PLAN (SIMULATED) ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "QUERY=SELECT COUNT(*) FROM ${TBL_FINAL} WHERE SRC_BATCH='${BATCH_ID_VAL}';"
            echo "PLAN=Partitioned access on SRC_BATCH only, no full table scan (simulated)."
          } > "$PLAN_LOG"
          TUNE_LOG="${DATA_ROOT}/logs/tuning_recommendations.log"
          {
            echo "=== TUNING RECOMMENDATIONS ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TABLE=${TBL_FINAL}"
            echo "RECOMMENDATION_1=Add Secondary Index on (SRC_BATCH, LOAD_TS)"
            echo "WHY=Frequent WHERE SRC_BATCH=? AND LOAD_TS>=? in reporting queries"
            echo "ESTIMATED_BENEFIT=Faster dashboards, less full scan"
            echo "ACTION_OWNER=DBA_TEAM"
          } > "$TUNE_LOG"
          echo_note "DBA ì„±ëŠ¥/ë½/íŒŒí‹°ì…˜/íŠœë‹ ë¡œê·¸ ìƒì„± ì™„ë£Œ"

      #######################################################################
      # 5. í’ˆì§ˆ ê²€ì¦
      #######################################################################
      - name: âœ… í’ˆì§ˆ ê²€ì¦ ë° QC ë¡œê·¸ ìƒì„±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          EXPECTED="$(grep '^ROW_EXPECTED=' \"$STATUS_FILE\" 2>/dev/null | cut -d= -f2 || true)"
          ACTUAL="$(grep '^ROW_LOADED=' \"$STATUS_FILE\" 2>/dev/null | cut -d= -f2 || true)"
          [ -z "$EXPECTED" ] && EXPECTED="${EXPECTED_ROWCOUNT}"
          [ -z "$ACTUAL" ] && ACTUAL="0"
          ROW_COUNT_OK="false"
          if [ "$EXPECTED" = "$ACTUAL" ]; then ROW_COUNT_OK="true"; fi
          HIGH_COUNT=$(grep 'FLAG-HIGH' "${DATA_ROOT}/landing_zone"/input_*.csv 2>/dev/null | wc -l || echo "0")
          if [ "$ACTUAL" -gt 0 ]; then
            HIGH_RATIO=$(echo "$HIGH_COUNT * 100 / $ACTUAL" | bc 2>/dev/null || echo "0")
          else
            HIGH_RATIO="0"
          fi
          LAST_HIST="${DATA_ROOT}/history/last_run_stats.txt"
          PREV_ROWS="N/A"; GROWTH="N/A"
          if [ -f "$LAST_HIST" ]; then
            PREV_ROWS=$(grep '^ACTUAL_ROWCOUNT=' "$LAST_HIST" | cut -d= -f2)
            if [ -n "$PREV_ROWS" ] && [ "$PREV_ROWS" != "N/A" ] && [ "$PREV_ROWS" -gt 0 ]; then
              GROWTH=$(echo "($ACTUAL-$PREV_ROWS)*100/$PREV_ROWS" | bc 2>/dev/null || echo "N/A")
            fi
          fi
          {
            echo "=== QUALITY CHECK ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "EXPECTED_ROWCOUNT=$EXPECTED"
            echo "ACTUAL_ROWCOUNT=$ACTUAL"
            echo "ROW_COUNT_OK=$ROW_COUNT_OK"
            echo
            echo "--- HIGH AMOUNT (FLAG-HIGH) ---"
            echo "HIGH_COUNT=$HIGH_COUNT"
            echo "HIGH_RATIO_PERCENT=$HIGH_RATIO"
            echo
            echo "--- GROWTH vs PREV RUN ---"
            echo "PREV_ACTUAL_ROWCOUNT=$PREV_ROWS"
            echo "ROW_GROWTH_PERCENT=$GROWTH"
            echo
            echo "--- SIMPLE SANITY RULES ---"
            echo "1) ê¸°ëŒ€ ë¡œìš° ìˆ˜ ì¼ì¹˜? -> $ROW_COUNT_OK"
            echo "2) ì´ˆê³ ì•¡ ê±°ë˜ ë¹„ìœ¨ ê³¼ë„? -> check HIGH_RATIO_PERCENT"
            echo "3) ì „ì¼ ëŒ€ë¹„ ê¸‰ì¦/ê¸‰ê°? -> $GROWTH"
          } > "$QUALITY_LOG"
          {
            echo "RUN_ID=$GITHUB_RUN_ID"
            echo "ACTUAL_ROWCOUNT=$ACTUAL"
            echo "HIGH_RATIO_PERCENT=$HIGH_RATIO"
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$LAST_HIST"
          echo_note "í’ˆì§ˆ ê²€ì¦ ë¡œê·¸ ì‘ì„± ì™„ë£Œ -> $QUALITY_LOG"
          head -n 50 "$QUALITY_LOG" || true

      #######################################################################
      # 5.3 DBA: ë°ì´í„° ë¶„í¬ ë¶ˆê· í˜•(Skewness) ë¶„ì„
      #######################################################################
      - name: âš–ï¸ ë°ì´í„° ë¶„í¬ ë¶ˆê· í˜•(Skewness) ë¶„ì„
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SKEW_LOG="${DATA_ROOT}/logs/dba_skewness_check.log"
          echo_note "DBA: ë°ì´í„° ë¶„í¬ ë¶ˆê· í˜•(Skewness) ë¶„ì„ ì‹œì‘"
          CURR_ROWS=$(cat "${DATA_ROOT}/tmp/generated_rowcount.txt" 2>/dev/null || echo "0")
          AMPS=6
          AVG_AMP_ROWS=$(( (CURR_ROWS + AMPS - 1) / AMPS ))
          MAX_AMP_ROWS=$(( AVG_AMP_ROWS + (AVG_AMP_ROWS * 20 / 100) ))
          if [ "$AVG_AMP_ROWS" -gt 0 ]; then
            SKEW_PERCENT=$(( (MAX_AMP_ROWS - AVG_AMP_ROWS) * 100 / AVG_AMP_ROWS ))
          else
            SKEW_PERCENT=0
          fi
          {
            echo "=== DATA SKEWNESS AUDIT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "PI_DEFINITION=COL1 (Primary Index)"
            echo "TOTAL_ROWS=$CURR_ROWS"
            echo "SIMULATED_AMPS=$AMPS"
            echo "AVG_ROWS_PER_AMP=$AVG_AMP_ROWS"
            echo "MAX_ROWS_PER_AMP=$MAX_AMP_ROWS (Simulated)"
            echo "SKEW_PERCENTAGE=${SKEW_PERCENT}%"
            echo
            echo "[ANALYSIS]"
            if [ "$SKEW_PERCENT" -gt 30 ]; then
              echo "Status: WARNING. Skewness exceeds 30%."
              echo "Action: Consider PI/Partition review or Columnar/Hash changes."
            else
              echo "Status: OK. Skewness within acceptable range."
            fi
          } > "$SKEW_LOG"
          echo_note "ë°ì´í„° ë¶„í¬ ë¶ˆê· í˜•(Skewness) ë¶„ì„ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $SKEW_LOG"

      #######################################################################
      # 5.4 DBA: ì‹¤í–‰ ê³„íš ë³€ê²½ Diff ê¸°ë¡
      #######################################################################
      - name: ğŸ§¾ ì‹¤í–‰ ê³„íš ë³€ê²½ Diff ê¸°ë¡
        run: |
          set -e
          source /tmp/echo_helpers.sh
          PREV_PLAN="${DATA_ROOT}/history/last_query_plan.txt"
          CURR_PLAN="${DATA_ROOT}/logs/current_query_plan.txt"
          PLAN_DIFF_LOG="${DATA_ROOT}/logs/plan_diff.log"
          echo_note "DBA: ì‹¤í–‰ ê³„íš ë³€ê²½ Diff ì‹œì‘"
          QUERY="SELECT COUNT(*) FROM ${TARGET_TABLE} WHERE SRC_BATCH = 'BATCH_PLACEHOLDER';"
          {
            echo "=== CURRENT QUERY PLAN ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "QUERY: $QUERY"
          } > "$CURR_PLAN"
          # í”Œëœ ë³¸ë¬¸ì€ ë¦¬í„°ëŸ´ heredocìœ¼ë¡œ ì•ˆì „ ê¸°ë¡ (YAML/ì‰˜ íŠ¹ìˆ˜ë¬¸ì ë¬´í•´í™”)
          cat >> "$CURR_PLAN" <<'PLAN_BODY'
1) Lock FINAL_DATA for access
2) Partitioned access on SRC_BATCH only
3) Aggregation (COUNT(*)) on partition
4) End transaction
PLAN_BODY
          if [ -f "$PREV_PLAN" ]; then
            echo_note "ì´ì „ í”Œëœê³¼ í˜„ì¬ í”Œëœ ë¹„êµ ì¤‘..."
            diff -u "$PREV_PLAN" "$CURR_PLAN" > "$PLAN_DIFF_LOG" || true
          else
            echo "[first run or no previous plan]" > "$PLAN_DIFF_LOG"
          fi
          cp "$CURR_PLAN" "$PREV_PLAN" 2>/dev/null || cp "$CURR_PLAN" "$PREV_PLAN"
          echo_note "ì‹¤í–‰ ê³„íš Diff ê²°ê³¼ -> $PLAN_DIFF_LOG"
          head -n 200 "$PLAN_DIFF_LOG" || true

      #######################################################################
      # 5.5 DBA ìœ ì§€ë³´ìˆ˜ / DR ë³µêµ¬ í”Œëœ / ìš©ëŸ‰ / RPO-RTO
      #######################################################################
      - name: ğŸ§® DBA ìœ ì§€ë³´ìˆ˜/DR/ìš©ëŸ‰/RPO-RTO ë¡œê·¸ ìƒì„±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          BATCH_ID_VAL="$(cat "${DATA_ROOT}/logs/batch_id.txt" 2>/dev/null || echo 'UNKNOWN_BATCH')"
          STATS_LOG="${DATA_ROOT}/logs/stats_maintenance.log"
          {
            echo "=== STATS MAINTENANCE REPORT ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "TABLE ${TBL_FINAL}: COLLECT STATS on (COL1, SRC_BATCH, LOAD_TS) (simulated)"
            echo "IMPACT=Optimizer cardinality accuracy â†‘"
            echo "NEXT_REVIEW=+1 day or rowcount_delta > 20%"
          } > "$STATS_LOG"
          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          LAST_CAP="${DATA_ROOT}/history/last_capacity.txt"
          CURR_STG_MB=$(( (RANDOM % 4000) + 1000 ))
          CURR_FINAL_MB=$(( (RANDOM % 9000) + 2000 ))
          PREV_STG_MB="N/A"
          PREV_FINAL_MB="N/A"
          if [ -f "$LAST_CAP" ]; then
            PREV_STG_MB=$(grep '^STG_MB=' "$LAST_CAP" | cut -d= -f2)
            PREV_FINAL_MB=$(grep '^FINAL_MB=' "$LAST_CAP" | cut -d= -f2)
          fi
          {
            echo "=== CAPACITY / GROWTH ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "STG_MB_CURR=$CURR_STG_MB"
            echo "FINAL_MB_CURR=$CURR_FINAL_MB"
            echo "STG_MB_PREV=$PREV_STG_MB"
            echo "FINAL_MB_PREV=$PREV_FINAL_MB"
            echo "NOTE=Track daily growth for capacity planning & cost mgmt."
          } > "$CAP_LOG"
          {
            echo "STG_MB=$CURR_STG_MB"
            echo "FINAL_MB=$CURR_FINAL_MB"
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$LAST_CAP"
          DRPLAY="${DATA_ROOT}/governance/recovery_playbook.txt"
          {
            echo "=== RECOVERY PLAYBOOK ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "BATCH_ID=${BATCH_ID_VAL}"
            echo "1) Pause downstream reporting jobs."
            echo "2) Restore FINAL_DATA partition for BATCH_ID=${BATCH_ID_VAL} from latest DR snapshot (.tar.gz or .iso)."
            echo "3) Validate rowcount vs EXPECTED_ROWCOUNT."
            echo "4) Re-run LOAD_AND_MERGE for failed batch only."
            echo "5) Notify BatchSRE and Compliance if discrepancy > 0.5%."
            echo
            echo "RPO_TARGET=15min"
            echo "RTO_TARGET=30min"
            echo "RPO_ESTIMATE=15min_ok (simulated)"
            echo "RTO_ESTIMATE=25min_ok (simulated)"
            echo
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            echo "POLICY=Snapshots older than ${DR_RETENTION_DAYS} day(s) are purged from ${DR_DIR_NAME}"
          } > "$DRPLAY"
          CLASS_LOG="${DATA_ROOT}/governance/table_classification.log"
          {
            echo "=== TABLE CLASSIFICATION MAP ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "STG_DATA: SENSITIVITY=HIGH_RAW (pre-mask), OWNER=ETL_TEAM, RETENTION=30d"
            echo "FINAL_DATA: SENSITIVITY=MASKED_CONFIDENTIAL, OWNER=RISK_ANALYTICS, RETENTION=indef(masked)"
            echo "LOAD_AUDIT_LOG: SENSITIVITY=OPERATIONS_AUDIT, OWNER=COMPLIANCE_TEAM, RETENTION=365d"
            echo "DR_BACKUP (FINAL_DATA subset): SENSITIVITY=MASKED_CONFIDENTIAL, OWNER=DBA_TEAM"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
          } > "$CLASS_LOG"
          echo_note "DBA ìœ ì§€ë³´ìˆ˜/DR/RPO-RTO/ìš©ëŸ‰/ë¶„ë¥˜ ë¡œê·¸ ìƒì„± ì™„ë£Œ"

      #######################################################################
      # 5.6 ë¯¼ê° ë°ì´í„° ë§ˆìŠ¤í‚¹ ì •ì±… ê°ì‚¬
      #######################################################################
      - name: ğŸ›¡ ë¯¼ê° ë°ì´í„° ë§ˆìŠ¤í‚¹ ì •ì±… ê°ì‚¬ ë¡œê·¸
        if: ${{ github.event.inputs.include_masking_audit != 'false' }}
        run: |
          set -e
          source /tmp/echo_helpers.sh
          MASK_LOG="${DATA_ROOT}/logs/masking_audit.log"
          {
            echo "[MASKING POLICY SIMULATION]"
            echo "- ë¯¼ê° í•„ë“œ ì˜ˆ: CARD_NO, SSN, ACCOUNT_ID ë“±"
            echo "- FINAL_DATA ì—ì„œëŠ” í•´ë‹¹ ë¯¼ê° í•„ë“œëŠ” SHA256 ë˜ëŠ” TOKEN_ID ë¡œë§Œ ì €ì¥"
            echo "- STG_DATA ì›ë³¸í˜•ì‹ì€ 24ì‹œê°„ ë‚´ íŒŒí‹°ì…˜ ì•„ì¹´ì´ë¸Œ í›„ ì ‘ê·¼ì°¨ë‹¨"
            echo "- ACCESS CONTROL: ANALYST_ROLE ì€ ë§ˆìŠ¤í‚¹ëœ ì»¬ëŸ¼ë§Œ SELECT ê°€ëŠ¥"
            echo "- ANALYST_ROLE ì€ ì§€ì ë³„ row-level filterë¡œ ì œí•œ"
            echo "- ë³€ê²½ ìŠ¹ì¸ì(DATA_OWNER_X) ìŠ¹ì¸ì¼=$(date +%Y-%m-%dT%H:%M:%S%z)"
          } > "$MASK_LOG"
          echo_note "ë¯¼ê° ë°ì´í„° ë§ˆìŠ¤í‚¹ ì •ì±… ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $MASK_LOG"
          head -n 40 "$MASK_LOG" || true

      #######################################################################
      # 6. ê¶Œí•œ/ì ‘ê·¼ì œì–´ ê°ì‚¬
      #######################################################################
      - name: ğŸ” ê¶Œí•œ/ì ‘ê·¼ì œì–´ ê°ì‚¬ ê¸°ë¡ ìƒì„±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          ACL_LOG="${DATA_ROOT}/logs/acl_audit.sql"
          {
            echo "-- ê¶Œí•œ ê°ì‚¬ ë¡œê·¸ (ì‹œë®¬ë ˆì´ì…˜)"
            echo "-- FINAL_DATA ì¡°íšŒ/ì“°ê¸° ê¶Œí•œ ì„¤ì • ì˜ˆì‹œ"
            echo "GRANT SELECT ON ${TBL_FINAL} TO ROLE ANALYST_ROLE;"
            echo "GRANT INSERT,UPDATE ON ${TBL_FINAL} TO ROLE ETL_LOADER_ROLE;"
            echo "REVOKE INSERT ON ${TBL_FINAL} FROM ROLE ANALYST_ROLE;"
            echo
            echo "-- AUDIT LOG í…Œì´ë¸” ì ‘ê·¼ í†µì œ"
            echo "GRANT SELECT ON ${TBL_AUDIT} TO ROLE AUDIT_READER_ROLE;"
            echo
            echo "-- Column-level masking / Row-level filtering (ë¬¸ì„œí™”ìš©)"
            echo "-- ANALYST_ROLE ì€ FINAL_DATA.COL2(ë©”ëª¨)ëŠ” ë§ˆìŠ¤í‚¹ ë²„ì „ë§Œ ì¡°íšŒ ê°€ëŠ¥"
            echo "-- ANALYST_ROLE ì€ ìì‹ ì˜ ì§€ì  ë°ì´í„°ë§Œ ì ‘ê·¼ (row filter)"
            echo
            echo "-- ì‹¤í–‰ ë©”íƒ€"
            echo "-- AUDIT_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "-- EXEC_BY=$GITHUB_ACTOR"
            echo "-- COMMIT_SHA=$GITHUB_SHA"
            echo "-- BRANCH=$GITHUB_REF_NAME"
          } > "$ACL_LOG"
          echo_note "ê¶Œí•œ ê°ì‚¬ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $ACL_LOG"
          head -n 60 "$ACL_LOG" || true

      #######################################################################
      # 7. tar.gz ìŠ¤ëƒ…ìƒ· ìƒì„± + snapshot_set ê¸°ë¡
      #######################################################################
      - name: ğŸ—œ ê²°ê³¼ë¬¼ tar.gz ìŠ¤ëƒ…ìƒ· ìƒì„±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SNAP_TAG="td-snapshot-$(date +%Y%m%d-%H%M%S)-${GITHUB_SHA:0:8}"
          SNAP_DIR="${DATA_ROOT}/release"
          SNAP_FILE="${SNAP_DIR}/${SNAP_TAG}.tar.gz"
          mkdir -p "$SNAP_DIR"
          # ìŠ¤ëƒ…ìƒ· ëŒ€ìƒë“¤ ì••ì¶•
          tar -czf "$SNAP_FILE" \
            -C "${DATA_ROOT}" \
            logs \
            quality \
            extract_out \
            source_files \
            landing_zone \
            tmp \
            health \
            governance \
            history \
            || true
          # ìŠ¤ëƒ…ìƒ· ë©”íƒ€ ì €ì¥ (ì¤‘ì•™í™”!)
          snapshot_set SNAP_TAG "$SNAP_TAG"
          snapshot_set SNAP_FILE "$SNAP_FILE"
          sha256sum "$SNAP_FILE" > "${DATA_ROOT}/logs/snapshot_hash.txt" 2>/dev/null || echo "hash_failed" > "${DATA_ROOT}/logs/snapshot_hash.txt"
          echo_note "tar.gz ìŠ¤ëƒ…ìƒ· ìƒì„± -> $SNAP_FILE"
          ls -lh "$SNAP_FILE" || true
          head -n 5 "${DATA_ROOT}/logs/snapshot_hash.txt" || true

      #######################################################################
      # 7.2 ISO ìŠ¤ëƒ…ìƒ· ì´ë¯¸ì§€ ìƒì„± (continue-on-error)
      #######################################################################
      - name: ğŸ§Š ISO ìŠ¤ëƒ…ìƒ· ì´ë¯¸ì§€ ìƒì„±
        continue-on-error: true
        run: |
          set +e
          source /tmp/echo_helpers.sh
          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"
          ISO_BUILD_LOG="${DATA_ROOT}/logs/iso_build.log"
          echo "ISO_BUILD_TS=$(date +%Y-%m-%dT%H:%M:%S%z)" > "$ISO_BUILD_LOG"
          echo "SNAP_TAG=$SNAP_TAG" >> "$ISO_BUILD_LOG"
          echo "SNAP_FILE=$SNAP_FILE" >> "$ISO_BUILD_LOG"
          echo "ISO_FILE=$ISO_FILE" >> "$ISO_BUILD_LOG"
          if [ -z "$SNAP_TAG" ] || [ -z "$SNAP_FILE" ]; then
            echo_note "SNAP_INFO ì—†ìŒ â†’ ISO ìƒì„± ìŠ¤í‚µ"
            exit 0
          fi
          ISO_SRC_DIR="${DATA_ROOT}/tmp/iso_src_${SNAP_TAG}"
          mkdir -p "$ISO_SRC_DIR"
          cat > "${ISO_SRC_DIR}/audit_summary_seed.txt" <<EOF
          Snapshot Tag: $SNAP_TAG
          Generated: $(date +%Y-%m-%dT%H:%M:%S%z)
          Contains: logs, quality, governance, health, DR topology, classification, recovery playbook, etc.
          RPO_TARGET=15min
          RTO_TARGET=30min
          DR_RETENTION_DAYS=${DR_RETENTION_DAYS}
          EOF
          cp -r "${DATA_ROOT}/logs" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/quality" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/governance" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/health" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/source_files" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/landing_zone" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/history" "${ISO_SRC_DIR}/" 2>>"$ISO_BUILD_LOG" || true
          cp -r "${DATA_ROOT}/${DR_TOPOLOGY_DIR}" "${ISO_SRC_DIR}/dr_topology" 2>>"$ISO_BUILD_LOG" || true
          echo "[TRY] installing genisoimage/xorriso" >> "$ISO_BUILD_LOG"
          sudo apt-get update -y >> "$ISO_BUILD_LOG" 2>&1 || true
          sudo apt-get install -y genisoimage xorriso >> "$ISO_BUILD_LOG" 2>&1 || true
          if command -v genisoimage >/dev/null 2>&1 ; then
            echo "[USE] genisoimage" >> "$ISO_BUILD_LOG"
            genisoimage -quiet -J -r -o "$ISO_FILE" "$ISO_SRC_DIR" >> "$ISO_BUILD_LOG" 2>&1 || { echo "[WARN] genisoimage failed" >> "$ISO_BUILD_LOG"; }
          fi
          if [ ! -s "$ISO_FILE" ] && command -v xorriso >/dev/null 2>&1 ; then
            echo "[USE] xorriso -as mkisofs" >> "$ISO_BUILD_LOG"
            xorriso -as mkisofs -J -r -o "$ISO_FILE" "$ISO_SRC_DIR" >> "$ISO_BUILD_LOG" 2>&1 || { echo "[WARN] xorriso mkisofs failed" >> "$ISO_BUILD_LOG"; }
          fi
          if [ -s "$ISO_FILE" ]; then
            sha256sum "$ISO_FILE" > "${DATA_ROOT}/logs/iso_hash.txt" 2>/dev/null || echo "hash_failed" > "${DATA_ROOT}/logs/iso_hash.txt"
            echo_note "ISO ìŠ¤ëƒ…ìƒ· ìƒì„± ì™„ë£Œ -> $ISO_FILE"
            ls -lh "$ISO_FILE" || true
          else
            echo_note "ISO ìŠ¤ëƒ…ìƒ· ìƒì„± ì‹¤íŒ¨(í™˜ê²½ ì œì•½). íŒŒì´í”„ë¼ì¸ ê³„ì†."
            echo "ISO_CREATION_FAILED=1" >> "$ISO_BUILD_LOG"
          fi
          set -e

      #######################################################################
      # 7.5 DR(ì¬í•´ë³µêµ¬) ë°±ì—… ë° 1ì¼ ë³´ê´€ ë¡œí…Œì´ì…˜
      #######################################################################
      - name: ğŸŒ DR(ì¬í•´ë³µêµ¬) ë°±ì—… ë° 1ì¼ ë³´ê´€ ë¡œí…Œì´ì…˜
        continue-on-error: true
        env:
          DUMMY_KEY: ${{ secrets.DR_BACKUP_KEY }}
        run: |
          set +e
          source /tmp/echo_helpers.sh
          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"
          ISO_HASH_FILE="${DATA_ROOT}/logs/iso_hash.txt"
          if [ -z "$SNAP_TAG" ] || [ -z "$SNAP_FILE" ]; then
            echo_note "SNAP_INFO ì—†ìŒ â†’ DR ë°±ì—…/ë¡œí…Œì´ì…˜ ìŠ¤í‚µ"
            exit 0
          fi
          mkdir -p "${DATA_ROOT}/${DR_DIR_NAME}"
          DR_DATE_DIR="${DATA_ROOT}/${DR_DIR_NAME}/$(date +%Y-%m-%d)"
          mkdir -p "$DR_DATE_DIR"
          DR_COPY_TAR="${DR_DATE_DIR}/${SNAP_TAG}.tar.gz"
          DR_COPY_ISO="${DR_DATE_DIR}/${SNAP_TAG}.iso"
          DR_META_PATH="${DR_DATE_DIR}/${SNAP_TAG}.meta.txt"
          cp -f "$SNAP_FILE" "$DR_COPY_TAR" 2>/dev/null || true
          if [ -s "$ISO_FILE" ]; then cp -f "$ISO_FILE" "$DR_COPY_ISO" 2>/dev/null || true; fi
          {
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE_LOCAL=$SNAP_FILE"
            echo "DR_STORED_TAR=$DR_COPY_TAR"
            if [ -s "$ISO_FILE" ]; then echo "DR_STORED_ISO=$DR_COPY_ISO"; else echo "DR_STORED_ISO=(none)"; fi
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            if [ -n "$DUMMY_KEY" ]; then
              echo "DR_KEY_PRESENT=yes"
              echo "EXTERNAL_UPLOAD_STATUS=SUCCESS(simulated)"
            else
              echo "DR_KEY_PRESENT=no"
              echo "EXTERNAL_UPLOAD_STATUS=FAILED(no credentials)"
            fi
            echo "NOTE=ë¡œì»¬ DR ì˜ì—­ì— tar.gz/iso ì €ì¥ í›„, ë³´ê´€ì£¼ê¸° ì§€ë‚œ ìŠ¤ëƒ…ìƒ·ì€ ì‚­ì œ"
            if [ -f "$ISO_HASH_FILE" ]; then
              echo "--- ISO HASH ---"
              cat "$ISO_HASH_FILE"
            fi
          } > "$DR_META_PATH"
          ROTATE_LOG="${DATA_ROOT}/logs/dr_rotation.log"
          {
            echo "=== DR ROTATION START ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "POLICY=keep ${DR_RETENTION_DAYS} day(s)"
            echo "ACTION=find ${DATA_ROOT}/${DR_DIR_NAME} -type f -mtime +${DR_RETENTION_DAYS} -delete"
            find "${DATA_ROOT}/${DR_DIR_NAME}" -type f -mtime +${DR_RETENTION_DAYS} -print
            find "${DATA_ROOT}/${DR_DIR_NAME}" -type f -mtime +${DR_RETENTION_DAYS} -delete || true
            echo "=== DR ROTATION END ==="
          } > "$ROTATE_LOG"
          DR_LOG="${DATA_ROOT}/logs/dr_backup_attempt.log"
          {
            echo "[DR BACKUP SIMULATION]"
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE=$SNAP_FILE"
            echo "DR_LOCAL_TAR=$DR_COPY_TAR"
            echo "DR_LOCAL_ISO=$DR_COPY_ISO"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            echo "RPO_TARGET=15min"
            echo "RTO_TARGET=30min"
            if [ -n "$DUMMY_KEY" ]; then
              echo "DR_KEY_PRESENT=yes (simulated external upload ok)"
            else
              echo "DR_KEY_PRESENT=no (external upload skipped/fail)"
            fi
          } > "$DR_LOG"
          echo_note "DR ë°±ì—…/ë¡œí…Œì´ì…˜ ì²˜ë¦¬ ì™„ë£Œ"
          head -n 80 "$DR_LOG" || true
          head -n 80 "$ROTATE_LOG" || true
          set -e

      #######################################################################
      # 8. GitHub Release ì—…ë¡œë“œ ì‹œë„ (ì˜µì…˜)
      #######################################################################
      - name: ğŸš€ GitHub Release(ìŠ¤ëƒ…ìƒ·) ì—…ë¡œë“œ ì‹œë„
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set +e
          source /tmp/echo_helpers.sh
          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"
          if [ -z "$SNAP_TAG" ] || [ -z "$SNAP_FILE" ]; then
            echo_note "SNAP_INFO ì—†ìŒ â†’ GitHub Release ì—…ë¡œë“œ ìŠ¤í‚µ"
            exit 0
          fi
          echo_note "ë¦´ë¦¬ì¦ˆ íƒœê·¸ ${SNAP_TAG} ì—…ë¡œë“œ ì‹œë„"
          if [ -s "$ISO_FILE" ]; then
            gh release create "$SNAP_TAG" "$SNAP_FILE" "$ISO_FILE" \
              --title "$SNAP_TAG" \
              --notes "ìë™ Teradata ë°°ì¹˜ ìŠ¤ëƒ…ìƒ· (tar.gz + iso). í¬í•¨ë‚´ìš©: ëŒ€ëŸ‰ë°ì´í„°, DBAë¡œê·¸, DR í† í´ë¡œì§€, RPO/RTO, ë§ˆìŠ¤í‚¹/ê¶Œí•œ ê°ì‚¬, ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ ë¡œê·¸ ë“±." \
              || echo "::warning::gh release create ì‹¤íŒ¨ (ê¶Œí•œ ë¶€ì¡± ë˜ëŠ” íƒœê·¸ ì¤‘ë³µ ê°€ëŠ¥)"
          else
            gh release create "$SNAP_TAG" "$SNAP_FILE" \
              --title "$SNAP_TAG" \
              --notes "ìë™ Teradata ë°°ì¹˜ ìŠ¤ëƒ…ìƒ· (tar.gz). í¬í•¨ë‚´ìš©: ëŒ€ëŸ‰ë°ì´í„°, DBAë¡œê·¸, DR í† í´ë¡œì§€, RPO/RTO, ë§ˆìŠ¤í‚¹/ê¶Œí•œ ê°ì‚¬, ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ ë¡œê·¸ ë“±." \
              || echo "::warning::gh release create ì‹¤íŒ¨ (ê¶Œí•œ ë¶€ì¡± ë˜ëŠ” íƒœê·¸ ì¤‘ë³µ ê°€ëŠ¥)"
          fi
          set -e

      #######################################################################
      # 8.5 ê±°ë²„ë„ŒìŠ¤/ìŠ¹ì¸/ì˜¨ì½œ ê¸°ë¡
      #######################################################################
      - name: ğŸ§‘â€ğŸ’¼ ê±°ë²„ë„ŒìŠ¤ ìŠ¹ì¸ & ë‹´ë‹¹ì ê¸°ë¡
        run: |
          set -e
          source /tmp/echo_helpers.sh
          GOV_LOG="${DATA_ROOT}/governance/governance_approval.log"
          {
            echo "=== GOVERNANCE / APPROVAL LOG ==="
            echo "TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "DATA_OWNER=DATA_OWNER_X"
            echo "APPROVER=COMPLIANCE_TEAM"
            echo "ONCALL_TEAM=ETL_OnCall"
            echo "ONCALL_CONTACT=etloncall@example.local"
            echo "LAST_SCHEMA_CHANGE=ì»¬ëŸ¼ AMOUNT ì¶”ê°€ / ë¯¼ê°í•„ë“œ ë§ˆìŠ¤í‚¹ ì •ì±… ë°˜ì˜"
            echo "SLA_CONTACT=BatchSRE"
            echo "BUSINESS_SLA=KST 06:00 ë¦¬í¬íŠ¸ ë§ˆê°ê¹Œì§€ ì™„ë£Œ"
            echo "BUSINESS_IMPACT=ì§€ì—° ì‹œ ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ ë³´ê³  ì§€ì—°"
            echo "DR_POLICY=DR snapshot retention ${DR_RETENTION_DAYS} day(s) under ${DR_DIR_NAME}/"
            echo "DR_TOPOLOGY_DOC=${DR_TOPOLOGY_DIR}/dr_topology.txt"
          } > "$GOV_LOG"
          echo_note "ê±°ë²„ë„ŒìŠ¤/ìŠ¹ì¸/ì˜¨ì½œ ì •ë³´ ê¸°ë¡ ì™„ë£Œ -> $GOV_LOG"
          head -n 50 "$GOV_LOG" || true

      #######################################################################
      # 9. HTML ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± (DBA ê³ ê¸‰ í•­ëª© í¬í•¨)
      #######################################################################
      - name: ğŸ–¨ HTML ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"
          : "${SNAP_TAG:=N/A}"
          : "${SNAP_FILE:=N/A}"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          ACL_LOG="${DATA_ROOT}/logs/acl_audit.sql"
          HASH_LOG="${DATA_ROOT}/logs/snapshot_hash.txt"
          ISO_HASH_FILE="${DATA_ROOT}/logs/iso_hash.txt"
          PARTITION_LOG="${DATA_ROOT}/logs/partition_access.log"
          LOCK_LOG="${DATA_ROOT}/logs/lock_contention.log"
          PLAN_LOG="${DATA_ROOT}/logs/query_plan_sample.log"
          TUNE_LOG="${DATA_ROOT}/logs/tuning_recommendations.log"
          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          STATS_LOG="${DATA_ROOT}/logs/stats_maintenance.log"
          CLASS_LOG="${DATA_ROOT}/governance/table_classification.log"
          DRPLAY="${DATA_ROOT}/governance/recovery_playbook.txt"
          DR_TOPO_FILE="${DATA_ROOT}/${DR_TOPOLOGY_DIR}/dr_topology.txt"
          ROTATE_LOG="${DATA_ROOT}/logs/dr_rotation.log"
          # ì¶”ê°€ëœ DBA ê³ ê¸‰ ë¡œê·¸
          LRQ_LOG="${DATA_ROOT}/logs/dba_lrq_check.log"
          SKEW_LOG="${DATA_ROOT}/logs/dba_skewness_check.log"
          PLAN_DIFF_LOG="${DATA_ROOT}/logs/plan_diff.log"
          REPORT_HTML="${DATA_ROOT}/release/report_${GITHUB_RUN_ID}.html"
          {
            echo "<html><body style='font-family:monospace;'>"
            echo "<h1>Teradata FinOps Batch Summary</h1>"
            echo "<p><b>Timestamp:</b> $(date +%Y-%m-%dT%H:%M:%S%z)</p>"
            echo "<p><b>Repo:</b> $GITHUB_REPOSITORY</p>"
            echo "<p><b>Actor:</b> $GITHUB_ACTOR</p>"
            echo "<p><b>Branch:</b> $GITHUB_REF_NAME</p>"
            echo "<p><b>Commit:</b> $GITHUB_SHA</p>"
            echo "<p><b>Batch Target Table:</b> ${TARGET_TABLE}</p>"
            echo "<p><b>Tables Used:</b> ${TBL_STAGE}, ${TBL_FINAL}, ${TBL_AUDIT}</p>"
            echo "<p><b>Snapshot Tag:</b> $SNAP_TAG</p>"
            echo "<p><b>Snapshot File (tar.gz):</b> $SNAP_FILE</p>"
            echo "<p><b>Snapshot ISO:</b> $ISO_FILE</p>"
            echo "<p><b>DR Retention Policy:</b> ${DR_RETENTION_DAYS} day(s) in ${DR_DIR_NAME}/ (auto-rotation)</p>"
            echo "<hr /><h2>Pipeline Status</h2><pre>"
            cat "$STATUS_FILE" 2>/dev/null || echo "(no pipeline_status.log)"
            echo "</pre><h2>Quality Check</h2><pre>"
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo "</pre><h2>System Upgrade (head)</h2><pre>"
            head -n 40 "$UPG_LOG" 2>/dev/null || echo "(no system_upgrade.log)"
            echo "</pre><h2>ACL / Access Control</h2><pre>"
            head -n 80 "$ACL_LOG" 2>/dev/null || echo "(no acl_audit.sql)"
            echo "</pre><h2>DBA Performance & Capacity</h2><pre>"
            echo "--- Long-Running Query Check (LRQ) ---"
            head -n 80 "$LRQ_LOG" 2>/dev/null || echo "(no dba_lrq_check.log)"
            echo
            echo "--- Skewness / Distribution Check ---"
            head -n 80 "$SKEW_LOG" 2>/dev/null || echo "(no dba_skewness_check.log)"
            echo
            echo "--- Query Plan Diff ---"
            head -n 80 "$PLAN_DIFF_LOG" 2>/dev/null || echo "(no plan_diff.log)"
            echo
            echo "--- Partition Access ---"
            head -n 80 "$PARTITION_LOG" 2>/dev/null || echo "(no partition_access.log)"
            echo
            echo "--- Lock Contention ---"
            head -n 80 "$LOCK_LOG" 2>/dev/null || echo "(no lock_contention.log)"
            echo
            echo "--- Query Plan Sample ---"
            head -n 80 "$PLAN_LOG" 2>/dev/null || echo "(no query_plan_sample.log)"
            echo
            echo "--- Tuning Recommendations ---"
            head -n 80 "$TUNE_LOG" 2>/dev/null || echo "(no tuning_recommendations.log)"
            echo
            echo "--- Stats Maintenance ---"
            head -n 80 "$STATS_LOG" 2>/dev/null || echo "(no stats_maintenance.log)"
            echo
            echo "--- Capacity Growth ---"
            head -n 80 "$CAP_LOG" 2>/dev/null || echo "(no capacity_growth.log)"
            echo "</pre><h2>Classification / DR / Snapshot Integrity</h2><pre>"
            echo "--- Table Classification ---"
            head -n 80 "$CLASS_LOG" 2>/dev/null || echo "(no table_classification.log)"
            echo
            echo "--- Recovery Playbook (RPO/RTO) ---"
            head -n 80 "$DRPLAY" 2>/dev/null || echo "(no recovery_playbook.txt)"
            echo
            echo "--- DR Topology ---"
            head -n 80 "$DR_TOPO_FILE" 2>/dev/null || echo "(no dr_topology.txt)"
            echo
            echo "--- DR Rotation ---"
            head -n 80 "$ROTATE_LOG" 2>/dev/null || echo "(no dr_rotation.log)"
            echo
            echo "--- Snapshot Integrity Hash (tar.gz) ---"
            cat "$HASH_LOG" 2>/dev/null || echo "(no snapshot_hash.txt)"
            echo
            echo "--- Snapshot ISO Hash ---"
            cat "$ISO_HASH_FILE" 2>/dev/null || echo "(no iso_hash.txt)"
            echo "</pre>"
            echo "<p>-- End of Report --</p>"
            echo "</body></html>"
          } > "$REPORT_HTML"
          echo_note "HTML ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± -> $REPORT_HTML"
          head -n 40 "$REPORT_HTML" || true

      #######################################################################
      # 9. SLA íƒ€ì´ë¨¸ ì¢…ë£Œ (ì˜µì…˜)
      #######################################################################
      - name: â± SLA íƒ€ì´ë¨¸ ì¢…ë£Œ ë° ì‹¤í–‰ì‹œê°„ ê¸°ë¡
        if: ${{ github.event.inputs.sla_tracking != 'false' }}
        run: |
          set -e
          source /tmp/echo_helpers.sh
          END_EPOCH=$(date +%s)
          START_EPOCH=$(cat "${DATA_ROOT}/logs/start_epoch.txt" 2>/dev/null || echo "$END_EPOCH")
          DURATION_SEC=$((END_EPOCH-START_EPOCH))
          {
            echo "SLA_END_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "SLA_DURATION_SEC=$DURATION_SEC"
            echo "BUSINESS_SLA=KST 06:00 ë¦¬í¬íŠ¸ ë§ˆê°ê¹Œì§€ ì™„ë£Œ"
            echo "BUSINESS_IMPACT=ì§€ì—° ì‹œ ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ ë³´ê³  ì§€ì—°"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
          } >> "${DATA_ROOT}/logs/sla_timing.log"
          echo_note "SLA_DURATION_SEC=${DURATION_SEC}s ê¸°ë¡ ì™„ë£Œ"

      #######################################################################
      # 10. ê°ì‚¬ ìš”ì•½ ë¡œê·¸ (audit_run_summary.log)
      #######################################################################
      - name: ğŸ§¾ ê°ì‚¬ ìš”ì•½ ë¡œê·¸ ìƒì„± (audit_run_summary.log)
        run: |
          set -e
          source /tmp/echo_helpers.sh
          SNAP_TAG="$(snapshot_get SNAP_TAG)"
          SNAP_FILE="$(snapshot_get SNAP_FILE)"
          SNAP_DIR="${DATA_ROOT}/release"
          ISO_FILE="${SNAP_DIR}/${SNAP_TAG}.iso"
          : "${SNAP_TAG:=N/A}"
          : "${SNAP_FILE:=N/A}"
          AUDIT_FILE="${DATA_ROOT}/logs/audit_run_summary.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"
          SLA_FILE="${DATA_ROOT}/logs/sla_timing.log"
          RETENTION_LOG="${DATA_ROOT}/logs/retention_policy.log"
          DIFF_LOG="${DATA_ROOT}/logs/schema_diff.log"
          DR_LOG="${DATA_ROOT}/logs/dr_backup_attempt.log"
          GOV_LOG="${DATA_ROOT}/governance/governance_approval.log"
          HASH_LOG="${DATA_ROOT}/logs/snapshot_hash.txt"
          ISO_HASH_FILE="${DATA_ROOT}/logs/iso_hash.txt"
          PARTITION_LOG="${DATA_ROOT}/logs/partition_access.log"
          LOCK_LOG="${DATA_ROOT}/logs/lock_contention.log"
          PLAN_LOG="${DATA_ROOT}/logs/query_plan_sample.log"
          TUNE_LOG="${DATA_ROOT}/logs/tuning_recommendations.log"
          STATS_LOG="${DATA_ROOT}/logs/stats_maintenance.log"
          CAP_LOG="${DATA_ROOT}/logs/capacity_growth.log"
          CLASS_LOG="${DATA_ROOT}/governance/table_classification.log"
          DRPLAY="${DATA_ROOT}/governance/recovery_playbook.txt"
          DR_TOPO_FILE="${DATA_ROOT}/${DR_TOPOLOGY_DIR}/dr_topology.txt"
          ROTATE_LOG="${DATA_ROOT}/logs/dr_rotation.log"
          # ì¶”ê°€ëœ DBA ê³ ê¸‰ ë¡œê·¸
          LRQ_LOG="${DATA_ROOT}/logs/dba_lrq_check.log"
          SKEW_LOG="${DATA_ROOT}/logs/dba_skewness_check.log"
          PLAN_DIFF_LOG="${DATA_ROOT}/logs/plan_diff.log"
          {
            echo "=== PIPELINE AUDIT SUMMARY ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "BRANCH=$GITHUB_REF_NAME"
            echo "COMMIT_SHA=$GITHUB_SHA"
            echo
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "TABLES_USED=${TBL_STAGE},${TBL_FINAL},${TBL_AUDIT}"
            echo "GEN_ROWS=${GEN_ROWS}"
            echo "DR_RETENTION_DAYS=${DR_RETENTION_DAYS}"
            echo
            echo "--- SNAPSHOT ARTIFACTS ---"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_TAR=$SNAP_FILE"
            echo "SNAP_ISO=$ISO_FILE"
            echo
            echo "--- STATUS_FILE ---"
            cat "$STATUS_FILE" 2>/dev/null || echo "(no pipeline_status.log)"
            echo
            echo "--- QUALITY_LOG ---"
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo
            echo "--- SLA_TIMING ---"
            cat "$SLA_FILE" 2>/dev/null || echo "(no sla_timing.log)"
            echo
            echo "--- SYSTEM UPGRADE LOG (head 40) ---"
            head -n 40 "$UPG_LOG" 2>/dev/null || echo "(no system_upgrade.log)"
            echo
            echo "--- RETENTION POLICY ---"
            cat "$RETENTION_LOG" 2>/dev/null || echo "(no retention_policy.log)"
            echo
            echo "--- SCHEMA DIFF ---"
            head -n 80 "$DIFF_LOG" 2>/dev/null || echo "(no schema_diff.log)"
            echo
            echo "--- DR BACKUP & ROTATION (${DR_RETENTION_DAYS}d) ---"
            head -n 60 "$DR_LOG" 2>/dev/null || echo "(no dr_backup_attempt.log)"
            echo ">>> DR ROTATION LOG"
            head -n 60 "$ROTATE_LOG" 2>/dev/null || echo "(no dr_rotation.log)"
            echo
            echo "--- GOVERNANCE / APPROVAL ---"
            head -n 60 "$GOV_LOG" 2>/dev/null || echo "(no governance_approval.log)"
            echo
            echo "--- DBA ADVANCED (LRQ / Skew / Plan Diff) ---"
            echo ">>> dba_lrq_check.log"
            head -n 60 "$LRQ_LOG" 2>/dev/null || echo "(no dba_lrq_check.log)"
            echo ">>> dba_skewness_check.log"
            head -n 60 "$SKEW_LOG" 2>/dev/null || echo "(no dba_skewness_check.log)"
            echo ">>> plan_diff.log"
            head -n 60 "$PLAN_DIFF_LOG" 2>/dev/null || echo "(no plan_diff.log)"
            echo
            echo "--- DBA PERFORMANCE / PARTITION / LOCK / PLAN / TUNING ---"
            echo ">>> partition_access.log"
            head -n 60 "$PARTITION_LOG" 2>/dev/null || echo "(no partition_access.log)"
            echo ">>> lock_contention.log"
            head -n 60 "$LOCK_LOG" 2>/dev/null || echo "(no lock_contention.log)"
            echo ">>> query_plan_sample.log"
            head -n 60 "$PLAN_LOG" 2>/dev/null || echo "(no query_plan_sample.log)"
            echo ">>> tuning_recommendations.log"
            head -n 60 "$TUNE_LOG" 2>/dev/null || echo "(no tuning_recommendations.log)"
            echo ">>> stats_maintenance.log"
            head -n 60 "$STATS_LOG" 2>/dev/null || echo "(no stats_maintenance.log)"
            echo ">>> capacity_growth.log"
            head -n 60 "$CAP_LOG" 2>/dev/null || echo "(no capacity_growth.log)"
            echo
            echo "--- CLASSIFICATION / DR PLAYBOOK / DR TOPOLOGY ---"
            echo ">>> table_classification.log"
            head -n 60 "$CLASS_LOG" 2>/dev/null || echo "(no table_classification.log)"
            echo ">>> recovery_playbook.txt (RPO/RTO)"
            head -n 60 "$DRPLAY" 2>/dev/null || echo "(no recovery_playbook.txt)"
            echo ">>> dr_topology.txt"
            head -n 60 "$DR_TOPO_FILE" 2>/dev/null || echo "(no dr_topology.txt)"
            echo
            echo "--- SNAPSHOT HASH (tar.gz) ---"
            cat "$HASH_LOG" 2>/dev/null || echo "(no snapshot_hash.txt)"
            echo
            echo "--- SNAPSHOT ISO HASH ---"
            cat "$ISO_HASH_FILE" 2>/dev/null || echo "(no iso_hash.txt)"
          } > "$AUDIT_FILE"
          echo_note "ê°ì‚¬ ìš”ì•½ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $AUDIT_FILE"
          head -n 200 "$AUDIT_FILE" || true

      #######################################################################
      # 11. Artifact ì—…ë¡œë“œ (ì „ì²´ ì¦ì  ë¬¶ìŒ)
      #######################################################################
      - name: ğŸ“¦ ì‹¤í–‰ ì‚°ì¶œë¬¼ ì—…ë¡œë“œ (logs / quality / source / snapshot ë“±)
        uses: actions/upload-artifact@v4
        with:
          name: teradata-run-${{ github.run_id }}
          path: |
            ${{ env.DATA_ROOT }}/logs/**
            ${{ env.DATA_ROOT }}/quality/**
            ${{ env.DATA_ROOT }}/source_files/**
            ${{ env.DATA_ROOT }}/release/**
            ${{ env.DATA_ROOT }}/landing_zone/**
            ${{ env.DATA_ROOT }}/${{ env.DR_DIR_NAME }}/**
            ${{ env.DATA_ROOT }}/${{ env.DR_TOPOLOGY_DIR }}/**
            ${{ env.DATA_ROOT }}/extract_out/**
            ${{ env.DATA_ROOT }}/tmp/**
            ${{ env.DATA_ROOT }}/health/**
            ${{ env.DATA_ROOT }}/governance/**
            ${{ env.DATA_ROOT }}/history/**
            ${{ env.LOG_DIR }}/**  # EchoOps ì¤‘ì‹¬ ë¡œê·¸ë„ ê°™ì´ ë³´ê´€
          if-no-files-found: warn
          retention-days: 14

      #######################################################################
      # 12. íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ (í•­ìƒ ì„±ê³µ)
      #######################################################################
      - name: âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (í•­ìƒ ì„±ê³µ)
        if: always()
        run: |
          source /tmp/echo_helpers.sh || true
          echo_note "Teradata FinOps Batch íŒŒì´í”„ë¼ì¸ ì „ì²´ ë‹¨ê³„ ìˆ˜í–‰ ì™„ë£Œ"
          echo_note "- EchoOps ë¡œê¹… (/tmp/echo_helpers.sh ê¸°ë°˜)"
          echo_note "- ëŒ€ëŸ‰ ê°€ìƒë°ì´í„° ìƒì„± / í’ˆì§ˆê²€ì¦ / ì¦ê°ë¥  / ê³ ìœ„í—˜ ê±°ë˜ë¹„ìœ¨"
          echo_note "- ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ & pkg diff ê¸°ë¡"
          echo_note "- DBA ê´€ì (íŒŒí‹°ì…˜ í”„ë£¨ë‹, ë½ ê²½í•©, ì¿¼ë¦¬ í”Œëœ, íŠœë‹, í†µê³„ìˆ˜ì§‘, ìš©ëŸ‰ ì„±ì¥)"
          echo_note "- DBA Advanced(LRQ, Skewness, Plan Diff) ì¶”ê°€ ë°˜ì˜"
          echo_note "- ë¯¼ê°ë°ì´í„° ë§ˆìŠ¤í‚¹/ê¶Œí•œ ê°ì‚¬/ê±°ë²„ë„ŒìŠ¤/ì˜¨ì½œ/RPOÂ·RTO ë¬¸ì„œí™”"
          echo_note "- tar.gz ìŠ¤ëƒ…ìƒ· + ISO ìŠ¤ëƒ…ìƒ· ìƒì„± (hash í¬í•¨)"
          echo_note "- DR í† í´ë¡œì§€/DR ë³´ê´€/1ì¼ ë¡œí…Œì´ì…˜/DR ë³µêµ¬ í”Œë ˆì´ë¶"
          echo_note "- GitHub Release ì—…ë¡œë“œ(ì˜µì…˜)"
          echo_note "- HTML ë¦¬í¬íŠ¸ / audit_run_summary.log / artifact ì—…ë¡œë“œ ì™„ë£Œ"
          echo "âœ… ëª¨ë“  ì‚°ì¶œë¬¼ì€ artifact teradata-run-${GITHUB_RUN_ID} ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
          exit 0
