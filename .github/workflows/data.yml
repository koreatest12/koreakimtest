name: "ğŸ¦ Teradata FinOps Batch â€” EchoOps + Audit + Snapshot + Schedule (ë§Œì „ëŒ€ë¹„ + ëŒ€ëŸ‰ê°€ìƒë°ì´í„° + ì‹œìŠ¤í…œì—…ê·¸ë ˆì´ë“œ)"

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:
  schedule:
    # ë§¤ì¼ 15:30 UTC == í•œêµ­ì‹œê°„(KST) ìƒˆë²½ 00:30 ìë™ ì•¼ê°„ë°°ì¹˜
    - cron: "30 15 * * *"

permissions:
  contents: write   # ë¦´ë¦¬ì¦ˆ íƒœê·¸/ìì‚° ì—…ë¡œë“œìš© (gh release create)

env:
  ###########################################################################
  # ê³µí†µ ê²½ë¡œ / ëŒ€ìƒ í…Œì´ë¸”
  ###########################################################################
  DATA_ROOT: /home/runner/td_data

  # ì„œë¹„ìŠ¤/ë„ë©”ì¸ë³„ í…Œì´ë¸” (Staging -> Final -> Audit)
  TBL_STAGE: STG_DATA
  TBL_FINAL: FINAL_DATA
  TBL_AUDIT: LOAD_AUDIT_LOG

  # ë³‘í•© íƒ€ê¹ƒ ìš´ì˜ í…Œì´ë¸”
  TARGET_TABLE: FINAL_DATA

  ###########################################################################
  # ê°€ìƒ ë°ì´í„°/í’ˆì§ˆ ê¸°ì¤€
  #
  # GEN_ROWS: ì´ë²ˆ ë°°ì¹˜ì—ì„œ "ìƒì„±/ì ì¬"í•œë‹¤ê³  ê°€ì •í•˜ëŠ” ê°€ìƒ ë ˆì½”ë“œ ìˆ˜
  # EXPECTED_ROWCOUNT: í’ˆì§ˆ ê²€ì¦ ì‹œ ê¸°ëŒ€ ë¡œìš° ìˆ˜ë¡œ ì‚¬ìš©
  ###########################################################################
  GEN_ROWS: "500000"
  EXPECTED_ROWCOUNT: "500000"

jobs:
  teradata_pipeline_job:
    runs-on: ubuntu-latest

    steps:
      #######################################################################
      # 0. ì½”ë“œ ì²´í¬ì•„ì›ƒ + ì‹œì‘ ì•Œë¦¼
      #######################################################################
      - name: ğŸ“¥ ì½”ë“œ ì²´í¬ì•„ì›ƒ
        uses: actions/checkout@v4

      - name: ğŸ“ ì›Œí¬í”Œë¡œìš° ì‹œì‘ ë¡œê¹…
        run: |
          echo "::notice::Teradata í†µí•© ì›Œí¬í”Œë¡œìš° ì‹œì‘ (ì•¼ê°„ë°°ì¹˜/ìˆ˜ë™ì‹¤í–‰/Push ê³µí†µ)."
          echo "::debug::DATA_ROOT=${DATA_ROOT}, TARGET_TABLE=${TARGET_TABLE}, GEN_ROWS=${GEN_ROWS}"

      #######################################################################
      # 0.5 ëŸ¬ë„ˆ í™˜ê²½ ì—…ê·¸ë ˆì´ë“œ(apt update && upgrade ë“±)
      #     - ë„¤íŠ¸ì›Œí¬/íŒ¨í‚¤ì§€ ë ˆí¬ ì´ìŠˆë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ continue-on-error
      #     - ê²°ê³¼ëŠ” ì—…ê·¸ë ˆì´ë“œ ë¡œê·¸ë¡œ ë‚¨ê¹€ (ë³´ì•ˆ/ê°ì‚¬ ê´€ì ì—ì„œ ì¤‘ìš”)
      #######################################################################
      - name: ğŸ”„ ëŸ¬ë„ˆ íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ (ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ & ì—…ê·¸ë ˆì´ë“œ)
        continue-on-error: true
        run: |
          set +e
          mkdir -p "${DATA_ROOT}/logs"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"

          {
            echo "===== SYSTEM UPGRADE START ====="
            date
            echo "--- apt-get update ---"
            sudo apt-get update -y || echo "[WARN] apt-get update ì‹¤íŒ¨"
            echo "--- apt-get upgrade (dist-upgrade) ---"
            sudo apt-get -o Dpkg::Options::="--force-confnew" dist-upgrade -y || echo "[WARN] dist-upgrade ì‹¤íŒ¨"
            echo "--- apt-get autoremove ---"
            sudo apt-get autoremove -y || true
            echo "--- uname -a ---"
            uname -a
            echo "--- lsb_release -a (ê°€ëŠ¥í•˜ë©´) ---"
            lsb_release -a 2>/dev/null || echo "lsb_release not available"
            echo "===== SYSTEM UPGRADE END ====="
          } > "$UPG_LOG" 2>&1

          echo "::notice::ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ(íŒ¨í‚¤ì§€ ìµœì‹ í™”) ì‹œë„ ì™„ë£Œ. ìƒì„¸ ë‚´ìš©ì€ $UPG_LOG ì°¸ê³ ."
          set -e

      #######################################################################
      # 1. ë””ë ‰í† ë¦¬ ì¤€ë¹„ + êµ¬ì¡° ìƒì„±
      #######################################################################
      - name: ğŸ“‚ ë””ë ‰í† ë¦¬ ìƒì„± ë° í™˜ê²½ ì´ˆê¸°í™”
        run: |
          set -e
          echo "ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì¤‘..."
          mkdir -p "${DATA_ROOT}"/{source_files,landing_zone,logs,archive,extract_out,tmp,quality,release}

          echo "ìƒì„±ëœ ë””ë ‰í† ë¦¬ í™•ì¸:"
          ls -R "${DATA_ROOT}" || true

      #######################################################################
      # 1.5 ê°€ìƒ ë°ì´í„° ëŒ€ëŸ‰ ìƒì„± (landing_zone ì— ëŒ€ìš©ëŸ‰ CSV ìƒì„±)
      #
      #  - GEN_ROWS ë§Œí¼ì˜ ëœë¤/ìœ ì‚¬ëœë¤ ë°ì´í„°ë¥¼ ìƒì„±
      #  - Teradata TPTê°€ ë¹¨ì•„ì˜¬ 'ì›ë³¸ ë¤í”„' ì—­í• 
      #  - ì‹¤ì œ ê°œì¸ì •ë³´ ì•„ë‹˜ (ì „ë¶€ ê°€ì§œ ê³ ê°/ê°€ë§¹ì /ì‹œê°„ ì •ë³´)
      #
      # CSV ìŠ¤í‚¤ë§ˆ:
      # COL1(ê³ ê°ID ë¹„ìŠ·), COL2(ê°€ë§¹ì /ë©”ëª¨), LOAD_TS(íƒ€ì„ìŠ¤íƒ¬í”„)
      #######################################################################
      - name: ğŸ— ê°€ìƒ ë°ì´í„° ëŒ€ëŸ‰ ìƒì„± (GEN_ROWS=${{ env.GEN_ROWS }})
        run: |
          set -e
          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"
          ROWS="${GEN_ROWS}"

          echo "COL1,COL2,LOAD_TS" > "$SRC_FILE"

          # ê°„ë‹¨í•œ ì˜ì‚¬ë‚œìˆ˜ ë°ì´í„° ìƒì„±:
          #   COL1: ì‹œí€€ìŠ¤ ë²ˆí˜¸
          #   COL2: MERCHANT_xxx í˜¹ì€ NOTE_xxx í˜•íƒœì˜ ê°€ìƒ ê±°ë˜ ë©”ëª¨
          #   LOAD_TS: í˜„ì¬ ì‹œê°„ ê¸°ì¤€
          #
          # ì„±ëŠ¥: seqëŠ” ë¹ ë¥´ì§€ë§Œ echo ë£¨í”„ëŠ” ëŠë¦´ ìˆ˜ ìˆìŒ.
          # GEN_ROWS=500000ë„ ê¹ƒí—ˆë¸Œ ëŸ¬ë„ˆì—ì„œ ìˆ˜ì´ˆ~ìˆ˜ì‹­ì´ˆ ìˆ˜ì¤€ì€ í—ˆìš© ê°€ëŠ¥.
          # í•„ìš” ì‹œ ì¤„ì—¬ë„ ë¨.
          echo "ê°€ìƒ ë°ì´í„° ${ROWS}í–‰ ìƒì„± ì¤‘..."
          i=1
          while [ $i -le $ROWS ]; do
            # ê°„ë‹¨ ì˜ì‚¬ë‚œìˆ˜: $RANDOM (0~32767). merchant id ê°€ì§œ
            MERCH=$(( (RANDOM % 9000) + 1000 ))
            echo "${i},MERCHANT_${MERCH}_NOTE_${i},$(date +%Y-%m-%dT%H:%M:%S%z)" >> "$SRC_FILE"
            i=$((i+1))
          done

          ls -lh "$SRC_FILE"
          head -n 5 "$SRC_FILE"
          tail -n 5 "$SRC_FILE"

          # í’ˆì§ˆ/ê²€ì¦ìš©ìœ¼ë¡œ row countë¥¼ ê¸°ë¡í•´ ë‘ 
          echo "${ROWS}" > "${DATA_ROOT}/tmp/generated_rowcount.txt"
          echo "::notice::ê°€ìƒ CSV ìƒì„± ì™„ë£Œ (${ROWS} rows) -> $SRC_FILE"

      #######################################################################
      # 2. í™˜ê²½ ìŠ¤ëƒ…ìƒ· (ë³´ì•ˆ/ê°ì‚¬ìš© ëŸ°íƒ€ì„ ì¦ë¹™)
      #######################################################################
      - name: ğŸ” ëŸ°íƒ€ì„ í™˜ê²½ ìŠ¤ëƒ…ìƒ· ì €ì¥
        run: |
          SNAP="${DATA_ROOT}/logs/env_snapshot.txt"
          {
            echo "===== ENV SNAPSHOT ====="
            date
            uname -a
            whoami
            echo "--- PATH ---"
            echo "$PATH"
            echo "--- DISK (df -h) ---"
            df -h
            echo "--- MEMORY (free -m) ---"
            free -m || true
            echo "--- GITHUB CONTEXT ---"
            echo "RUN_ID=$GITHUB_RUN_ID"
            echo "RUN_NUMBER=$GITHUB_RUN_NUMBER"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "SHA=$GITHUB_SHA"
            echo "BRANCH=$GITHUB_REF_NAME"
          } > "$SNAP"
          echo "::debug::í™˜ê²½ ìŠ¤ëƒ…ìƒ· ê¸°ë¡ ì™„ë£Œ -> $SNAP"

      #######################################################################
      # 3. DDL / TPT / ì¿¼ë¦¬ ì •ì˜ ë°±ì—…
      #    (ê° ì„œë¹„ìŠ¤ë³„ table & query ë¬¸ì„œí™”)
      #######################################################################
      - name: ğŸ§± DDL/TPT/ì¿¼ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ë°±ì—… (ê° ì„œë¹„ìŠ¤ë³„ table / query)
        run: |
          set -e
          mkdir -p "${DATA_ROOT}/source_files/sql"
          mkdir -p "${DATA_ROOT}/source_files/tpt"

          # í…Œì´ë¸” ì •ì˜
          cat > "${DATA_ROOT}/source_files/sql/create_tables.sql" <<'SQL'
          -- [STAGING TABLE] (ì›ì²œ ë¤í”„ë¥¼ ê·¸ëŒ€ë¡œ ì ì¬)
          CREATE TABLE STG_DATA (
            COL1       INTEGER,
            COL2       VARCHAR(100),
            LOAD_TS    TIMESTAMP,
            BATCH_ID   VARCHAR(40)
          );

          -- [FINAL TABLE] (ì •ì œ/ë³€í™˜/ì¤‘ë³µì œê±° í›„ ìš´ì˜ì˜ì—­)
          CREATE TABLE FINAL_DATA (
            COL1       INTEGER,
            COL2       VARCHAR(100),
            LOAD_TS    TIMESTAMP,
            ETL_TS     TIMESTAMP,
            SRC_BATCH  VARCHAR(40)
          );

          -- [AUDIT TABLE] (ë°°ì¹˜ ì ì¬ ì´ë ¥/í’ˆì§ˆ ê²€ì¦ ê²°ê³¼)
          CREATE TABLE LOAD_AUDIT_LOG (
            AUDIT_TS        TIMESTAMP,
            BATCH_ID        VARCHAR(40),
            SRC_FILE        VARCHAR(255),
            ROW_LOADED      INTEGER,
            ROW_EXPECTED    INTEGER,
            STATUS_CODE     INTEGER,
            STATUS_MESSAGE  VARCHAR(2000),
            OPERATOR        VARCHAR(128)
          );

          -- ì¸ë±ìŠ¤ ì˜ˆì‹œ
          CREATE INDEX IDX_FINAL_DATA_COL1 ON FINAL_DATA (COL1);
          CREATE INDEX IDX_AUDIT_BATCH     ON LOAD_AUDIT_LOG (BATCH_ID);

          SQL

          # ë³‘í•©/ì •ì œ í”„ë¡œì‹œì € ì˜ˆì‹œ
          cat > "${DATA_ROOT}/source_files/sql/proc_LOAD_AND_MERGE.sql" <<'SQL'
          REPLACE PROCEDURE LOAD_AND_MERGE (
            IN p_batch_id VARCHAR(40),
            IN p_src_file VARCHAR(255)
          )
          BEGIN
            /* 1) STG_DATA -> FINAL_DATA ë¡œ ë³‘í•©/ì •ì œ */
            INSERT INTO FINAL_DATA (COL1, COL2, LOAD_TS, ETL_TS, SRC_BATCH)
            SELECT
              COL1,
              TRIM(COL2),
              LOAD_TS,
              CURRENT_TIMESTAMP,
              p_batch_id
            FROM STG_DATA
            WHERE BATCH_ID = p_batch_id;

            /* 2) AUDIT LOG ì— ì´ë ¥ ì¶•ì  */
            INSERT INTO LOAD_AUDIT_LOG (
              AUDIT_TS, BATCH_ID, SRC_FILE,
              ROW_LOADED, ROW_EXPECTED,
              STATUS_CODE, STATUS_MESSAGE, OPERATOR
            )
            VALUES (
              CURRENT_TIMESTAMP,
              p_batch_id,
              p_src_file,
              NULL,   -- ì‚¬í›„ UPDATE
              NULL,   -- ì‚¬í›„ UPDATE
              0,
              'LOAD_AND_MERGE executed',
              USER
            );
          END;
          SQL

          # Teradata TPT Bulk Load í…œí”Œë¦¿
          cat > "${DATA_ROOT}/source_files/tpt/load_stg_data.tpt" <<'TPT'
          DEFINE JOB LOAD_STG_DATA
          (
            DEFINE SCHEMA STG_SCHEMA
            (
              COL1       INTEGER,
              COL2       VARCHAR(100),
              LOAD_TS    VARCHAR(30)
            );

            DEFINE OPERATOR FILE_READER
            TYPE DATACONNECTOR PRODUCER
            SCHEMA STG_SCHEMA
            ATTRIBUTES
            (
              FileName = '/home/runner/td_data/landing_zone/input_YYYYMMDD.csv',
              Format   = 'Delimited'
            );

            DEFINE OPERATOR TPT_INSERTER
            TYPE STREAM
            TARGET TABLE STG_DATA
            ATTRIBUTES
            (
              TdpId    = 'TERADATA_SID',
              UserName = 'ETL_USER',
              UserPassword = 'ETL_PASS',
              LogTable = 'ETL_LOG_TABLE'
            );

            APPLY
            (
              'INSERT INTO STG_DATA (COL1, COL2, LOAD_TS, BATCH_ID)
               VALUES (:COL1, :COL2, TIMESTAMP :LOAD_TS, ''BATCH_PLACEHOLDER'');'
            )
            TO OPERATOR (TPT_INSERTER[1])
            SELECT
              COL1, COL2, LOAD_TS
            FROM OPERATOR (FILE_READER[1]);
          );
          TPT

          # í’ˆì§ˆ ê²€ì¦ / í†µê³„ / ê°ì‚¬ ì¡°íšŒ ì¿¼ë¦¬ ëª¨ìŒ
          cat > "${DATA_ROOT}/source_files/sql/quality_queries.sql" <<'SQL'
          -- íŠ¹ì • ë°°ì¹˜ì— ëŒ€í•´ STG_DATA ì ì¬ ê±´ìˆ˜
          SELECT COUNT(*) AS CNT_STG
          FROM STG_DATA
          WHERE BATCH_ID = :BATCH_ID;

          -- ìµœì¢… FINAL_DATA ë°˜ì˜ ê±´ìˆ˜
          SELECT COUNT(*) AS CNT_FINAL
          FROM FINAL_DATA
          WHERE SRC_BATCH = :BATCH_ID;

          -- í’ˆì§ˆ: ê³µë°±/NULL ë©”ëª¨ ë¹„ìœ¨ ë“±
          SELECT
            SUM(CASE WHEN COL2 IS NULL OR TRIM(COL2) = '' THEN 1 ELSE 0 END) AS NULL_MEMO_ROWS,
            COUNT(*) AS TOTAL_ROWS
          FROM FINAL_DATA
          WHERE SRC_BATCH = :BATCH_ID;

          -- ìµœê·¼ AUDIT ë¡œê·¸ (ë°°ì¹˜ë³„ ìµœì‹  íˆìŠ¤í† ë¦¬)
          SELECT *
          FROM LOAD_AUDIT_LOG
          QUALIFY ROW_NUMBER()
            OVER (PARTITION BY BATCH_ID ORDER BY AUDIT_TS DESC) = 1
          ORDER BY AUDIT_TS DESC;
          SQL

          echo "::notice::DDL/TPT/ì¿¼ë¦¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ${DATA_ROOT}/source_files ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
          ls -R "${DATA_ROOT}/source_files" || true

      #######################################################################
      # 4. ë°ì´í„° ì ì¬ & ë³‘í•© (ì‹¤ì œ Teradata ë™ì‘ ëŒ€ì‹  ì‹œë®¬)
      #    - ê°€ìƒ ëŒ€ëŸ‰ CSVë¥¼ TPTë¡œ ë„£ì—ˆë‹¤ê³  ê°€ì •
      #    - LOAD_AND_MERGE í˜¸ì¶œí–ˆë‹¤ê³  ê°€ì •
      #    - ì‹¤íŒ¨í•´ë„ íŒŒì´í”„ë¼ì¸ì€ í•­ìƒ ê³„ì† (continue-on-error)
      #    - ìƒíƒœì½”ë“œ/í–‰ìˆ˜ ê¸°ë¡
      #######################################################################
      - name: ğŸ”„ ë°ì´í„° ë¡œë“œ ë° ë³€í™˜/ë³‘í•© ì‹¤í–‰ (ëŒ€ëŸ‰ ê°€ìƒë°ì´í„° / ì‹¤íŒ¨í•´ë„ ê³„ì†)
        continue-on-error: true
        run: |
          set +e
          BATCH_ID="BATCH_$(date +%Y%m%d_%H%M%S)"
          SRC_FILE="${DATA_ROOT}/landing_zone/input_$(date +%Y%m%d).csv"
          PIPELINE_LOG="${DATA_ROOT}/logs/pipeline_load_merge.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"

          # ì‹¤ì œ Teradataë¼ë©´:
          # 1) TPT ì‹¤í–‰ìœ¼ë¡œ ${SRC_FILE} -> ${TBL_STAGE}
          # 2) CALL LOAD_AND_MERGE(BATCH_ID, SRC_FILE);
          # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰.

          GENERATED_COUNT=$(cat "${DATA_ROOT}/tmp/generated_rowcount.txt" 2>/dev/null || echo "${GEN_ROWS}")

          {
            echo "=== LOAD STAGE START ==="
            echo "BATCH_ID=$BATCH_ID"
            echo "SRC_FILE=$SRC_FILE"
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "TBL_STAGE=${TBL_STAGE}, TBL_FINAL=${TBL_FINAL}, TBL_AUDIT=${TBL_AUDIT}"
            echo
            echo "[1] (ì‹œë®¬) TPT Bulk Load ${GENERATED_COUNT}í–‰ -> ${TBL_STAGE}"
            echo "    FROM ${SRC_FILE}"
            echo
            echo "[2] (ì‹œë®¬) CALL LOAD_AND_MERGE('${BATCH_ID}','${SRC_FILE}')"
            echo "    ${TBL_STAGE} -> ${TBL_FINAL} ë¡œ ì •ì œ/ë³‘í•© ë° AUDIT ë¡œê¹…"
            echo
            echo "[3] (ì‹œë®¬) ${TBL_AUDIT} ì— ê°ì‚¬ ì´ë ¥ insert"
            echo "=== LOAD STAGE END ==="
          } | tee "$PIPELINE_LOG"

          # ì„±ê³µ ê°€ì • â†’ STATUS_CODE=0
          STATUS_CODE=0
          STATUS_MSG="OK(vdata-bulk-load)"
          echo "STATUS_CODE=${STATUS_CODE}"        >  "$STATUS_FILE"
          echo "STATUS_MSG=${STATUS_MSG}"         >> "$STATUS_FILE"
          echo "BATCH_ID=${BATCH_ID}"             >> "$STATUS_FILE"
          echo "SRC_FILE=${SRC_FILE}"             >> "$STATUS_FILE"
          echo "ROW_EXPECTED=${EXPECTED_ROWCOUNT}" >> "$STATUS_FILE"
          echo "ROW_LOADED=${GENERATED_COUNT}"     >> "$STATUS_FILE"

          echo "::notice::ë°ì´í„° ì ì¬/ë³‘í•©(ëŒ€ìš©ëŸ‰ ê°€ìƒ ë°ì´í„°) ë‹¨ê³„ ì™„ë£Œ. íŒŒì´í”„ë¼ì¸ì€ ê³„ì†ë©ë‹ˆë‹¤."
          set -e

      #######################################################################
      # 5. í’ˆì§ˆ ê²€ì¦ / QC ë¡œê·¸ ìƒì„±
      #    - ê¸°ëŒ€ ê±´ìˆ˜ vs ì‹¤ì œ ê±´ìˆ˜ ë¹„êµ
      #    - NULL/ì´ìƒë°ì´í„° ë“± ê¸°ë³¸ sanity check ê²°ê³¼
      #######################################################################
      - name: âœ… í’ˆì§ˆ ê²€ì¦ ë° QC ë¡œê·¸ ìƒì„±
        run: |
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"

          EXPECTED="${EXPECTED_ROWCOUNT}"
          ACTUAL="$(grep '^ROW_LOADED=' \"$STATUS_FILE\" 2>/dev/null | cut -d= -f2)"
          if [ -z "$ACTUAL" ]; then
            ACTUAL="0"
          fi

          ROW_COUNT_OK="false"
          if [ "$EXPECTED" = "$ACTUAL" ]; then
            ROW_COUNT_OK="true"
          fi

          {
            echo "=== QUALITY CHECK ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "EXPECTED_ROWCOUNT=$EXPECTED"
            echo "ACTUAL_ROWCOUNT=$ACTUAL"
            echo "ROW_COUNT_OK=$ROW_COUNT_OK"
            echo
            echo "--- PIPELINE STATUS SNAPSHOT ---"
            cat "$STATUS_FILE" 2>/dev/null || echo "NO STATUS_FILE"
            echo
            echo "--- SIMPLE SANITY RULES ---"
            echo "1) ì ì¬ ê±´ìˆ˜ ê¸°ëŒ€ì¹˜ì™€ ë™ì¼í•œê°€? -> $ROW_COUNT_OK"
            echo "2) (ì‹œë®¬) NULL/ê³µë°± ë©”ëª¨ ë ˆì½”ë“œ ë¹„ìœ¨ì€ HIGHí•˜ì§€ ì•Šì€ê°€? -> assumed OK"
          } > "$QUALITY_LOG"

          echo "::notice::í’ˆì§ˆ ê²€ì¦ ë¡œê·¸ ì‘ì„± ì™„ë£Œ -> $QUALITY_LOG"
          head -n 50 "$QUALITY_LOG" || true

      #######################################################################
      # 6. ê¶Œí•œ/ì ‘ê·¼ì œì–´ ê°ì‚¬ ê¸°ë¡ (ROLE/GRANT/REVOKE)
      #######################################################################
      - name: ğŸ” ê¶Œí•œ/ì ‘ê·¼ì œì–´ ê°ì‚¬ ê¸°ë¡ ìƒì„±
        run: |
          ACL_LOG="${DATA_ROOT}/logs/acl_audit.sql"
          {
            echo "-- ê¶Œí•œ ê°ì‚¬ ë¡œê·¸ (ì‹œë®¬ë ˆì´ì…˜)"
            echo "-- FINAL_DATA ì¡°íšŒ/ì“°ê¸° ê¶Œí•œ ì„¤ì • ì˜ˆì‹œ"
            echo "GRANT SELECT ON ${TBL_FINAL} TO ROLE ANALYST_ROLE;"
            echo "GRANT INSERT,UPDATE ON ${TBL_FINAL} TO ROLE ETL_LOADER_ROLE;"
            echo "REVOKE INSERT ON ${TBL_FINAL} FROM ROLE ANALYST_ROLE;"
            echo
            echo "-- AUDIT LOG í…Œì´ë¸” ì ‘ê·¼ í†µì œ"
            echo "GRANT SELECT ON ${TBL_AUDIT} TO ROLE AUDIT_READER_ROLE;"
            echo
            echo "-- ì‹¤í–‰ ë©”íƒ€"
            echo "-- AUDIT_TS=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "-- EXEC_BY=$GITHUB_ACTOR"
            echo "-- COMMIT_SHA=$GITHUB_SHA"
            echo "-- BRANCH=$GITHUB_REF_NAME"
          } > "$ACL_LOG"

          echo "::debug::ê¶Œí•œ ê°ì‚¬ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $ACL_LOG"
          head -n 40 "$ACL_LOG" || true

      #######################################################################
      # 7. ë¦´ë¦¬ì¦ˆ ìŠ¤ëƒ…ìƒ· & ì••ì¶•ë³¸ ìƒì„± (tar.gz)
      #    - ë‚ ì§œ+SHA ì¡°í•©ëœ íƒœê·¸
      #######################################################################
      - name: ğŸ—œ ê²°ê³¼ë¬¼ tar.gz ìŠ¤ëƒ…ìƒ· ìƒì„±
        run: |
          SNAP_TAG="td-snapshot-$(date +%Y%m%d-%H%M%S)-${GITHUB_SHA:0:8}"
          SNAP_DIR="${DATA_ROOT}/release"
          SNAP_FILE="${SNAP_DIR}/${SNAP_TAG}.tar.gz"

          echo "SNAP_TAG=$SNAP_TAG"    | tee "${DATA_ROOT}/logs/snapshot_tag.txt"
          echo "SNAP_FILE=$SNAP_FILE"  | tee -a "${DATA_ROOT}/logs/snapshot_tag.txt"

          mkdir -p "$SNAP_DIR"

          # íšŒê³„/ì •ì‚°ë³¸ ëŠë‚Œìœ¼ë¡œ ë‚¨ê¸¸ ì •ë³´ë“¤ ì „ë¶€ ë¬¶ìŒ
          tar -czf "$SNAP_FILE" \
            -C "${DATA_ROOT}" \
            logs \
            quality \
            extract_out \
            source_files \
            landing_zone \
            tmp \
            || true

          echo "::notice::ìŠ¤ëƒ…ìƒ· ì•„ì¹´ì´ë¸Œ ìƒì„± -> $SNAP_FILE"
          ls -lh "$SNAP_FILE" || true

      #######################################################################
      # 8. GitHub Release ì—…ë¡œë“œ ì‹œë„ (ì˜µì…˜)
      #######################################################################
      - name: ğŸš€ GitHub Release(ìŠ¤ëƒ…ìƒ·) ì—…ë¡œë“œ ì‹œë„
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          if [ -f "$SNAP_INFO" ]; then
            SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
            SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)
          fi

          if [ -n "${SNAP_TAG:-}" ] && [ -f "${SNAP_FILE:-}" ]; then
            echo "::notice::ë¦´ë¦¬ì¦ˆ íƒœê·¸ $SNAP_TAG, íŒŒì¼ $SNAP_FILE ì—…ë¡œë“œ ì‹œë„ ì¤‘"
            gh release create "$SNAP_TAG" "$SNAP_FILE" \
              --title "$SNAP_TAG" \
              --notes "ìë™ Teradata ë°°ì¹˜ ìŠ¤ëƒ…ìƒ· (ê°€ìƒ ëŒ€ëŸ‰ë°ì´í„°, ì—…ê·¸ë ˆì´ë“œ ë¡œê·¸, í…Œì´ë¸”/ê¶Œí•œ ì •ì˜, í’ˆì§ˆê²€ì¦, ê°ì‚¬ë¡œê·¸ í¬í•¨)" \
              || echo "::warning::gh release create ì‹¤íŒ¨ (ê¶Œí•œ ë¶€ì¡± ë˜ëŠ” íƒœê·¸ ì¤‘ë³µ ê°€ëŠ¥)"
          else
            echo "::warning::ìŠ¤ëƒ…ìƒ· ì •ë³´ê°€ ì—†ì–´ ë¦´ë¦¬ì¦ˆ ìƒëµ"
          fi

      #######################################################################
      # 9. ê°ì‚¬ ìš”ì•½ ë¡œê·¸ (audit_run_summary.log)
      #    - ì „ì²´ ì‹¤í–‰ ë©”íƒ€/ìƒíƒœ/í’ˆì§ˆ/ìŠ¤ëƒ…ìƒ· ê²½ë¡œ ê¸°ë¡
      #######################################################################
      - name: ğŸ§¾ ê°ì‚¬ ìš”ì•½ ë¡œê·¸ ìƒì„± (audit_run_summary.log)
        run: |
          AUDIT_FILE="${DATA_ROOT}/logs/audit_run_summary.log"
          STATUS_FILE="${DATA_ROOT}/logs/pipeline_status.log"
          SNAP_INFO="${DATA_ROOT}/logs/snapshot_tag.txt"
          QUALITY_LOG="${DATA_ROOT}/quality/quality_check.log"
          UPG_LOG="${DATA_ROOT}/logs/system_upgrade.log"

          SNAP_TAG="N/A"
          SNAP_FILE="N/A"
          if [ -f "$SNAP_INFO" ]; then
            SNAP_TAG=$(grep '^SNAP_TAG=' "$SNAP_INFO" | cut -d= -f2)
            SNAP_FILE=$(grep '^SNAP_FILE=' "$SNAP_INFO" | cut -d= -f2)
          fi

          {
            echo "=== PIPELINE AUDIT SUMMARY ==="
            echo "TIMESTAMP=$(date +%Y-%m-%dT%H:%M:%S%z)"
            echo "ACTOR=$GITHUB_ACTOR"
            echo "REPO=$GITHUB_REPOSITORY"
            echo "BRANCH=$GITHUB_REF_NAME"
            echo "COMMIT_SHA=$GITHUB_SHA"
            echo
            echo "TARGET_TABLE=${TARGET_TABLE}"
            echo "TABLES_USED=${TBL_STAGE},${TBL_FINAL},${TBL_AUDIT}"
            echo "GEN_ROWS=${GEN_ROWS}"
            echo
            echo "--- STATUS_FILE ---"
            cat "$STATUS_FILE" 2>/dev/null || echo "(no pipeline_status.log)"
            echo
            echo "--- QUALITY_LOG ---"
            cat "$QUALITY_LOG" 2>/dev/null || echo "(no quality_check.log)"
            echo
            echo "--- SYSTEM UPGRADE LOG (ìš”ì•½ ì•ë¶€ë¶„ 60ì¤„) ---"
            head -n 60 "$UPG_LOG" 2>/dev/null || echo "(no system_upgrade.log)"
            echo
            echo "--- SNAPSHOT INFO ---"
            echo "SNAP_TAG=$SNAP_TAG"
            echo "SNAP_FILE=$SNAP_FILE"
            echo "DATA_ROOT=$DATA_ROOT"
          } > "$AUDIT_FILE"

          echo "::notice::ê°ì‚¬ ìš”ì•½ ë¡œê·¸ ìƒì„± ì™„ë£Œ -> $AUDIT_FILE"
          head -n 200 "$AUDIT_FILE" || true

      #######################################################################
      # 10. Artifact ì—…ë¡œë“œ (ë¡œê·¸/DDL/ìŠ¤ëƒ…ìƒ·/ê°€ìƒë°ì´í„°)
      #######################################################################
      - name: ğŸ“¦ ì‹¤í–‰ ì‚°ì¶œë¬¼ ì—…ë¡œë“œ (logs / quality / source / snapshot ë“±)
        uses: actions/upload-artifact@v4
        with:
          name: teradata-run-${{ github.run_id }}
          path: |
            ${{ env.DATA_ROOT }}/logs/**
            ${{ env.DATA_ROOT }}/quality/**
            ${{ env.DATA_ROOT }}/source_files/**
            ${{ env.DATA_ROOT }}/release/**
            ${{ env.DATA_ROOT }}/landing_zone/**
            ${{ env.DATA_ROOT }}/extract_out/**
            ${{ env.DATA_ROOT }}/tmp/**
          if-no-files-found: warn
          retention-days: 14

      #######################################################################
      # 11. ìµœì¢… ì¢…ë£Œ (í•­ìƒ ì„±ê³µìœ¼ë¡œ ì¢…ë£Œ)
      #######################################################################
      - name: âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (í•­ìƒ ì„±ê³µ)
        if: always()
        run: |
          echo "âœ… Teradata FinOps Batch íŒŒì´í”„ë¼ì¸ ì „ì²´ ë‹¨ê³„ ìˆ˜í–‰ ì™„ë£Œ."
          echo "   - ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ ë¡œê·¸ ê¸°ë¡"
          echo "   - ëŒ€ëŸ‰ ê°€ìƒë°ì´í„°(${GEN_ROWS} rows) ìƒì„± ë° ì ì¬ ì‹œë®¬"
          echo "   - í…Œì´ë¸”/ê¶Œí•œ/í’ˆì§ˆ/ê°ì‚¬/ìŠ¤ëƒ…ìƒ· ì „ë¶€ ì‚°ì¶œë¬¼í™”"
          echo "   - tar.gz ìŠ¤ëƒ…ìƒ· + (ì˜µì…˜) Release ì—…ë¡œë“œ"
          echo "   - audit_run_summary.log, quality_check.log, pipeline_status.log ë“± ì¶”ì  ê°€ëŠ¥ ë¡œê·¸ ìƒì„±"
          echo "ëª¨ë“  ì‚°ì¶œë¬¼ì´ artifact teradata-run-${GITHUB_RUN_ID} ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
          exit 0
