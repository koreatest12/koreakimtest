name: "üá∞üá∑ Korea Finance ‚Äî OMNIVERSE ETERNITY v16.1 (STABILITY-PLUS)"

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:
    inputs:
      active_customer_count:
        description: "Active Customers (Target: 1M)"
        default: "500000"
        required: true
      deploy_env:
        description: "Deployment Target"
        default: "production-god-mode"

permissions:
  contents: write

env:
  TZ: Asia/Seoul
  DEBIAN_FRONTEND: noninteractive
  # Paths
  LOG_DIR: .github/logs
  ARTIFACT_DIR: artifacts
  VISUAL_DIR: artifacts/visuals
  VAULT_DIR: artifacts/vault
  # Credentials
  PG_USER: postgres
  PG_PASS: postgres_secret_v16
  ENC_KEY: "korea-fin-eternity-god-key"
  DOCKER_OPTS: "--restart unless-stopped --log-driver local"

jobs:
  eternity-god-build:
    name: "üèõÔ∏è ETERNITY GOD-MODE (Intelligent OLAP)"
    runs-on: ubuntu-latest
    
    steps:
      # ===========================================================================
      # 0) HYPER-INITIALIZATION
      # ===========================================================================
      - name: "üîß Init System & Dependencies"
        run: |
          mkdir -p ${LOG_DIR} ${ARTIFACT_DIR} ${VISUAL_DIR} ${VAULT_DIR}
          
          # Install Python Data Science Stack
          sudo apt-get update -qq && sudo apt-get install -y -qq postgresql-client python3-pip
          pip3 install pandas matplotlib seaborn sqlalchemy psycopg2-binary
          
          echo "‚úÖ System Ready: Python Data Science Stack Loaded."

      # ===========================================================================
      # 1) HYBRID DATA CLUSTER BOOT (PG + ClickHouse)
      # ===========================================================================
      - name: "üêò‚ö° Boot Hybrid Cluster"
        run: |
          # 1. OLTP: PostgreSQL 16
          docker run -d --name pg_core $DOCKER_OPTS \
            -e POSTGRES_PASSWORD=${{ env.PG_PASS }} \
            -p 5432:5432 postgres:16-alpine

          # 2. OLAP: ClickHouse (Performance Tuned)
          docker run -d --name ch_analytics $DOCKER_OPTS \
            -p 8123:8123 -p 9000:9000 \
            --ulimit nofile=262144:262144 \
            clickhouse/clickhouse-server:latest

          echo "‚è≥ Waiting for Cluster Warm-up..."
          sleep 10
          
          # Health Check Loop
          for i in {1..15}; do
            if curl -s "http://localhost:8123/ping" | grep -q "Ok"; then
              echo "‚úÖ ClickHouse OLAP is READY!"
              break
            fi
            sleep 3
          done

      # ===========================================================================
      # 2) MASSIVE DATA GENERATION (OLTP) - [FIX APPLIED]
      # ===========================================================================
      - name: "üí∞ Generate Wealth Data (OLTP)"
        env:
          PGPASSWORD: ${{ env.PG_PASS }}
          # GitHub InputÏùÑ ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú Î™ÖÏãúÏ†Å Îß§Ìïë
          INPUT_ROWS: ${{ inputs.active_customer_count }}
        run: |
          # [FIX 1] Input Validation Logic (Îπà Í∞í Î∞©ÏßÄ Î∞è Ïà´Ïûê Í≤ÄÏ¶ù)
          # ÏûÖÎ†•Í∞íÏù¥ ÏóÜÍ±∞ÎÇò Ïà´ÏûêÍ∞Ä ÏïÑÎãàÎ©¥ Í∏∞Î≥∏Í∞í 100,000ÏúºÎ°ú ÏÑ§Ï†ï
          if [[ -z "${INPUT_ROWS}" ]] || ! [[ "${INPUT_ROWS}" =~ ^[0-9]+$ ]]; then
            echo "‚ö†Ô∏è Invalid Input Detected. Fallback to default: 100000"
            ROWS=100000
          else
            ROWS=${INPUT_ROWS}
          fi
          
          echo "üéØ Target Rows: $ROWS"

          echo "üë• Generating Accounts in PostgreSQL..."
          docker exec -i pg_core psql -U ${{ env.PG_USER }} -c "
            CREATE TABLE accounts (
              id SERIAL PRIMARY KEY, 
              acc_num TEXT, 
              balance BIGINT, 
              risk_score INT,
              region TEXT,
              created_at TIMESTAMP DEFAULT NOW()
            );
            
            -- [FIX 2] Explicit Casting inside SQL to prevent type errors
            INSERT INTO accounts (acc_num, balance, risk_score, region)
            SELECT 
              'KR-'||g, 
              (random() * 1000000000)::bigint, 
              floor(random()*100)::int,
              (ARRAY['Seoul','Busan','Jeju','Incheon','Gwangju'])[floor(random()*5)+1]
            FROM generate_series(1, $ROWS) AS g;
            
            CREATE INDEX idx_region ON accounts(region);
          "

      # ===========================================================================
      # 3) HIGH-PERFORMANCE ETL & PARTITIONING (OLAP)
      # ===========================================================================
      - name: "üîÑ Streaming ETL to ClickHouse (Partitioned)"
        env:
          PGPASSWORD: ${{ env.PG_PASS }}
        run: |
          echo "üèóÔ∏è Designing ClickHouse Schema (Partitioned by Region)..."
          
          # PARTITION BY region allows faster per-region analytics
          docker exec -i ch_analytics clickhouse-client --query="
            CREATE TABLE IF NOT EXISTS default.dw_finance (
              acc_num String,
              balance Int64,
              risk_score Int8,
              region String
            ) ENGINE = MergeTree() 
            PARTITION BY region 
            ORDER BY (balance, acc_num);
          "
          
          echo "üöÄ Streaming Data: PG -> Pipe -> ClickHouse..."
          # Removing LIMIT to process FULL dataset
          psql -h 127.0.0.1 -U ${{ env.PG_USER }} -c "COPY (SELECT acc_num, balance, risk_score, region FROM accounts) TO STDOUT WITH CSV" \
          | docker exec -i ch_analytics clickhouse-client --format_csv_allow_single_quotes=0 --query="INSERT INTO default.dw_finance FORMAT CSV"

      # ===========================================================================
      # 4) INTELLIGENCE: PYTHON ANALYTICS & VISUALIZATION
      # ===========================================================================
      - name: "üß† Run AI Analytics & Visualization"
        run: |
          cat <<EOF > analytics.py
          import pandas as pd
          import matplotlib.pyplot as plt
          import seaborn as sns
          import os

          # Connect to ClickHouse
          print("üîÑ Connecting to ClickHouse for Analysis...")
          
          # Fetch Data (Using direct CSV export for CI simplicity)
          os.system('docker exec -i ch_analytics clickhouse-client --query="SELECT * FROM default.dw_finance LIMIT 10000" --format CSVWithNames > data_sample.csv')
          
          try:
              df = pd.read_csv("data_sample.csv")
              if df.empty:
                  print("‚ö†Ô∏è Warning: Dataset is empty.")
                  exit(0)
                  
              # 1. Wealth Distribution Plot
              plt.figure(figsize=(10, 6))
              sns.histplot(data=df, x='balance', hue='region', element="step", stat="density", common_norm=False)
              plt.title('Wealth Distribution by Region (Sample)')
              plt.savefig('${VISUAL_DIR}/wealth_distribution.png')
              print("‚úÖ Graph Generated: wealth_distribution.png")
              
              # 2. Risk vs Wealth Scatter
              plt.figure(figsize=(10, 6))
              sns.scatterplot(data=df, x='risk_score', y='balance', hue='region', alpha=0.6)
              plt.title('Risk Score vs Balance Analysis')
              plt.savefig('${VISUAL_DIR}/risk_analysis.png')
              print("‚úÖ Graph Generated: risk_analysis.png")
          
          except Exception as e:
              print(f"‚ùå Analytics Error: {e}")
          EOF
          
          # Run Python Script
          python3 analytics.py

      # ===========================================================================
      # 5) SECURE VAULT & RELEASE GENERATION
      # ===========================================================================
      - name: "üì¶ Pack Secure Artifacts & HTML Report"
        run: |
          # Generate HTML Dashboard
          cat <<EOF > ${ARTIFACT_DIR}/dashboard.html
          <!DOCTYPE html>
          <html>
          <head><title>üá∞üá∑ ETERNITY v16.1 Intelligence Report</title></head>
          <body style="font-family: sans-serif; padding: 20px;">
            <h1>üá∞üá∑ ETERNITY v16.1 (STABILITY-PLUS) Report</h1>
            <hr>
            <h3>üìä Wealth Distribution AI Analysis</h3>
            <img src="./visuals/wealth_distribution.png" style="max-width: 100%; border: 1px solid #ccc;">
            <br><br>
            <h3>‚ö†Ô∏è Risk Factor Analysis</h3>
            <img src="./visuals/risk_analysis.png" style="max-width: 100%; border: 1px solid #ccc;">
            <hr>
            <p>Generated by GitHub Actions & ClickHouse OLAP</p>
          </body>
          </html>
          EOF

          # Encrypt Data (PBKDF2)
          tar -czf ${VAULT_DIR}/raw_data.tar.gz ${VISUAL_DIR}
          openssl enc -aes-256-cbc -salt -pbkdf2 -iter 100000 -in ${VAULT_DIR}/raw_data.tar.gz -out ${VAULT_DIR}/secure_assets.enc -k "${ENC_KEY}"
          
          echo "‚úÖ All Assets Secured."

      - name: "üöÄ Release v16.1"
        uses: ncipollo/release-action@v1
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          tag: "v16.1-stability-plus"
          name: "üá∞üá∑ ETERNITY v16.1 (Crash Fix & Stability)"
          body: |
            ## üá∞üá∑ v16.1 CRITICAL FIX
            
            **Stability Patch:**
            - üõ°Ô∏è **Input Validation:** Fixed the `syntax error at or near ")"` by adding fallback logic for empty inputs.
            - üîß **Defensive Scripting:** Ensures `$ROWS` is always a valid integer before SQL execution.
            
            **Capabilities:**
            - üß† **Embedded AI Analytics:** Python (Pandas/Seaborn) integration.
            - ‚ö° **ClickHouse Partitioning:** `PARTITION BY region` enabled.
            - üõ°Ô∏è **Security:** PBKDF2 Encryption.
          artifacts: "artifacts/**/*"
          allowUpdates: true
          makeLatest: true
